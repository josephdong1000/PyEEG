{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PythonEEG - Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "note"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook pythoneeg_figures.ipynb to script\n",
      "[NbConvertApp] Writing 47611 bytes to pythoneeg/figures.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script pythoneeg_figures.ipynb --TagRemovePreprocessor.remove_cell_tags='{\"note\"}' --output-dir ./pythoneeg --output figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "note"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/isilon/marsh_single_unit/PythonEEG\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "print(os.getcwd())\n",
    "sys.path.append(r\".\\pythoneeg\")\n",
    "from pythoneeg import core  # noqa: E402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import math\n",
    "import copy\n",
    "import re\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "from statistics import geometric_mean\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.axes\n",
    "import pandas as pd\n",
    "# from scipy import signal, integrate\n",
    "from scipy.stats import zscore, gzscore, gaussian_kde, linregress\n",
    "# import mne\n",
    "# import mne_connectivity\n",
    "\n",
    "# import spikeinterface.extractors as se\n",
    "# import spikeinterface.preprocessing as spre\n",
    "# import spikeinterface.sorters as ss\n",
    "# import spikeinterface.postprocessing as spost\n",
    "# import spikeinterface.qualitymetrics as sqm\n",
    "# import spikeinterface.exporters as sexp\n",
    "# import spikeinterface.comparison as scmp\n",
    "# import spikeinterface.curation as scur\n",
    "# import spikeinterface.sortingcomponents as sc\n",
    "# import spikeinterface.widgets as sw\n",
    "\n",
    "import cmasher as cmr\n",
    "# import PyQt5.QtWidgets as qw\n",
    "# import PyQt5.QtCore as qc\n",
    "# import pygwalker as pyg\n",
    "# import PyInstaller.__main__\n",
    "\n",
    "from pythoneeg import core\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimalFeatureParser:\n",
    "\n",
    "    GENOTYPE_ALIASES = {'WT' : ['WT', 'wildtype'],\n",
    "                        'KO' : ['KO', 'knockout']}\n",
    "    CHNAME_ALIASES = {'A' : ['Aud', 'aud'],\n",
    "                      'V' : ['Vis', 'vis'],\n",
    "                      'H' : ['Hip', 'hip'],\n",
    "                      'B' : ['Bar', 'bar'],\n",
    "                      'M' : ['Mot', 'mot']}\n",
    "    LR_ALIASES = {'L' : ['left', 'Left', 'L ', ' L'],\n",
    "                  'R' : ['right', 'Right', 'R ', ' R']}\n",
    "    DEFAULT_CHNUM_TO_NAME = {9: 'LA',\n",
    "                          10: 'LV',\n",
    "                          12: 'LH',\n",
    "                          14: 'LB',\n",
    "                          15: 'LM',\n",
    "                          16: 'RM',\n",
    "                          17: 'RB',\n",
    "                          19: 'RH',\n",
    "                          21: 'RV',\n",
    "                          22: 'RA',}\n",
    "    BAND_FREQS = core.LongRecordingAnalyzer.FREQ_BANDS\n",
    "    BAND_NAMES = [k for k,_ in BAND_FREQS.items()]\n",
    "    LINEAR_FEATURE = ['rms', 'ampvar', 'psdtotal', 'psdslope']\n",
    "    BAND_FEATURE = ['psdband']\n",
    "    MATRIX_FEATURE = ['cohere', 'pcorr']\n",
    "    HIST_FEATURE = ['psd']\n",
    "\n",
    "    def _sanitize_feature_request(self, features: list[str], exclude: list[str]=[]):\n",
    "        if features == [\"all\"]:\n",
    "            feat = copy.deepcopy(core.LongRecordingAnalyzer.FEATURES)\n",
    "        elif not features:\n",
    "            raise ValueError(\"Features cannot be empty\")\n",
    "        else:\n",
    "            assert all(f in core.LongRecordingAnalyzer.FEATURES for f in features), f\"Available features are: {core.LongRecordingAnalyzer.FEATURES}\"\n",
    "            feat = copy.deepcopy(features)\n",
    "        if exclude is not None:\n",
    "            for e in exclude:\n",
    "                try:\n",
    "                    feat.remove(e)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "        return feat\n",
    "    \n",
    "    def _parse_filename_to_animalday(self, binfolder:str|Path, id_index:int=0, delimiter:str=' ', date_pattern=None):\n",
    "        # not tested if this handles alternative __readmodes yet\n",
    "        geno = self._parse_filename_to_genotype(binfolder)\n",
    "\n",
    "        animid = self._parse_filename_to_animal(binfolder, id_index, delimiter)\n",
    "        \n",
    "        day = self._parse_filename_to_day(binfolder, date_pattern=date_pattern).strftime(\"%b-%d-%Y\")\n",
    "\n",
    "        return f\"{animid} {geno} {day}\"\n",
    "    \n",
    "    def _parse_filename_to_animal(self,binfolder:str|Path, id_index:int=0, delimiter:str=' '):\n",
    "        animid = Path(binfolder).name.split(delimiter)\n",
    "        animid = animid[id_index]\n",
    "        return animid\n",
    "\n",
    "    def _parse_filename_to_genotype(self, filename:str):\n",
    "        name = Path(filename).name\n",
    "        # for k,v in self.GENOTYPE_ALIASES.items():\n",
    "        #     if any([x in name for x in v]):\n",
    "        #         return k\n",
    "        # raise ValueError(f\"Folder {filename} does not contain genotype\\nAvailable genotypes: {self.GENOTYPE_ALIASES}\")\n",
    "        return self.__get_key_from_match_values(name, self.GENOTYPE_ALIASES)\n",
    "    \n",
    "    def _parse_filename_to_day(self, filename:str, date_pattern=None) -> datetime:\n",
    "        date_pattern = r'(\\d{2})\\D*(\\d{2})\\D*(\\d{4})' if date_pattern is None else date_pattern\n",
    "        # self.date_format = date_format.replace(\"N\", \"[0-9]\")\n",
    "        match = re.search(date_pattern, Path(filename).name)\n",
    "        if match:\n",
    "            month, day, year = match.groups()\n",
    "            month = int(month)\n",
    "            day = int(day)\n",
    "            year = int(year)\n",
    "        else:\n",
    "            month, day, year = (1, 1, 1)\n",
    "\n",
    "        # name = Path(filename).name\n",
    "        return datetime(year=year, month=month, day=day)\n",
    "\n",
    "    def _parse_chname_to_abbrev(self, channel_name:str, assume_channels=False):\n",
    "        try:\n",
    "            lr = self.__get_key_from_match_values(channel_name, self.LR_ALIASES)\n",
    "            chname = self.__get_key_from_match_values(channel_name, self.CHNAME_ALIASES)\n",
    "        except ValueError as e:\n",
    "            if assume_channels:\n",
    "                num = int(channel_name.split('-')[-1])\n",
    "                return self.DEFAULT_CHNUM_TO_NAME[num]\n",
    "            else:\n",
    "                raise e\n",
    "        return lr + chname\n",
    "\n",
    "    def __get_key_from_match_values(self, searchonvals:str, dictionary:dict):\n",
    "        for k,v in dictionary.items():\n",
    "            if any([x in searchonvals for x in v]):\n",
    "                return k\n",
    "        raise ValueError(f\"{searchonvals} does not have any matching values. Available values: {dictionary}\")\n",
    "    \n",
    "    def _average_feature(self, df:pd.DataFrame, colname:str, weightsname:str|None='duration'):\n",
    "        column = df[colname]\n",
    "        if weightsname is None:\n",
    "            weights = np.ones(column.size)\n",
    "        else:\n",
    "            weights = df[weightsname]\n",
    "        colitem = column.iloc[0]\n",
    "\n",
    "        match colname:\n",
    "            case 'rms' | 'ampvar' | 'psdtotal' | 'pcorr':\n",
    "                col_agg = np.stack(column, axis=-1)\n",
    "            case 'psdslope':\n",
    "                col_agg = np.array([*column.tolist()])\n",
    "                col_agg = col_agg.transpose(1, 2, 0)\n",
    "            case 'cohere' | 'psdband':\n",
    "                col_agg = {k : np.stack([d[k] for d in column], axis=-1) for k in colitem.keys()}\n",
    "            case 'psd':\n",
    "                col_agg = np.stack([x[1] for x in column], axis=-1)\n",
    "                col_agg = (colitem[0], col_agg)\n",
    "            case 'nspike':\n",
    "                agg_all = np.stack([list(map(sum, x[0])) for x in column], axis=-1)\n",
    "                agg_indiv = np.stack([list(map(sum, x[1])) for x in column], axis=-1)\n",
    "                col_agg = np.stack([agg_all, agg_indiv], axis=1)\n",
    "            case 'wavetemp':\n",
    "                warnings.warn(\"wavetemp cannot be averaged. Use get_wavetemp() instead\")\n",
    "                col_agg = np.NaN\n",
    "            case _:\n",
    "                raise TypeError(f\"Unrecognized type in column {colname}: {colitem}\")\n",
    "\n",
    "        # if type(colitem) is np.ndarray:\n",
    "        #     col_agg = np.stack(column, axis=-1)\n",
    "        # elif type(colitem) is tuple:\n",
    "        #     if colname == 'psdslope':\n",
    "        #         col_agg = np.stack([np.array(x[1]) for x in column], axis=-1)\n",
    "        #     elif colname == 'psd':\n",
    "        #         col_agg = np.stack([x[1] for x in column], axis=-1)\n",
    "        #         col_agg = (colitem[0], col_agg)\n",
    "        #     elif colname == 'nspike':\n",
    "        #         agg_all = np.stack([list(map(sum, x[0])) for x in column], axis=-1)\n",
    "        #         agg_indiv = np.stack([list(map(sum, x[1])) for x in column], axis=-1)\n",
    "        #         col_agg = np.stack([agg_all, agg_indiv], axis=1)\n",
    "        #     elif colname == 'wavetemp':\n",
    "        #         warnings.warn(\"wavetemp cannot be averaged. Use get_wavetemp() instead\")\n",
    "        #         col_agg = np.NaN\n",
    "        #     else:\n",
    "        #         raise TypeError(f\"Unrecognized type in column {colname}\\n{colitem}\")\n",
    "        # elif type(colitem) is dict and all(type(v) is np.ndarray for v in colitem.values()):\n",
    "        #     col_agg = {k : np.stack([d[k] for d in column], axis=-1) for k in colitem.keys()}\n",
    "        # elif type(colitem) is list and all(type(x) is tuple for x in colitem):\n",
    "        #     col_agg = np.stack([np.array(x) for x in column], axis=-1)\n",
    "        # else:\n",
    "        #     raise TypeError(f\"Unrecognized type in column {colname}\\n{colitem}\")\n",
    "\n",
    "        if type(col_agg) is dict:\n",
    "            avg = {k:self._nanaverage(v, axis=-1, weights=weights) for k,v in col_agg.items()}\n",
    "        elif type(col_agg) is tuple:\n",
    "            if colname == 'psd':\n",
    "                avg = (col_agg[0], self._nanaverage(col_agg[1], axis=-1, weights=weights))\n",
    "            elif colname == 'nspike':\n",
    "                avg = col_agg.sum(axis=-1) / np.sum(weights)\n",
    "            else:\n",
    "                avg = None\n",
    "        elif np.isnan(col_agg).all():\n",
    "            avg = np.nan\n",
    "        else:\n",
    "            avg = self._nanaverage(col_agg, axis=-1, weights=weights)\n",
    "        return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowAnalysisResult(AnimalFeatureParser):\n",
    "    \"\"\"\n",
    "    Wrapper for output of windowed analysis. Has useful features like group-wise and global averaging, filtering, and saving\n",
    "    \"\"\"\n",
    "    def __init__(self, result: pd.DataFrame, animal_id:str=None, genotype:str=None, channel_names:list[str]=None, assume_channels=True) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            result (pd.DataFrame): Result comes from AnimalOrganizer.compute_windowed_analysis()\n",
    "            animal_id (str, optional): Identifier for the animal where result was computed from. Defaults to None.\n",
    "            genotype (str, optional): Genotype of animal. Defaults to None.\n",
    "            channel_names (list[str], optional): List of channel names. Defaults to None.\n",
    "            assume_channels (bool, optional): If true, assumes channel names according to AnimalFeatureParser.DEFAULT_CHNUM_TO_NAME. Defaults to True.\n",
    "        \"\"\"\n",
    "        self.result = result\n",
    "        columns = result.columns\n",
    "        self.feature_names = [x for x in columns if x in core.LongRecordingAnalyzer.FEATURES]\n",
    "        self._nonfeat_cols = [x for x in columns if x not in core.LongRecordingAnalyzer.FEATURES]\n",
    "        animaldays = result.loc[:, \"animalday\"].unique()\n",
    "        # print(animaldays)\n",
    "        # animaldays = [self._parse_filename_to_animalday(x) for x in animaldays]\n",
    "        # animaldays.sort()\n",
    "        self.animaldays = animaldays\n",
    "        self.avg_result: pd.DataFrame\n",
    "        self.animal_id = animal_id\n",
    "        self.genotype = genotype\n",
    "        self.channel_names = channel_names\n",
    "        self.assume_channels = assume_channels\n",
    "        self.short_chnames = [self._parse_chname_to_abbrev(x, assume_channels=assume_channels) for x in self.channel_names]\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.result.__str__()\n",
    "    \n",
    "    def get_result(self, features: list[str], exclude: list[str]=[], allow_missing=False):\n",
    "        \"\"\"Get windowed analysis result dataframe, with helpful filters\n",
    "\n",
    "        Args:\n",
    "            features (list[str]): List of features to get from result\n",
    "            exclude (list[str], optional): List of features to exclude from result; will override the features parameter. Defaults to [].\n",
    "            allow_missing (bool, optional): If True, will return all requested features as columns regardless if they exist in result. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            result: pd.DataFrame object with features in columns and windows in rows\n",
    "        \"\"\"\n",
    "        features = self._sanitize_feature_request(features, exclude)\n",
    "        if not allow_missing:\n",
    "            return self.result.loc[:, self._nonfeat_cols + features]\n",
    "        else:\n",
    "            return self.result.reindex(columns=self._nonfeat_cols + features)\n",
    "    \n",
    "    def get_groupavg_result(self, features: list[str], exclude: list[str]=[], df: pd.DataFrame = None, groupby=\"animalday\"):\n",
    "        \"\"\"Group result and average within groups. Preserves data structure and shape for each feature.\n",
    "\n",
    "        Args:\n",
    "            features (list[str]): List of features to get from result\n",
    "            exclude (list[str], optional): List of features to exclude from result. Will override the features parameter. Defaults to [].\n",
    "            df (pd.DataFrame, optional): If not None, this function will use this dataframe instead of self.result. Defaults to None.\n",
    "            groupby (str, optional): Feature or list of features to group by before averaging. Passed to the `by` parameter in pd.DataFrame.groupby(). Defaults to \"animalday\".\n",
    "\n",
    "        Returns:\n",
    "            grouped_result: result grouped by `groupby` and averaged for each group.\n",
    "        \"\"\"\n",
    "        result_grouped, result_validcols = self.__get_groups(features=features, exclude=exclude, df=df, groupby=groupby)\n",
    "        features = self._sanitize_feature_request(features, exclude)\n",
    "\n",
    "        avg_results = []\n",
    "        for f in features:\n",
    "            if f in result_validcols:\n",
    "                avg_result_col = result_grouped.apply(self._average_feature, f, \"duration\", include_groups=False)\n",
    "                avg_result_col.name = f\n",
    "                avg_results.append(avg_result_col)\n",
    "            else:\n",
    "                warnings.warn(f\"{f} not calculated, skipping\")\n",
    "\n",
    "        return pd.concat(avg_results, axis=1)\n",
    "    \n",
    "    def __get_groups(self, features: list[str], exclude: list[str]=[], df: pd.DataFrame = None, groupby=\"animalday\"):\n",
    "        features = self._sanitize_feature_request(features, exclude)\n",
    "        result_win = self.result if df is None else df\n",
    "        return result_win.groupby(groupby), result_win.columns\n",
    "    \n",
    "    def get_grouprows_result(self, features: list[str], exclude: list[str]=[], df: pd.DataFrame = None, \n",
    "                            multiindex=[\"animalday\", \"animal\", \"genotype\"], include=[\"duration\", \"endfile\"]):\n",
    "        features = self._sanitize_feature_request(features, exclude)\n",
    "        result_win = self.result if df is None else df\n",
    "        result_win = result_win.filter(features + multiindex + include)\n",
    "        return result_win.set_index(multiindex)\n",
    "    \n",
    "    # Must output by animal only, unless writing some way to mix inhomogenous spike outputs\n",
    "    def get_wavetemp(self, df:pd.DataFrame=None, animalcol=\"animalday\", wavetempcol=\"wavetemp\"):\n",
    "        result_win = self.result if df is None else df\n",
    "        return result_win.groupby(animalcol)[[animalcol, wavetempcol]].head(1).set_index(animalcol)\n",
    "    \n",
    "    # NOTE add this info to documentation: False = remove, True = keep. Will need to AND the arrays together to get the final list\n",
    "    def get_filter_rms_range(self, df:pd.DataFrame=None, z_range=2, **kwargs):\n",
    "        result = df.copy() if df is not None else self.result.copy()\n",
    "        z_range = abs(z_range)\n",
    "        np_rms = np.array(result['rms'].tolist())\n",
    "        np_rms = np.log(np_rms)\n",
    "        np_rmsz = zscore(np_rms, axis=0, nan_policy='omit')\n",
    "        np_rms[(np_rmsz > z_range) | (np_rmsz < -z_range)] = np.nan\n",
    "        result['rms'] = np_rms.tolist()\n",
    "        \n",
    "        out = np.full(np_rms.shape, True)\n",
    "        out[(np_rmsz > z_range) | (np_rmsz < -z_range)] = False\n",
    "        return out\n",
    "    \n",
    "    def get_filter_high_rms(self, df:pd.DataFrame=None, max_rms=3000, **kwargs):\n",
    "        result = df.copy() if df is not None else self.result.copy()\n",
    "        np_rms = np.array(result['rms'].tolist())\n",
    "        np_rmsnan = np_rms.copy()\n",
    "        np_rmsnan[np_rms > max_rms] = np.nan\n",
    "        result['rms'] = np_rmsnan.tolist()\n",
    "        \n",
    "        out = np.full(np_rms.shape, True)\n",
    "        out[np_rms > max_rms] = False\n",
    "        return out\n",
    "    \n",
    "    def get_filter_high_beta(self, df:pd.DataFrame=None, max_beta=0.25, throw_all=True, **kwargs):\n",
    "        result = df.copy() if df is not None else self.result.copy()\n",
    "        df_cohere = pd.DataFrame(result['psdband'].tolist())\n",
    "        np_beta = np.array(df_cohere['beta'].tolist())\n",
    "        np_allbands = np.array(df_cohere.values.tolist())\n",
    "        np_allbands = np_allbands.sum(axis=1)\n",
    "        np_prop = np_beta / np_allbands\n",
    "\n",
    "        out = np.full(np_prop.shape, True)\n",
    "        out[np_prop > max_beta] = False\n",
    "        out = np.broadcast_to(np.all(out, axis=-1)[:, np.newaxis], out.shape)\n",
    "        return out\n",
    "\n",
    "    def filter_all(self, df:pd.DataFrame=None, verbose=True, inplace=True, **kwargs):\n",
    "        filters = [self.get_filter_rms_range, self.get_filter_high_rms, self.get_filter_high_beta]\n",
    "        filt_bools = []\n",
    "        for filt in filters:\n",
    "            filt_bool = filt(df, **kwargs)\n",
    "            filt_bools.append(filt_bool)\n",
    "            if verbose:\n",
    "                print(f\"{filt.__name__}:\\tfiltered {filt_bool.size - np.count_nonzero(filt_bool)}/{filt_bool.size}\")\n",
    "        filt_bool_all = np.prod(np.stack(filt_bools, axis=-1), axis=-1).astype(bool)\n",
    "        filtered_result = self._apply_filter(filt_bool_all, verbose=verbose)\n",
    "        if inplace:\n",
    "            del self.result\n",
    "            self.result = filtered_result\n",
    "        return filtered_result\n",
    "\n",
    "    # NOTE filter_tfs is a Mfragments x Nchannels numpy array\n",
    "    def _apply_filter(self, filter_tfs:np.ndarray, verbose=True):\n",
    "        result = self.result.copy()\n",
    "        filter_tfs = np.array(filter_tfs, dtype=bool)\n",
    "        for feat in core.LongRecordingAnalyzer.FEATURES:\n",
    "            if verbose:\n",
    "                print(f\"Filtering {feat}..\")\n",
    "            match feat:\n",
    "                case 'rms' | 'ampvar' | 'psdtotal':\n",
    "                    vals = np.array(result[feat].tolist())\n",
    "                    vals[~filter_tfs] = np.nan\n",
    "                    result[feat] = vals.tolist()\n",
    "                case 'psd':\n",
    "                    coords = np.array([x[0] for x in result[feat].tolist()])\n",
    "                    vals = np.array([x[1] for x in result[feat].tolist()])\n",
    "                    mask = np.broadcast_to(filter_tfs[:, np.newaxis, :], vals.shape)\n",
    "                    vals[~mask] = np.nan\n",
    "                    outs = [(c, vals[i, :, :]) for i,c in enumerate(coords)]\n",
    "                    result[feat] = outs\n",
    "                case 'psdband':\n",
    "                    vals = pd.DataFrame(result[feat].tolist())\n",
    "                    for colname in vals.columns:\n",
    "                        v = np.array(vals[colname].tolist())\n",
    "                        v[~filter_tfs] = np.nan\n",
    "                        vals[colname] = v.tolist()\n",
    "                    result[feat] = vals.to_dict('records')\n",
    "                case 'psdslope':\n",
    "                    vals = np.array(result[feat].tolist())\n",
    "                    mask = np.broadcast_to(filter_tfs[:, :, np.newaxis], vals.shape)\n",
    "                    vals[~mask] = np.nan\n",
    "                    # vals = [list(map(tuple, x)) for x in vals.tolist()]\n",
    "                    result[feat] = vals.tolist()\n",
    "                case 'cohere':\n",
    "                    vals = pd.DataFrame(result[feat].tolist())\n",
    "                    shape = np.array(vals.iloc[:, 0].tolist()).shape\n",
    "                    mask = np.broadcast_to(filter_tfs[:, :, np.newaxis], shape)\n",
    "                    for colname in vals.columns:\n",
    "                        v = np.array(vals[colname].tolist())\n",
    "                        v[~mask] = np.nan\n",
    "                        v[~mask.transpose(0, 2, 1)] = np.nan\n",
    "                        vals[colname] = v.tolist()\n",
    "                    result[feat] = vals.to_dict('records')\n",
    "                case 'pcorr':\n",
    "                    vals = np.array(result[feat].tolist())\n",
    "                    mask = np.broadcast_to(filter_tfs[:, :, np.newaxis], vals.shape)\n",
    "                    vals[~mask] = np.nan\n",
    "                    vals[~mask.transpose(0, 2, 1)] = np.nan\n",
    "                    result[feat] = vals.tolist()\n",
    "                case 'nspike' | 'wavetemp':\n",
    "                    warnings.warn('nspike and wavetemp are not supported for filtering yet')\n",
    "                case _:\n",
    "                    raise ValueError(f'Unknown feature to filter {feat}')\n",
    "        return result\n",
    "\n",
    "    # def filter_bad_fragments(self, filt_beta=True, beta_prop=0.75):\n",
    "    #     # STUB filter out whole fragments based on telling channel features\n",
    "    #     ...\n",
    "\n",
    "\n",
    "    def to_pickle_and_json(self, folder):\n",
    "        filebase =  Path(folder) / f\"{self.genotype}-{self.animal_id}\"\n",
    "        filebase = str(filebase)\n",
    "\n",
    "        self.result.to_pickle(filebase + '.pkl')\n",
    "        with open(filebase + \".json\", \"w\") as f:\n",
    "            json.dump(self.channel_names, f, indent=2)\n",
    "            \n",
    "\n",
    "    # def get_temps_from_wavetemp(self, sa_sas=None, **kwargs): # TODO have some way of storing spikes and traces\n",
    "    #     if sa_sas is None:\n",
    "    #         if not hasattr(self, \"computed_sorting_analyzer\") or not hasattr(self, \"computed_sorting_analyzers\"):\n",
    "    #             self.compute_wavetemp(sa_sas, **kwargs)\n",
    "    #         sa_sas = (self.computed_sorting_analyzer, self.computed_sorting_analyzers)\n",
    "    #     sa, sas = sa_sas\n",
    "    #     assert isinstance(sa, si.SortingAnalyzer)\n",
    "    #     for e in sas:\n",
    "    #         assert isinstance(e, si.SortingAnalyzer)\n",
    "\n",
    "    #     if sa.get_num_units() > 0:\n",
    "    #         ext_temps = sa.get_extension(\"templates\")\n",
    "    #         temp_out = ext_temps.get_data(\"average\")\n",
    "    #     else:\n",
    "    #         print(\"No units across all channels, skipping..\")\n",
    "            \n",
    "    #     temps_out = []\n",
    "    #     for i,e in enumerate(sas): # across sorting analyzers\n",
    "    #         if e.get_num_units() == 0:\n",
    "    #             print(f\"No units in channel {i}, skipping..\")\n",
    "    #             temps_out.append(None)\n",
    "    #             continue\n",
    "    #         ext_temps = e.get_extension(\"templates\")\n",
    "    #         avg_temps = ext_temps.get_data(\"average\")\n",
    "    #         temps_out.append(avg_temps)\n",
    "\n",
    "    #     return temp_out, temps_out\n",
    "\n",
    "    def _nanaverage(self, A, weights, axis=-1):\n",
    "        masked = np.ma.masked_array(A, np.isnan(A))\n",
    "        avg = np.ma.average(masked, axis=axis, weights=weights)\n",
    "        return avg.filled(np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimalOrganizer(AnimalFeatureParser):\n",
    "\n",
    "    READ_MODES = [\"nest\", \"concat\", \"base\", \"noday\"]\n",
    "\n",
    "    def __init__(self, base_folder_path, anim_id: str, date_format=\"NN*NN*NNNN\", mode=\"concat\", skip_days: list[str] = [], truncate:bool|int=False) -> None:\n",
    "        \n",
    "        self.base_folder_path = Path(base_folder_path)\n",
    "        self.anim_id = anim_id\n",
    "        self.date_format = date_format.replace(\"N\", \"[0-9]\")\n",
    "        self.__readmode = mode\n",
    "\n",
    "        assert mode in self.READ_MODES\n",
    "        match mode:\n",
    "            case \"nest\":\n",
    "                self.bin_folder_pat = self.base_folder_path / f\"*{self.anim_id}*\" / f\"*{self.date_format}*\"\n",
    "            case \"concat\":\n",
    "                self.bin_folder_pat = self.base_folder_path / f\"*{self.anim_id}*{self.date_format}*\"\n",
    "            case \"base\":\n",
    "                self.bin_folder_pat = self.base_folder_path\n",
    "            case \"noday\":\n",
    "                self.bin_folder_pat = self.base_folder_path / f\"*{self.anim_id}*\"\n",
    "        self.__bin_folders = glob.glob(str(self.bin_folder_pat))\n",
    "        self.__bin_folders = [x for x in self.__bin_folders if not any(y in x for y in skip_days)]\n",
    "\n",
    "        if mode == \"noday\" and len(self.__bin_folders) > 1:\n",
    "            raise ValueError(f\"Animal ID '{self.anim_id}' is not unique, found: {', '.join(self.__bin_folders)}\")\n",
    "        elif len(self.__bin_folders) == 0:\n",
    "            raise ValueError(f\"No files found for animal ID {self.anim_id} and date format {date_format}\")\n",
    "        \n",
    "        self.long_recordings: list[core.LongRecordingOrganizer] = []\n",
    "        self.long_analyzers: list[core.LongRecordingAnalyzer] = []\n",
    "        self.metadatas: list[core.DDFBinaryMetadata] = []\n",
    "        self.animaldays: list[str] = []\n",
    "        for e in self.__bin_folders:\n",
    "            self.long_recordings.append(core.LongRecordingOrganizer(e, truncate=truncate))\n",
    "            self.metadatas.append(self.long_recordings[-1].meta)\n",
    "            self.animaldays.append(self._parse_filename_to_animalday(e))\n",
    "\n",
    "        self.__genotypes = [self._parse_filename_to_genotype(x) for x in self.animaldays]\n",
    "        if len(set(self.__genotypes)) > 1:\n",
    "            raise ValueError(f\"Inconsistent genotypes in {self.animaldays}\")\n",
    "        self.genotype = self.__genotypes[0]\n",
    "        self.__channel_names = [x.channel_names for x in self.long_recordings]\n",
    "        if len(set([\" \".join(x) for x in self.__channel_names])) > 1:\n",
    "            raise ValueError(f\"Inconsistent channel names in {self.__channel_names}\")\n",
    "        self.channel_names = self.__channel_names[0]\n",
    "        self.__animal_ids = [self._parse_filename_to_animal(x) for x in self.animaldays]\n",
    "        if len(set(self.__animal_ids)) > 1:\n",
    "            raise ValueError(f\"Inconsistent animal IDs in {self.animaldays}\")\n",
    "        self.animal_id = self.__animal_ids[0]\n",
    "\n",
    "        self.features_df: pd.DataFrame = pd.DataFrame()\n",
    "        self.features_avg_df: pd.DataFrame = pd.DataFrame()\n",
    "\n",
    "    def convert_colbins_to_rowbins(self, overwrite=False):\n",
    "        for lrec in self.long_recordings:\n",
    "            lrec.convert_colbins_to_rowbins(overwrite=overwrite)\n",
    "\n",
    "    def convert_rowbins_to_rec(self):\n",
    "        for lrec in self.long_recordings:\n",
    "            lrec.convert_rowbins_to_rec()\n",
    "\n",
    "    def cleanup_rec(self):\n",
    "        for lrec in self.long_recordings:\n",
    "            lrec.cleanup_rec()\n",
    "\n",
    "    def compute_windowed_analysis(self, features: list[str], exclude: list[str]=[], window_s=4, **kwargs):\n",
    "        \"\"\"Computes windowed analysis of animal recordings. The data is divided into windows (time bins), then features are extracted from each window. The result is\n",
    "        formatted to a Dataframe and wrapped into a WindowAnalysisResult object.\n",
    "\n",
    "        Args:\n",
    "            features (list[str]): List of features to compute. See individual compute_...() functions for output format\n",
    "            exclude (list[str], optional): List of features to ignore. Will override the features parameter. Defaults to [].\n",
    "            window_s (int, optional): Length of each window in seconds. Note that some features break with very short window times. Defaults to 4.\n",
    "\n",
    "        Raises:\n",
    "            AttributeError: If a feature's compute_...() function was not implemented, this error will be raised.\n",
    "\n",
    "        Returns:\n",
    "            window_analysis_result: a WindowAnalysisResult object\n",
    "        \"\"\"\n",
    "        features = self._sanitize_feature_request(features, exclude)\n",
    "\n",
    "        dataframes = []\n",
    "        for lrec in self.long_recordings:\n",
    "            out = []\n",
    "            lan = core.LongRecordingAnalyzer(lrec, fragment_len_s=window_s)\n",
    "            for j,feat in enumerate(features):\n",
    "                \n",
    "                if 'n_jobs_coh' in kwargs and feat == 'cohere':\n",
    "                    lan.setup_njobs()\n",
    "\n",
    "                func_name = f\"compute_{feat}\"\n",
    "                func = getattr(lan, func_name)\n",
    "                if callable(func):\n",
    "                    print(f\"Computing {feat}..\")\n",
    "                    t = time.process_time()\n",
    "                    for idx in range(lan.n_fragments):\n",
    "                        if j == 0:\n",
    "                            out.append({})\n",
    "\n",
    "                            lan_folder = lan.LongRecording.base_folder_path\n",
    "                            out[idx][\"animalday\"] = self._parse_filename_to_animalday(lan_folder)\n",
    "                            out[idx][\"animal\"] = self._parse_filename_to_animal(lan_folder)\n",
    "                            out[idx][\"day\"] = self._parse_filename_to_day(lan_folder)\n",
    "                            out[idx][\"genotype\"] = self._parse_filename_to_genotype(lan_folder)\n",
    "                            out[idx][\"duration\"] = lan.LongRecording.get_dur_fragment(window_s, idx)\n",
    "                            out[idx][\"endfile\"] = lan.get_file_end(idx)\n",
    "                            \n",
    "                            frag_dt = lan.LongRecording.get_datetime_fragment(window_s, idx)\n",
    "                            out[idx][\"timestamp\"] = frag_dt\n",
    "                            out[idx][\"isday\"] = core.is_day(frag_dt)\n",
    "\n",
    "                        out[idx][feat] = func(idx, **kwargs)\n",
    "                    print(f\"\\t..done in {time.process_time() - t} s\")\n",
    "\n",
    "                else:\n",
    "                    raise AttributeError(f\"Invalid function {func}\")\n",
    "\n",
    "            self.long_analyzers.append(lan)\n",
    "            out = pd.DataFrame(out)\n",
    "            dataframes.append(out)\n",
    "        \n",
    "        self.features_df = pd.concat(dataframes)\n",
    "        self.features_df.reset_index(inplace=True)\n",
    "\n",
    "        self.window_analysis_result = WindowAnalysisResult(self.features_df, self.animal_id, self.genotype, self.channel_names)\n",
    "\n",
    "        return self.window_analysis_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "note"
    ]
   },
   "outputs": [],
   "source": [
    "%%script true\n",
    "\n",
    "# ao = AnimalOrganizer(r\"Z:\\PythonEEG Data Bins\", \"F22\", mode=\"concat\", skip_days=[\"12_12-2023\", \"12_15_2023\"], truncate=10)\n",
    "ao = AnimalOrganizer(r\"Z:\\PythonEEG Data Bins\", \"F22\", mode=\"concat\")\n",
    "ao.convert_colbins_to_rowbins(overwrite=False)\n",
    "ao.convert_rowbins_to_rec()\n",
    "\n",
    "# result = ao.compute_windowed_analysis(['all'])\n",
    "# result = ao.compute_windowed_analysis(['rms'])\n",
    "# result = ao.compute_windowed_analysis(['all'], exclude=['nspike', 'wavetemp', 'cohere', 'pcorr'])\n",
    "war = ao.compute_windowed_analysis(['all'], exclude=['nspike', 'wavetemp'])\n",
    "# result = ao.compute_windowed_analysis(['cohere', 'pcorr']) # nspike needs work\n",
    "# result = ao.compute_windowed_analysis(['nspike', 'wavetemp']) # nspike needs work\n",
    "# result = ao.compute_windowed_analysis(['psd', 'cohere'], welch_bin_t=2, magnitude=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "note"
    ]
   },
   "outputs": [],
   "source": [
    "%%script true\n",
    "ao.cleanup_rec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "note"
    ]
   },
   "outputs": [],
   "source": [
    "# display(WindowAnalysisResult(result.result).get_result(['all'], allow_missing=True).head(2))\n",
    "# display(WindowAnalysisResult(result.result).get_groupavg_result(['all'], groupby=['day', 'isday']))\n",
    "# display(WindowAnalysisResult(result.result).get_grouprows_result(['all']).head(2))\n",
    "\n",
    "# WindowAnalysisResult(result.result).get_wavetemp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimalPlotter(AnimalFeatureParser):\n",
    "\n",
    "    # TODO Plot hist features across every channel as a histogram with standard deviations, as a spectrograph\n",
    "    # Plot matrix features as several grids per-band for cohere + 1 for pcorr, as a spectrograph flattening channel combinations\n",
    "    # Plot linear features as a line over time, as an box plot collapsing over time, as a datapoint on another ExperimentPlotter (wip)\n",
    "    # Plot band features as many lines over time, or as a spectrograph over time\n",
    "    # TODO Plot spike features (wip) as raster plot per channel, as line plot in time bins, as unit traces over channels\n",
    "    # STUB Plot peri-spike frequency plot, convolving over time based on frequency with an (arbitrarily chosen) gaussian filter\n",
    "    # STUB make experimental UMAP (q-UMAP?) plotter\n",
    "\n",
    "    def __init__(self, war: WindowAnalysisResult) -> None:\n",
    "        self.window_result = war\n",
    "        self.genotype = war.genotype\n",
    "        self.channel_names = war.channel_names\n",
    "        self.n_channels = len(self.channel_names)\n",
    "        self.__assume_channels = war.assume_channels\n",
    "        self.short_chnames = war.short_chnames\n",
    "\n",
    "    # REVIEW this function may not be necessary\n",
    "    # def get_animalday_metadata(self, animalday) -> core.DDFBinaryMetadata:\n",
    "    #     return self.window_result.meta[self.window_result.animaldays.index(animalday)]\n",
    "\n",
    "    def _abbreviate_channel(self, ch_name:str):\n",
    "        for k,v in self.CHNAME_TO_ABBREV:\n",
    "            if k in ch_name:\n",
    "                return v\n",
    "        return ch_name\n",
    "    \n",
    "    def plot_coherecorr_matrix(self, groupby=\"animalday\", bands=None, figsize=None, cmap='viridis', **kwargs):\n",
    "        # avg_result = self.window_result.get_grouped_avg(self.MATRIX_FEATURE, groupby=groupby)\n",
    "        # avg_coheresplit = pd.json_normalize(avg_result['cohere']).set_index(avg_result.index)\n",
    "        # avg_result = avg_coheresplit.join(avg_result)\n",
    "        avg_result = self.__get_groupavg_coherecorr(groupby, **kwargs)\n",
    "\n",
    "        if bands is None:\n",
    "            bands = list(core.LongRecordingAnalyzer.FREQ_BANDS.keys()) + ['pcorr']\n",
    "        elif isinstance(bands, str):\n",
    "            bands = [bands]\n",
    "        n_row = avg_result.index.size\n",
    "        # rowcount = 0\n",
    "        fig, ax = plt.subplots(n_row, len(bands), squeeze=False, figsize=figsize, **kwargs)\n",
    "        \n",
    "        normlist = [matplotlib.colors.Normalize(vmin=0, vmax=np.max(np.concatenate(avg_result[band].values))) for band in bands]\n",
    "        for i, (_, row) in enumerate(avg_result.iterrows()):\n",
    "            self._plot_coherecorr_matrixgroup(row, bands, ax[i, :], show_bandname=i == 0, norm_list=normlist, cmap=cmap, **kwargs)\n",
    "            # rowcount += 1\n",
    "        plt.show()\n",
    "\n",
    "    def plot_coherecorr_diff(self, groupby=\"isday\", bands=None, figsize=None, cmap='bwr', **kwargs):\n",
    "        avg_result = self.__get_groupavg_coherecorr(groupby, **kwargs)\n",
    "        avg_result = avg_result.drop('cohere', axis=1, errors='ignore')\n",
    "        if len(avg_result.index) != 2:\n",
    "            raise ValueError(f\"Difference can only be calculated between 2 rows. {groupby} resulted in {len(avg_result.index)} rows\")\n",
    "        \n",
    "        if bands is None:\n",
    "            bands = list(core.LongRecordingAnalyzer.FREQ_BANDS.keys()) + ['pcorr']\n",
    "        elif isinstance(bands, str):\n",
    "            bands = [bands]\n",
    "\n",
    "        diff_result = avg_result.iloc[1] - avg_result.iloc[0]\n",
    "        diff_result.name = f\"{avg_result.iloc[1].name} - {avg_result.iloc[0].name}\"\n",
    "\n",
    "        fig, ax = plt.subplots(1, len(bands), squeeze=False, figsize=figsize, **kwargs)\n",
    "        \n",
    "        self._plot_coherecorr_matrixgroup(diff_result, bands, ax[0, :], show_bandname=True, center_cmap=True, cmap=cmap, **kwargs)\n",
    "\n",
    "    def _plot_coherecorr_matrixgroup(self, group:pd.Series, bands:list[str], ax:list[matplotlib.axes.Axes], show_bandname,\n",
    "                                    center_cmap=False, norm_list=None, show_channelname=True, **kwargs):\n",
    "        rowname = group.name\n",
    "        for i, band in enumerate(bands):\n",
    "            if norm_list is None:\n",
    "                if center_cmap:\n",
    "                    divnorm = matplotlib.colors.CenteredNorm()\n",
    "                else:\n",
    "                    divnorm = None\n",
    "                ax[i].imshow(group[band], norm=divnorm, **kwargs)\n",
    "            else:\n",
    "                ax[i].imshow(group[band], norm=norm_list[i], **kwargs)\n",
    "\n",
    "            if show_bandname:\n",
    "                ax[i].set_xlabel(band, fontsize='x-large')\n",
    "                ax[i].xaxis.set_label_position('top')\n",
    "\n",
    "            if show_channelname:\n",
    "                ax[i].set_xticks(range(self.n_channels), self.short_chnames, rotation='vertical')\n",
    "                ax[i].set_yticks(range(self.n_channels), self.short_chnames)\n",
    "            else:\n",
    "                ax[i].set_xticks(range(self.n_channels), \" \")\n",
    "                ax[i].set_yticks(range(self.n_channels), \" \")\n",
    "\n",
    "        ax[0].set_ylabel(rowname, rotation='horizontal', ha='right')\n",
    "    \n",
    "    def __get_groupavg_coherecorr(self, groupby=\"animalday\", **kwargs):\n",
    "        avg_result = self.window_result.get_groupavg_result(self.MATRIX_FEATURE, groupby=groupby)\n",
    "        avg_coheresplit = pd.json_normalize(avg_result['cohere']).set_index(avg_result.index) # Split apart the cohere dictionaries\n",
    "        return avg_coheresplit.join(avg_result)\n",
    "    \n",
    "    def plot_linear_temporal(self, multiindex=[\"animalday\", \"animal\", \"genotype\"], features:list[str]=None, channels:list[int]=None, figsize=None,\n",
    "                             score_type='z', show_endfile=False, **kwargs):\n",
    "        if features is None:\n",
    "            features = self.LINEAR_FEATURE + self.BAND_FEATURE\n",
    "        if channels is None:\n",
    "            channels = np.arange(self.n_channels)\n",
    "\n",
    "        height_ratios = {'rms' : 1,\n",
    "                         'ampvar' : 1,\n",
    "                         'psdtotal' : 1,\n",
    "                         'psdslope' : 2,\n",
    "                         'psdband' : 5}\n",
    "\n",
    "        # df_featgroups = self.window_result.get_grouped(features, groupby=groupby)\n",
    "        df_rowgroup = self.window_result.get_grouprows_result(features, multiindex=multiindex)\n",
    "        for i, df_row in df_rowgroup.groupby(level=0):\n",
    "            fig, ax = plt.subplots(len(features), 1, figsize=figsize, sharex=True,\n",
    "                                   gridspec_kw={'height_ratios' : [height_ratios[x] for x in features]})\n",
    "            plt.subplots_adjust(hspace=0)\n",
    "\n",
    "            for j, feat in enumerate(features):\n",
    "                self._plot_linear_temporalgroup(group=df_row, feature=feat, ax=ax[j], score_type=score_type, channels=channels, show_endfile=show_endfile, **kwargs)\n",
    "            ax[-1].set_xlabel(\"Time (s)\")\n",
    "            fig.suptitle(i)\n",
    "            plt.show()\n",
    "\n",
    "    def _plot_linear_temporalgroup(self, group:pd.DataFrame, feature:str, ax:matplotlib.axes.Axes, channels:list[int]=None, score_type:str='z', \n",
    "                                     duration_name='duration', channel_y_offset=10, feature_y_offset=10, endfile_name='endfile', show_endfile=False, show_channelname=True, **kwargs):\n",
    "        \n",
    "        data_Z = self.__get_linear_feature(group=group, feature=feature, score_type=score_type)\n",
    "\n",
    "        data_t = group[duration_name]\n",
    "        data_T = np.cumsum(data_t)\n",
    "\n",
    "        if channels is None:\n",
    "            channels = np.arange(data_Z.shape[1])\n",
    "        data_Z = data_Z[:, channels, :]\n",
    "\n",
    "        n_chan = data_Z.shape[1]\n",
    "        n_feat = data_Z.shape[2]\n",
    "        chan_offset = np.linspace(0, channel_y_offset * n_chan, n_chan, endpoint=False).reshape((1, -1, 1))\n",
    "        feat_offset = np.linspace(0, feature_y_offset * n_chan * n_feat, n_feat, endpoint=False).reshape((1, 1, -1))\n",
    "        data_Z += chan_offset\n",
    "        data_Z += feat_offset\n",
    "        ytick_offset = feat_offset.squeeze() + np.mean(chan_offset.flatten())\n",
    "\n",
    "        for i in range(n_feat):\n",
    "            ax.plot(data_T, data_Z[:, :, i], c=f'C{i}', **kwargs)\n",
    "        match feature:\n",
    "            case 'rms' | 'ampvar' | 'psdtotal':\n",
    "                ax.set_yticks([ytick_offset], [feature])\n",
    "            case 'psdslope':\n",
    "                ax.set_yticks(ytick_offset, ['psdslope', 'psdintercept'])\n",
    "            case 'psdband':\n",
    "                ax.set_yticks(ytick_offset, self.BAND_NAMES)\n",
    "            case _:\n",
    "                raise ValueError(f\"Invalid feature {feature}\")\n",
    "\n",
    "        if show_endfile:\n",
    "            self._plot_filediv_lines(group=group, ax=ax, duration_name=duration_name, endfile_name=endfile_name)\n",
    "\n",
    "    def __get_linear_feature(self, group:pd.DataFrame, feature:str, score_type='z', triag=True):\n",
    "        match feature:\n",
    "            case 'psdband':\n",
    "                data_X = np.array([list(d.values()) for d in group[feature]])\n",
    "                data_X = np.stack(data_X, axis=-1)\n",
    "                data_X = np.transpose(data_X)\n",
    "            case 'psdslope':\n",
    "                data_X = np.array(group[feature].to_list())\n",
    "                data_X[:, :, 0] = -data_X[:, :, 0]\n",
    "            case 'rms' | 'ampvar' | 'psdtotal':\n",
    "                data_X = np.array(group[feature].to_list())\n",
    "                data_X = np.expand_dims(data_X, axis=-1)\n",
    "            case 'cohere':\n",
    "                data_X = np.array([list(d.values()) for d in group[feature]])\n",
    "                data_X = np.stack(data_X, axis=-1)\n",
    "                if triag:\n",
    "                    tril = np.tril_indices(data_X.shape[1], k=-1)\n",
    "                    data_X = data_X[:, tril[0], tril[1], :]\n",
    "                data_X = data_X.reshape(data_X.shape[0], -1, data_X.shape[-1])\n",
    "                data_X = np.transpose(data_X)\n",
    "            case 'pcorr':\n",
    "                data_X = np.stack(group[feature], axis=-1)\n",
    "                if triag:\n",
    "                    tril = np.tril_indices(data_X.shape[1], k=-1)\n",
    "                    data_X = data_X[tril[0], tril[1], :]\n",
    "                data_X = data_X.reshape(-1, data_X.shape[-1])\n",
    "                data_X = data_X.transpose()\n",
    "                data_X = np.expand_dims(data_X, axis=-1)\n",
    "            case _:\n",
    "                raise ValueError(f\"Invalid feature {feature}\")\n",
    "\n",
    "        return self._calculate_standard_data(data_X, mode=score_type, axis=0)\n",
    "    \n",
    "    def _plot_filediv_lines(self, group:pd.DataFrame, ax:matplotlib.axes.Axes, duration_name, endfile_name):\n",
    "        filedivs = self.__get_filediv_times(group, duration_name, endfile_name)\n",
    "        for xpos in filedivs:\n",
    "            ax.axvline(xpos, ls='--', c='black', lw=1)\n",
    "\n",
    "    def __get_filediv_times(self, group, duration_name, endfile_name):\n",
    "        cumulative = group[duration_name].cumsum().shift(fill_value=0)\n",
    "        # display( group[[endfile_name]].dropna().head())\n",
    "        # display(cumulative.head())\n",
    "        filedivs = group[endfile_name].dropna() + cumulative[group[endfile_name].notna()]\n",
    "        return filedivs.tolist()\n",
    "\n",
    "    def _calculate_standard_data(self, X, mode='z', axis=0):\n",
    "        match mode:\n",
    "            case \"z\":\n",
    "                data_Z = zscore(X, axis=axis, nan_policy='omit')\n",
    "            case \"zall\":\n",
    "                data_Z = zscore(X, axis=None, nan_policy='omit')\n",
    "            case \"gz\":\n",
    "                data_Z = gzscore(X, axis=axis, nan_policy='omit')\n",
    "            case \"modz\":\n",
    "                data_Z = self.__calculate_modified_zscore(X, axis=axis)\n",
    "            case \"none\" | None:\n",
    "                data_Z = X\n",
    "            case \"center\":\n",
    "                data_Z = X - np.nanmean(X, axis=axis, keepdims=True)\n",
    "            case _:\n",
    "                raise ValueError(f\"Invalid mode {mode}\")\n",
    "        return data_Z\n",
    "\n",
    "    def __calculate_modified_zscore(self, X, axis=0):\n",
    "        X_mid = np.nanmedian(X, axis=axis)\n",
    "        X_absdev = np.nanmedian(np.abs(X - X_mid), axis=axis)\n",
    "        return 0.6745 * (X - X_mid) / X_absdev\n",
    "    \n",
    "    def plot_coherecorr_spectral(self, multiindex=[\"animalday\", \"animal\", \"genotype\"], features:list[str]=None, figsize=None, score_type='z', cmap='bwr', triag=True,\n",
    "                                 show_endfile=False, duration_name='duration', endfile_name='endfile', **kwargs):\n",
    "        if features is None:\n",
    "            features = self.MATRIX_FEATURE\n",
    "        height_ratios = {'cohere' : 5,\n",
    "                         'pcorr' : 1}\n",
    "\n",
    "        df_rowgroup = self.window_result.get_grouprows_result(features, multiindex=multiindex)\n",
    "        for i, df_row in df_rowgroup.groupby(level=0):\n",
    "            fig, ax = plt.subplots(len(features), 1, figsize=figsize, sharex=True,\n",
    "                                   gridspec_kw={'height_ratios' : [height_ratios[x] for x in features]})\n",
    "            plt.subplots_adjust(hspace=0)\n",
    "            for j, feat in enumerate(features):\n",
    "                self._plot_coherecorr_spectralgroup(group=df_row, feature=feat, ax=ax[j], score_type=score_type, triag=triag, show_endfile=show_endfile,\n",
    "                                                    duration_name=duration_name, endfile_name=endfile_name, **kwargs)\n",
    "            ax[-1].set_xlabel(\"Time (s)\")\n",
    "            fig.suptitle(i)\n",
    "            plt.show()\n",
    "\n",
    "    def _plot_coherecorr_spectralgroup(self, group:pd.DataFrame, feature:str, ax:matplotlib.axes.Axes, \n",
    "                                        center_cmap=True, score_type='z', norm_list=None, show_featurename=True, show_endfile=False, \n",
    "                                        duration_name='duration', endfile_name='endfile', cmap='bwr', triag=True, **kwargs):\n",
    "        \n",
    "        data_Z = self.__get_linear_feature(group=group, feature=feature, score_type=score_type)\n",
    "        std_dev = np.nanstd(data_Z.flatten())\n",
    "        \n",
    "        # data_flat = data_Z.reshape(data_Z.shape[0], -1).transpose()\n",
    "\n",
    "        if center_cmap:\n",
    "            norm = matplotlib.colors.CenteredNorm(halfrange=std_dev * 2)\n",
    "        else:\n",
    "            norm = None\n",
    "        \n",
    "        n_ch = data_Z.shape[1]\n",
    "        n_bands = len(self.BAND_NAMES)\n",
    "\n",
    "        for i in range(data_Z.shape[-1]):\n",
    "            extent = (0, data_Z.shape[0] * group['duration'].median(), i * n_ch, (i+1) * n_ch)\n",
    "            ax.imshow(data_Z[:, :, i].transpose(), interpolation='none', aspect='auto', norm=norm, cmap=cmap, extent=extent)\n",
    "\n",
    "        if show_featurename:\n",
    "            match feature:\n",
    "                case 'cohere':\n",
    "                    ticks = n_ch * np.linspace(1/2, n_bands + 1/2, n_bands, endpoint=False)\n",
    "                    ax.set_yticks(ticks=ticks, labels=self.BAND_NAMES)\n",
    "                    for ypos in np.linspace(0, n_bands * n_ch, n_bands, endpoint=False):\n",
    "                        ax.axhline(ypos, lw=1, ls='--', color='black')\n",
    "                case 'pcorr':\n",
    "                    ax.set_yticks(ticks=[1/2 * n_ch], labels=[feature])\n",
    "                case _:\n",
    "                    raise ValueError(f\"Unknown feature name {feature}\")\n",
    "        \n",
    "        if show_endfile:\n",
    "            self._plot_filediv_lines(group=group, ax=ax, duration_name=duration_name, endfile_name=endfile_name)\n",
    "\n",
    "    def plot_psd_histogram(self, groupby='animalday', figsize=None, avg_channels=False, plot_type='loglog', plot_slope=True, xlim=None, **kwargs):\n",
    "        avg_result = self.window_result.get_groupavg_result(['psd'], groupby=groupby)\n",
    "        \n",
    "        n_col = avg_result.index.size\n",
    "        fig, ax = plt.subplots(1, n_col, squeeze=False, figsize=figsize, sharex=True, sharey=True, **kwargs)\n",
    "        plt.subplots_adjust(wspace=0)\n",
    "        for i, (idx, row) in enumerate(avg_result.iterrows()):\n",
    "            freqs = row['psd'][0]\n",
    "            psd = row['psd'][1]\n",
    "            if avg_channels:\n",
    "                psd = np.average(psd, axis=-1, keepdims=True)\n",
    "                label = 'Average'\n",
    "            else:\n",
    "                label = self.short_chnames\n",
    "            match plot_type:\n",
    "                case 'loglog':\n",
    "                    ax[0, i].loglog(freqs, psd, label=label)\n",
    "                case 'semilogy':\n",
    "                    ax[0, i].semilogy(freqs, psd, label=label)\n",
    "                case 'semilogx':\n",
    "                    ax[0, i].semilogy(freqs, psd, label=label)\n",
    "                case 'none':\n",
    "                    ax[0, i].plot(freqs, psd, label=label)\n",
    "                case _:\n",
    "                    raise ValueError(f\"Invalid plot type {plot_type}\")\n",
    "\n",
    "            frange = np.logical_and(freqs >= core.LongRecordingAnalyzer.FREQ_BAND_TOTAL[0], \n",
    "                                    freqs <= core.LongRecordingAnalyzer.FREQ_BAND_TOTAL[1])\n",
    "            logf = np.log10(freqs[frange])\n",
    "            logpsd = np.log10(psd[frange, :])\n",
    "\n",
    "            linfit = np.zeros((psd.shape[1], 2))\n",
    "            for k in range(psd.shape[1]):\n",
    "                result = linregress(logf, logpsd[:, k], 'less')\n",
    "                linfit[k, :] = [result.slope, result.intercept]\n",
    "\n",
    "            for j, (m,b) in enumerate(linfit.tolist()):\n",
    "                ax[0, i].plot(freqs, 10**(b + m * np.log10(freqs)), c=f'C{j}', alpha=0.75)\n",
    "\n",
    "            ax[0, i].set_title(idx)\n",
    "            ax[0, i].set_xlabel(\"Frequency (Hz)\")\n",
    "        ax[0, 0].set_ylabel(\"PSD (uV^2/Hz)\")\n",
    "        ax[0, -1].legend(loc='center left', bbox_to_anchor=(1.05, 0.5))\n",
    "        ax[0, -1].set_xlim(xlim)\n",
    "        plt.show()\n",
    "\n",
    "    # STUB plot spectrogram over time, doing gaussian filter convolving when relevant, scaling logarithmically\n",
    "    def plot_psd_spectrogram(self, multiindex=['animalday', 'animal', 'genotype'], freq_range=(1, 50), center_stat='mean', mode='z', figsize=None, cmap='magma', **kwargs):\n",
    "        # if features is None:\n",
    "        #     features = self.MATRIX_FEATURE\n",
    "        # height_ratios = {'cohere' : 5,\n",
    "        #                  'pcorr' : 1}\n",
    "\n",
    "        df_rowgroup = self.window_result.get_grouprows_result(['psd'], multiindex=multiindex)\n",
    "        for i, df_row in df_rowgroup.groupby(level=0):\n",
    "\n",
    "            freqs = df_row.iloc[0]['psd'][0]\n",
    "            psd = np.array([x[1] for x in df_row['psd'].tolist()])\n",
    "            match center_stat:\n",
    "                case 'mean':\n",
    "                    psd = np.nanmean(psd, axis=-1).transpose()\n",
    "                case 'median':\n",
    "                    psd = np.nanmedian(psd, axis=-1).transpose()\n",
    "                case _:\n",
    "                    raise ValueError(f\"Invalid statistic {center_stat}. Pick mean or median\")\n",
    "            psd = np.log10(psd)\n",
    "            psd = self._calculate_standard_data(psd, mode=mode, axis=-1) # FIXME what is the appropriate axis to average on here? if any?\n",
    "            freq_mask = np.logical_and((freq_range[0] <= freqs), (freqs <= freq_range[1]))\n",
    "            freqs = freqs[freq_mask]\n",
    "            psd = psd[freq_mask, :]\n",
    "            \n",
    "            extent = (0, psd.shape[1] * df_row['duration'].median(), np.min(freqs), np.max(freqs))\n",
    "            # print(psd.nanmin(), psd.nanmax())\n",
    "            norm = matplotlib.colors.Normalize()\n",
    "            # norm = matplotlib.colors.LogNorm()\n",
    "            # norm = matplotlib.colors.CenteredNorm()\n",
    "\n",
    "            \n",
    "            fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "            # ax.pcolormesh(psd, )\n",
    "            axim = ax.imshow(np.flip(psd, axis=0), interpolation='none', aspect='auto', norm=norm, cmap=cmap, extent=extent)\n",
    "            cbar = fig.colorbar(axim, ax=ax)\n",
    "            cbar.set_label(f'log(PSD) {mode}')\n",
    "\n",
    "            ax.set_xlabel(\"Time (s)\")\n",
    "            ax.set_ylabel(\"Frequency (Hz)\")\n",
    "            ax.set_title(i)\n",
    "            plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentPlotter(AnimalFeatureParser):\n",
    "    def __init__(self, wars:list[WindowAnalysisResult], features=['all'], exclude=None, groupby=['animalday', 'animal', 'genotype']) -> None:\n",
    "        self.results: list[WindowAnalysisResult] = wars\n",
    "        self.channel_names: list[list[str]] = [war.channel_names for war in wars]\n",
    "        dftemp = []\n",
    "        for i, war in enumerate(wars):\n",
    "            df = war.get_groupavg_result(features=features, exclude=exclude, groupby=groupby)\n",
    "            df.insert(-1, 'chnames', self.channel_names[i])\n",
    "        self.df_results:pd.DataFrame = pd.concat(dftemp, axis=1)\n",
    "\n",
    "    def plot_boxplots(self, ):\n",
    "        ...\n",
    "\n",
    "    # flatten each column down to scalars, for ease of plotting. Or extract to multiindex column?\n",
    "    # Try proof of concept with a test boxplot\n",
    "    def _get_columnflattened_results(self):\n",
    "        ...\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "note"
    ]
   },
   "outputs": [],
   "source": [
    "%%script true\n",
    "ap = AnimalPlotter(WindowAnalysisResult(war.result, \"WT\", war.channel_names), assume_channels=True)\n",
    "ap.plot_coherecorr_matrix(['isday'], bands=None, figsize=(16,5), cmap='viridis')\n",
    "ap.plot_coherecorr_diff(['isday'], bands=None, figsize=(16,5))\n",
    "# ap.plot_linear_temporal(figsize=(20, 5), score_type='z', lw=1, channels=[0, 1])\n",
    "ap.plot_linear_temporal(['isday'], figsize=(20, 5), score_type='z', lw=1, channels=[0, 1, 2, 3])\n",
    "\n",
    "# ap.plot_coherecorr_spectral(figsize=(20, 5), score_type='center')\n",
    "ap.plot_coherecorr_spectral(['isday'], figsize=(20, 5), score_type='center')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run All Animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "note"
    ]
   },
   "outputs": [],
   "source": [
    "%%script true\n",
    "# animids = ['A5 WT', 'A10 KO', 'F22 KO', 'G25', 'G26', 'N21', 'N22', 'N23 Cage 2', 'N24 Cage 3',]\n",
    "animids = ['N25']\n",
    "wars = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": [
     "note"
    ]
   },
   "outputs": [],
   "source": [
    "%%script true\n",
    "for animid in animids:\n",
    "    ao = AnimalOrganizer(r\"Z:\\PythonEEG Data Bins\", animid, mode=\"concat\")\n",
    "    ao.convert_colbins_to_rowbins(overwrite=False)\n",
    "    ao.convert_rowbins_to_rec()\n",
    "    war = ao.compute_windowed_analysis(['all'], exclude=['nspike', 'wavetemp'])\n",
    "    war.to_pickle_and_json(r'Z:\\PythonEEG\\analysis')\n",
    "    wars.append(war)\n",
    "    try:\n",
    "        ao.cleanup_rec()\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [
     "note"
    ]
   },
   "outputs": [],
   "source": [
    "%%script true\n",
    "\n",
    "ap = AnimalPlotter(wars[0], assume_channels=True)\n",
    "\n",
    "# ap.plot_coherecorr_matrix(['isday'], bands=None, figsize=(16,5), cmap='viridis')\n",
    "# ap.plot_coherecorr_diff(['isday'], bands=None, figsize=(16,5))\n",
    "# ap.plot_linear_temporal(figsize=(20, 5), score_type='z', lw=1, channels=[0, 1, 2, 3], show_endfile=True)\n",
    "# ap.plot_coherecorr_spectral(figsize=(20, 5), score_type='z')\n",
    "ap.plot_coherecorr_spectral(['animalday'], figsize=(20, 5), score_type='z', show_endfile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": [
     "note"
    ]
   },
   "outputs": [],
   "source": [
    "# Load Window Analysis Results from files\n",
    "# warnames = ['WT-A5', 'KO-F22', 'KO-A10', 'WT-G25', 'WT-N21', 'WT-G26']\n",
    "warnames = ['KO-N22', 'KO-N24']\n",
    "# warnames = ['WT-G26', 'WT-N21']\n",
    "# warnames = ['WT-N23', 'WT-N25']\n",
    "reconstruct_war:list[WindowAnalysisResult] = []\n",
    "# del war\n",
    "for i,warname in enumerate(warnames):\n",
    "    # with open(rf\"Z:\\PythonEEG\\analysis\\{warname}.json\", 'r') as f:\n",
    "    with open(rf\"/mnt/isilon/marsh_single_unit/PythonEEG/analysis/{warname}.json\", 'r') as f:\n",
    "        ch_names = json.load(f)\n",
    "        war = WindowAnalysisResult(pd.read_pickle(rf\"/mnt/isilon/marsh_single_unit/PythonEEG/analysis/{warname}.pkl\"), genotype='WT', channel_names=ch_names)\n",
    "    \n",
    "    # war.filter_bad_channel_fragments() # Filtering can be done after the fact\n",
    "    reconstruct_war.append(war)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": [
     "note"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                                                        0\n",
       "animalday                                   N24 KO Feb-07-2024\n",
       "animal                                                     N24\n",
       "day                                        2024-02-07 00:00:00\n",
       "genotype                                                    KO\n",
       "duration                                                   4.0\n",
       "endfile                                                   1.32\n",
       "timestamp                           2024-02-07 16:04:02.400000\n",
       "isday                                                     True\n",
       "rms          [76.2094955444336, 65.19975280761719, 81.84725...\n",
       "ampvar       [5799.49560546875, 4250.6943359375, 6657.67675...\n",
       "psd          ([0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0,...\n",
       "psdtotal     [3953.51708984375, 3317.52197265625, 3376.2128...\n",
       "psdband      {'delta': [1063.90283203125, 959.9813842773438...\n",
       "psdslope     [[-1.441457885625114, 3.17260698451502], [-1.5...\n",
       "cohere       {'delta': [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "pcorr        [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "Name: 0, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(war.result.iloc[0])\n",
    "# filtout = war.filter_all()\n",
    "\n",
    "\n",
    "\n",
    "# war = WindowAnalysisResult(war.result, genotype='WT', channel_names=ch_names)\n",
    "\n",
    "# temp, tfs = war.get_filter_rms_range(z_range=0.5)\n",
    "# temp, tfs2 = war.get_filter_high_rms(temp)\n",
    "# temp, tfs3 = war.get_filter_high_beta(temp)\n",
    "# tfs_all = tfs * tfs2 * tfs3\n",
    "# for i in range(10):\n",
    "#     # print(f\"{tfs.shape[0] - np.count_nonzero(tfs[:, i])}/{tfs.shape[0]}\")\n",
    "#     # print(f\"{tfs2.shape[0] - np.count_nonzero(tfs2[:, i])}/{tfs2.shape[0]}\")\n",
    "#     # print(f\"{tfs3.shape[0] - np.count_nonzero(tfs3[:, i])}/{tfs3.shape[0]}\")\n",
    "#     print(f\"{tfs_all.shape[0] - np.count_nonzero(tfs_all[:, i])}/{tfs_all.shape[0]}\")\n",
    "# filtout = war._apply_filter(tfs_all)\n",
    "# filtout = war.filter_all()\n",
    "\n",
    "# display(filtout.head(2))\n",
    "# display(war.result.head(2))\n",
    "# print(filtout['cohere'].iloc[2])\n",
    "\n",
    "# warfilt = WindowAnalysisResult(filtout, genotype='WT', channel_names=ch_names)\n",
    "# warfilt.get_groupavg_result(['all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "note"
    ]
   },
   "outputs": [],
   "source": [
    "# for war in wars:\n",
    "for war in reconstruct_war:\n",
    "    # war = WindowAnalysisResult()\n",
    "    grs:pd.DataFrame = war.get_grouprows_result(['all'], include=['duration', 'timestamp', 'isday'])\n",
    "    grs = grs.reset_index()\n",
    "    \n",
    "    temp_short_chnames = [war._parse_chname_to_abbrev(x, assume_channels=True) for x in war.channel_names]\n",
    "    outs = []\n",
    "    chfeats = ['rms', 'ampvar', 'psdtotal']\n",
    "    for feat in chfeats:\n",
    "        temp = pd.DataFrame(grs[feat].tolist(), columns=[f'{feat}-{i+1}-{temp_short_chnames[i]}' for i in range(len(war.channel_names))])\n",
    "        outs.append(temp)\n",
    "\n",
    "    temp = pd.DataFrame(grs['psdslope'].apply(lambda x: [y[0] for y in x]).tolist(), columns=[f'psdslope-{i+1}-{temp_short_chnames[i]}' for i in range(len(war.channel_names))])\n",
    "    outs.append(temp)\n",
    "\n",
    "\n",
    "    bandnames = core.LongRecordingAnalyzer.FREQ_BAND_NAMES\n",
    "    temp = pd.DataFrame(grs['psdband'].apply(lambda x: [np.mean(y) for i,y in x.items()]).tolist(), columns=[f'psdband-{bname}' for bname in bandnames])\n",
    "    outs.append(temp)\n",
    "    \n",
    "    triinds = np.tril_indices(len(temp_short_chnames), k=-1)\n",
    "    temp = pd.DataFrame(grs['cohere'].apply(lambda x: [y[triinds[0], triinds[1]].mean() for i,y in x.items()]).tolist(), columns=[f'cohere-{bname}' for bname in bandnames])\n",
    "    outs.append(temp)\n",
    "    temp = pd.DataFrame(grs['pcorr'].apply(lambda x: x[triinds[0], triinds[1]].mean()).tolist(), columns=['pcorr'])\n",
    "    outs.append(temp)\n",
    "\n",
    "    # for x in outs:\n",
    "    #     display(x.head(1))\n",
    "    out = pd.concat(outs, axis=1)\n",
    "    out = pd.concat([grs[['animalday', 'animal', 'genotype', 'duration', 'timestamp', 'isday']], out], axis=1)\n",
    "\n",
    "    display(out.head(1))\n",
    "    with pd.ExcelWriter(rf\"C:\\Users\\dongjp\\Downloads\\{war.genotype}-{war._parse_filename_to_animal(war.animaldays[0])}.xlsx\") as writer:\n",
    "        out.to_excel(writer)\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "note"
    ]
   },
   "outputs": [],
   "source": [
    "ap = AnimalPlotter(war)\n",
    "# print(ap.window_result.result.index.size)\n",
    "\n",
    "# ap.plot_coherecorr_matrix(['isday'], bands=None, figsize=(16,5), cmap='viridis')\n",
    "# ap.plot_coherecorr_diff(['isday'], bands=None, figsize=(16,5))\n",
    "# # ap.plot_linear_temporal(figsize=(20, 5), score_type='z', lw=1, channels=[0, 1])\n",
    "# ap.plot_linear_temporal(['isday'], figsize=(20, 5), score_type='z', lw=1, channels=[0, 1, 2, 3])\n",
    "\n",
    "# # ap.plot_coherecorr_spectral(figsize=(20, 5), score_type='center')\n",
    "# ap.plot_coherecorr_spectral(['isday'], figsize=(20, 5), score_type='center')\n",
    "# ap.plot_psd_histogram(['isday'], figsize=(10, 4), avg_channels=True)\n",
    "ap.plot_psd_spectrogram(['isday'], figsize=(20, 4), mode='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "note"
    ]
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "lan = core.LongRecordingAnalyzer(lrec, 100)\n",
    "print(lan.n_fragments)\n",
    "\n",
    "for i in range(lan.n_fragments):\n",
    "    if i == 10:\n",
    "        break\n",
    "    lan.compute_rms(i)\n",
    "    lan.compute_ampvar(i)\n",
    "    f, psd = lan.compute_psd(i)\n",
    "    lan.compute_psdband(i, f_psd=(f, psd))\n",
    "    lan.compute_psdtotal(i, f_psd=(f, psd))\n",
    "    mb = lan.compute_psdslope(i, f_psd=(f, psd))\n",
    "    lan.compute_cohere(i)\n",
    "    corr = lan.compute_corr(i)\n",
    "    print(corr)\n",
    "\n",
    "    # plt.imshow(corr)\n",
    "    # plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 3))\n",
    "ax.loglog(f, psd)\n",
    "for i, (m,b) in enumerate(mb):\n",
    "    ax.plot(f, 10**(b + m * np.log10(f)), c=f'C{i}')\n",
    "ax.set_ylabel(\"PSD (uV^2/Hz)\")\n",
    "ax.set_xlabel(\"Frequency (Hz)\")\n",
    "ax.axvline(60, c='black', ls='--', alpha=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "note"
    ]
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "# Computationally heavy tests\n",
    "# lan.compute_spikes() \n",
    "# lan.compute_wavetemp()\n",
    "# lan.compute_nspike(10)\n",
    "wt, wts = lan.get_temps_from_wavetemp()\n",
    "\n",
    "for wtch in wts:\n",
    "    if wtch is not None:\n",
    "        print(wtch.shape)\n",
    "    else:\n",
    "        continue\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 1.5))\n",
    "    for i in range(wtch.shape[0]):\n",
    "        ax.plot(wtch[i, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "note"
    ]
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "temp = np.array(war.filter_rms_range(war.result)['rms'].tolist())\n",
    "print(temp)\n",
    "print(np.count_nonzero(np.isnan(temp)))\n",
    "for i in range(10):\n",
    "    hist, bins = np.histogram(temp[:, i], bins=np.linspace(-4, 4, 100), density=True)\n",
    "    plt.hist(temp[:, i], bins=np.linspace(-4, 4, 100), density=True, color=f'C{i}', alpha=0.25)\n",
    "    density = gaussian_kde(temp[~np.isnan(temp[:, i]), i])\n",
    "    # print(x.shape, density(x).shape)\n",
    "    plt.plot(bins, density(bins), c=f'C{i}')\n",
    "    # plt.hist(temp[:, i], )\n",
    "    \n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-win",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
