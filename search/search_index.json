{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Neurodent","text":"<p><code>neurodent</code> is a Python package for standardizing rodent EEG analysis and figure generation. Various EEG formats are loadable and features are extracted in parallel. It also includes a Snakemake workflow for automated analysis.</p>"},{"location":"#usage","title":"Usage","text":"<p><code>notebooks/examples</code> and <code>notebooks/tests</code> contain several usage examples.</p>"},{"location":"reference/contributing/","title":"Contributing","text":"<p># TODO</p>"},{"location":"reference/examples/","title":"Examples","text":"<p># TODO</p>"},{"location":"reference/tutorials/","title":"Tutorials","text":"<p># TODO</p>"},{"location":"reference/utilities/","title":"Utilities","text":""},{"location":"reference/utilities/#neurodent.core.utils.Natural_Neighbor","title":"<code>Natural_Neighbor</code>","text":"<p>               Bases: <code>object</code></p> <p>Natural Neighbor algorithm implementation for finding natural neighbors in a dataset.</p> <p>This class implements the Natural Neighbor algorithm which finds mutual neighbors in a dataset by iteratively expanding the neighborhood radius until convergence.</p> Source code in <code>src/neurodent/core/utils.py</code> <pre><code>class Natural_Neighbor(object):\n    \"\"\"\n    Natural Neighbor algorithm implementation for finding natural neighbors in a dataset.\n\n    This class implements the Natural Neighbor algorithm which finds mutual neighbors\n    in a dataset by iteratively expanding the neighborhood radius until convergence.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize the Natural Neighbor algorithm.\n\n        Attributes:\n            nan_edges (dict): Graph of mutual neighbors\n            nan_num (dict): Number of natural neighbors for each instance\n            repeat (dict): Data structure that counts repetitions of the count method\n            target (list): Set of classes\n            data (list): Set of instances\n            knn (dict): Structure that stores neighbors of each instance\n        \"\"\"\n        self.nan_edges = {}  # Graph of mutual neighbors\n        self.nan_num = {}  # Number of natural neighbors for each instance\n        self.repeat = {}  # Data structure that counts repetitions of the count method\n        self.target = []  # Set of classes\n        self.data = []  # Set of instances\n        self.knn = {}  # Structure that stores neighbors of each instance\n\n    def load(self, filename):\n        \"\"\"\n        Load dataset from a CSV file, separating attributes and classes.\n\n        Args:\n            filename (str): Path to the CSV file containing the dataset\n        \"\"\"\n        aux = []\n        with open(filename, \"r\") as dataset:\n            data = list(csv.reader(dataset))\n            for inst in data:\n                inst_class = inst.pop(-1)\n                self.target.append(inst_class)\n                row = [float(x) for x in inst]\n                aux.append(row)\n        self.data = np.array(aux)\n\n    def read(self, data: np.ndarray):\n        \"\"\"\n        Load data directly from a numpy array.\n\n        Args:\n            data (np.ndarray): Input data array\n        \"\"\"\n        self.data = data\n\n    def asserts(self):\n        \"\"\"\n        Initialize data structures for the algorithm.\n\n        Sets up the necessary data structures including:\n        - nan_edges as an empty set\n        - knn, nan_num, and repeat dictionaries for each instance\n        \"\"\"\n        self.nan_edges = set()\n        for j in range(len(self.data)):\n            self.knn[j] = set()\n            self.nan_num[j] = 0\n            self.repeat[j] = 0\n\n    def count(self):\n        \"\"\"\n        Count the number of instances that have no natural neighbors.\n\n        Returns:\n            int: Number of instances with zero natural neighbors\n        \"\"\"\n        nan_zeros = 0\n        for x in self.nan_num:\n            if self.nan_num[x] == 0:\n                nan_zeros += 1\n        return nan_zeros\n\n    def findKNN(self, inst, r, tree):\n        \"\"\"\n        Find the indices of the k nearest neighbors.\n\n        Args:\n            inst: Instance to find neighbors for\n            r (int): Radius/parameter for neighbor search\n            tree: KDTree object for efficient neighbor search\n\n        Returns:\n            np.ndarray: Array of neighbor indices (excluding the instance itself)\n        \"\"\"\n        _, ind = tree.query([inst], r + 1)\n        return np.delete(ind[0], 0)\n\n    def algorithm(self):\n        \"\"\"\n        Execute the Natural Neighbor algorithm.\n\n        The algorithm iteratively expands the neighborhood radius until convergence,\n        finding mutual neighbors between instances.\n\n        Returns:\n            int: The final radius value when convergence is reached\n        \"\"\"\n        # Initialize KDTree for efficient neighbor search\n        tree = KDTree(self.data)\n        self.asserts()\n        flag = 0\n        r = 1\n\n        while flag == 0:\n            for i in range(len(self.data)):\n                knn = self.findKNN(self.data[i], r, tree)\n                n = knn[-1]\n                self.knn[i].add(n)\n                if i in self.knn[n] and (i, n) not in self.nan_edges:\n                    self.nan_edges.add((i, n))\n                    self.nan_edges.add((n, i))\n                    self.nan_num[i] += 1\n                    self.nan_num[n] += 1\n\n            cnt = self.count()\n            rep = self.repeat[cnt]\n            self.repeat[cnt] += 1\n            if cnt == 0 or rep &gt;= math.sqrt(r - rep):\n                flag = 1\n            else:\n                r += 1\n        return r\n</code></pre>"},{"location":"reference/utilities/#neurodent.core.utils.Natural_Neighbor.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the Natural Neighbor algorithm.</p> <p>Attributes:</p> Name Type Description <code>nan_edges</code> <code>dict</code> <p>Graph of mutual neighbors</p> <code>nan_num</code> <code>dict</code> <p>Number of natural neighbors for each instance</p> <code>repeat</code> <code>dict</code> <p>Data structure that counts repetitions of the count method</p> <code>target</code> <code>list</code> <p>Set of classes</p> <code>data</code> <code>list</code> <p>Set of instances</p> <code>knn</code> <code>dict</code> <p>Structure that stores neighbors of each instance</p> Source code in <code>src/neurodent/core/utils.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initialize the Natural Neighbor algorithm.\n\n    Attributes:\n        nan_edges (dict): Graph of mutual neighbors\n        nan_num (dict): Number of natural neighbors for each instance\n        repeat (dict): Data structure that counts repetitions of the count method\n        target (list): Set of classes\n        data (list): Set of instances\n        knn (dict): Structure that stores neighbors of each instance\n    \"\"\"\n    self.nan_edges = {}  # Graph of mutual neighbors\n    self.nan_num = {}  # Number of natural neighbors for each instance\n    self.repeat = {}  # Data structure that counts repetitions of the count method\n    self.target = []  # Set of classes\n    self.data = []  # Set of instances\n    self.knn = {}  # Structure that stores neighbors of each instance\n</code></pre>"},{"location":"reference/utilities/#neurodent.core.utils.Natural_Neighbor.algorithm","title":"<code>algorithm()</code>","text":"<p>Execute the Natural Neighbor algorithm.</p> <p>The algorithm iteratively expands the neighborhood radius until convergence, finding mutual neighbors between instances.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>The final radius value when convergence is reached</p> Source code in <code>src/neurodent/core/utils.py</code> <pre><code>def algorithm(self):\n    \"\"\"\n    Execute the Natural Neighbor algorithm.\n\n    The algorithm iteratively expands the neighborhood radius until convergence,\n    finding mutual neighbors between instances.\n\n    Returns:\n        int: The final radius value when convergence is reached\n    \"\"\"\n    # Initialize KDTree for efficient neighbor search\n    tree = KDTree(self.data)\n    self.asserts()\n    flag = 0\n    r = 1\n\n    while flag == 0:\n        for i in range(len(self.data)):\n            knn = self.findKNN(self.data[i], r, tree)\n            n = knn[-1]\n            self.knn[i].add(n)\n            if i in self.knn[n] and (i, n) not in self.nan_edges:\n                self.nan_edges.add((i, n))\n                self.nan_edges.add((n, i))\n                self.nan_num[i] += 1\n                self.nan_num[n] += 1\n\n        cnt = self.count()\n        rep = self.repeat[cnt]\n        self.repeat[cnt] += 1\n        if cnt == 0 or rep &gt;= math.sqrt(r - rep):\n            flag = 1\n        else:\n            r += 1\n    return r\n</code></pre>"},{"location":"reference/utilities/#neurodent.core.utils.Natural_Neighbor.asserts","title":"<code>asserts()</code>","text":"<p>Initialize data structures for the algorithm.</p> <p>Sets up the necessary data structures including: - nan_edges as an empty set - knn, nan_num, and repeat dictionaries for each instance</p> Source code in <code>src/neurodent/core/utils.py</code> <pre><code>def asserts(self):\n    \"\"\"\n    Initialize data structures for the algorithm.\n\n    Sets up the necessary data structures including:\n    - nan_edges as an empty set\n    - knn, nan_num, and repeat dictionaries for each instance\n    \"\"\"\n    self.nan_edges = set()\n    for j in range(len(self.data)):\n        self.knn[j] = set()\n        self.nan_num[j] = 0\n        self.repeat[j] = 0\n</code></pre>"},{"location":"reference/utilities/#neurodent.core.utils.Natural_Neighbor.count","title":"<code>count()</code>","text":"<p>Count the number of instances that have no natural neighbors.</p> <p>Returns:</p> Name Type Description <code>int</code> <p>Number of instances with zero natural neighbors</p> Source code in <code>src/neurodent/core/utils.py</code> <pre><code>def count(self):\n    \"\"\"\n    Count the number of instances that have no natural neighbors.\n\n    Returns:\n        int: Number of instances with zero natural neighbors\n    \"\"\"\n    nan_zeros = 0\n    for x in self.nan_num:\n        if self.nan_num[x] == 0:\n            nan_zeros += 1\n    return nan_zeros\n</code></pre>"},{"location":"reference/utilities/#neurodent.core.utils.Natural_Neighbor.findKNN","title":"<code>findKNN(inst, r, tree)</code>","text":"<p>Find the indices of the k nearest neighbors.</p> <p>Parameters:</p> Name Type Description Default <code>inst</code> <p>Instance to find neighbors for</p> required <code>r</code> <code>int</code> <p>Radius/parameter for neighbor search</p> required <code>tree</code> <p>KDTree object for efficient neighbor search</p> required <p>Returns:</p> Type Description <p>np.ndarray: Array of neighbor indices (excluding the instance itself)</p> Source code in <code>src/neurodent/core/utils.py</code> <pre><code>def findKNN(self, inst, r, tree):\n    \"\"\"\n    Find the indices of the k nearest neighbors.\n\n    Args:\n        inst: Instance to find neighbors for\n        r (int): Radius/parameter for neighbor search\n        tree: KDTree object for efficient neighbor search\n\n    Returns:\n        np.ndarray: Array of neighbor indices (excluding the instance itself)\n    \"\"\"\n    _, ind = tree.query([inst], r + 1)\n    return np.delete(ind[0], 0)\n</code></pre>"},{"location":"reference/utilities/#neurodent.core.utils.Natural_Neighbor.load","title":"<code>load(filename)</code>","text":"<p>Load dataset from a CSV file, separating attributes and classes.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to the CSV file containing the dataset</p> required Source code in <code>src/neurodent/core/utils.py</code> <pre><code>def load(self, filename):\n    \"\"\"\n    Load dataset from a CSV file, separating attributes and classes.\n\n    Args:\n        filename (str): Path to the CSV file containing the dataset\n    \"\"\"\n    aux = []\n    with open(filename, \"r\") as dataset:\n        data = list(csv.reader(dataset))\n        for inst in data:\n            inst_class = inst.pop(-1)\n            self.target.append(inst_class)\n            row = [float(x) for x in inst]\n            aux.append(row)\n    self.data = np.array(aux)\n</code></pre>"},{"location":"reference/utilities/#neurodent.core.utils.Natural_Neighbor.read","title":"<code>read(data)</code>","text":"<p>Load data directly from a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input data array</p> required Source code in <code>src/neurodent/core/utils.py</code> <pre><code>def read(self, data: np.ndarray):\n    \"\"\"\n    Load data directly from a numpy array.\n\n    Args:\n        data (np.ndarray): Input data array\n    \"\"\"\n    self.data = data\n</code></pre>"},{"location":"reference/utilities/#neurodent.core.utils.TimestampMapper","title":"<code>TimestampMapper</code>","text":"<p>Map each fragment to its source file's timestamp.</p> <p>This class provides functionality to map data fragments back to their original file timestamps when data has been concatenated from multiple files with different recording times.</p> <p>Attributes:</p> Name Type Description <code>file_end_datetimes</code> <code>list[datetime]</code> <p>The end datetimes of each source file.</p> <code>file_durations</code> <code>list[float]</code> <p>The durations of each source file in seconds.</p> <code>file_start_datetimes</code> <code>list[datetime]</code> <p>Computed start datetimes of each file.</p> <code>cumulative_durations</code> <code>ndarray</code> <p>Cumulative sum of file durations.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from datetime import datetime, timedelta\n&gt;&gt;&gt; # Set up files with known end times and durations\n&gt;&gt;&gt; end_times = [datetime(2023, 1, 1, 12, 0), datetime(2023, 1, 1, 13, 0)]\n&gt;&gt;&gt; durations = [3600.0, 1800.0]  # 1 hour, 30 minutes\n&gt;&gt;&gt; mapper = TimestampMapper(end_times, durations)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Get timestamp for fragment at index 2 with 60s fragments\n&gt;&gt;&gt; timestamp = mapper.get_fragment_timestamp(2, 60.0)\n&gt;&gt;&gt; print(timestamp)\n2023-01-01 11:02:00\n</code></pre> Source code in <code>src/neurodent/core/utils.py</code> <pre><code>class TimestampMapper:\n    \"\"\"\n    Map each fragment to its source file's timestamp.\n\n    This class provides functionality to map data fragments back to their original\n    file timestamps when data has been concatenated from multiple files with\n    different recording times.\n\n    Attributes:\n        file_end_datetimes (list[datetime]): The end datetimes of each source file.\n        file_durations (list[float]): The durations of each source file in seconds.\n        file_start_datetimes (list[datetime]): Computed start datetimes of each file.\n        cumulative_durations (np.ndarray): Cumulative sum of file durations.\n\n    Examples:\n        &gt;&gt;&gt; from datetime import datetime, timedelta\n        &gt;&gt;&gt; # Set up files with known end times and durations\n        &gt;&gt;&gt; end_times = [datetime(2023, 1, 1, 12, 0), datetime(2023, 1, 1, 13, 0)]\n        &gt;&gt;&gt; durations = [3600.0, 1800.0]  # 1 hour, 30 minutes\n        &gt;&gt;&gt; mapper = TimestampMapper(end_times, durations)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Get timestamp for fragment at index 2 with 60s fragments\n        &gt;&gt;&gt; timestamp = mapper.get_fragment_timestamp(2, 60.0)\n        &gt;&gt;&gt; print(timestamp)\n        2023-01-01 11:02:00\n    \"\"\"\n\n    def __init__(self, file_end_datetimes: list[datetime], file_durations: list[float]):\n        \"\"\"\n        Initialize the TimestampMapper.\n\n        Args:\n            file_end_datetimes (list[datetime]): The end datetimes of each file.\n            file_durations (list[float]): The durations of each file in seconds.\n\n        Raises:\n            ValueError: If the lengths of file_end_datetimes and file_durations don't match.\n        \"\"\"\n        if len(file_end_datetimes) != len(file_durations):\n            raise ValueError(\"file_end_datetimes and file_durations must have the same length\")\n\n        self.file_end_datetimes = file_end_datetimes\n        self.file_durations = file_durations\n\n        self.file_start_datetimes = [\n            file_end_datetime - timedelta(seconds=file_duration)\n            for file_end_datetime, file_duration in zip(self.file_end_datetimes, self.file_durations)\n        ]\n        self.cumulative_durations = np.cumsum(self.file_durations)\n\n    def get_fragment_timestamp(self, fragment_idx: int, fragment_len_s: float) -&gt; datetime:\n        \"\"\"\n        Get the timestamp for a specific fragment based on its index and length.\n\n        Args:\n            fragment_idx (int): The index of the fragment (0-based).\n            fragment_len_s (float): The length of each fragment in seconds.\n\n        Returns:\n            datetime: The timestamp corresponding to the start of the specified fragment.\n\n        Examples:\n            &gt;&gt;&gt; # Get timestamp for the 5th fragment (index 4) with 30-second fragments\n            &gt;&gt;&gt; timestamp = mapper.get_fragment_timestamp(4, 30.0)\n            &gt;&gt;&gt; # This returns the timestamp 2 minutes into the first file\n        \"\"\"\n        # Find which file this fragment belongs to\n        fragment_start_time = fragment_idx * fragment_len_s\n        file_idx = np.searchsorted(self.cumulative_durations, fragment_start_time)\n        file_idx = min(file_idx, len(self.cumulative_durations) - 1)\n\n        offset_in_file = fragment_start_time - self.cumulative_durations[file_idx]  # Negative\n\n        # Return actual timestamp + offset\n        return self.file_end_datetimes[file_idx] + timedelta(seconds=offset_in_file)\n</code></pre>"},{"location":"reference/utilities/#neurodent.core.utils.TimestampMapper.__init__","title":"<code>__init__(file_end_datetimes, file_durations)</code>","text":"<p>Initialize the TimestampMapper.</p> <p>Parameters:</p> Name Type Description Default <code>file_end_datetimes</code> <code>list[datetime]</code> <p>The end datetimes of each file.</p> required <code>file_durations</code> <code>list[float]</code> <p>The durations of each file in seconds.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the lengths of file_end_datetimes and file_durations don't match.</p> Source code in <code>src/neurodent/core/utils.py</code> <pre><code>def __init__(self, file_end_datetimes: list[datetime], file_durations: list[float]):\n    \"\"\"\n    Initialize the TimestampMapper.\n\n    Args:\n        file_end_datetimes (list[datetime]): The end datetimes of each file.\n        file_durations (list[float]): The durations of each file in seconds.\n\n    Raises:\n        ValueError: If the lengths of file_end_datetimes and file_durations don't match.\n    \"\"\"\n    if len(file_end_datetimes) != len(file_durations):\n        raise ValueError(\"file_end_datetimes and file_durations must have the same length\")\n\n    self.file_end_datetimes = file_end_datetimes\n    self.file_durations = file_durations\n\n    self.file_start_datetimes = [\n        file_end_datetime - timedelta(seconds=file_duration)\n        for file_end_datetime, file_duration in zip(self.file_end_datetimes, self.file_durations)\n    ]\n    self.cumulative_durations = np.cumsum(self.file_durations)\n</code></pre>"},{"location":"reference/utilities/#neurodent.core.utils.TimestampMapper.get_fragment_timestamp","title":"<code>get_fragment_timestamp(fragment_idx, fragment_len_s)</code>","text":"<p>Get the timestamp for a specific fragment based on its index and length.</p> <p>Parameters:</p> Name Type Description Default <code>fragment_idx</code> <code>int</code> <p>The index of the fragment (0-based).</p> required <code>fragment_len_s</code> <code>float</code> <p>The length of each fragment in seconds.</p> required <p>Returns:</p> Name Type Description <code>datetime</code> <code>datetime</code> <p>The timestamp corresponding to the start of the specified fragment.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Get timestamp for the 5th fragment (index 4) with 30-second fragments\n&gt;&gt;&gt; timestamp = mapper.get_fragment_timestamp(4, 30.0)\n&gt;&gt;&gt; # This returns the timestamp 2 minutes into the first file\n</code></pre> Source code in <code>src/neurodent/core/utils.py</code> <pre><code>def get_fragment_timestamp(self, fragment_idx: int, fragment_len_s: float) -&gt; datetime:\n    \"\"\"\n    Get the timestamp for a specific fragment based on its index and length.\n\n    Args:\n        fragment_idx (int): The index of the fragment (0-based).\n        fragment_len_s (float): The length of each fragment in seconds.\n\n    Returns:\n        datetime: The timestamp corresponding to the start of the specified fragment.\n\n    Examples:\n        &gt;&gt;&gt; # Get timestamp for the 5th fragment (index 4) with 30-second fragments\n        &gt;&gt;&gt; timestamp = mapper.get_fragment_timestamp(4, 30.0)\n        &gt;&gt;&gt; # This returns the timestamp 2 minutes into the first file\n    \"\"\"\n    # Find which file this fragment belongs to\n    fragment_start_time = fragment_idx * fragment_len_s\n    file_idx = np.searchsorted(self.cumulative_durations, fragment_start_time)\n    file_idx = min(file_idx, len(self.cumulative_durations) - 1)\n\n    offset_in_file = fragment_start_time - self.cumulative_durations[file_idx]  # Negative\n\n    # Return actual timestamp + offset\n    return self.file_end_datetimes[file_idx] + timedelta(seconds=offset_in_file)\n</code></pre>"},{"location":"reference/utilities/#neurodent.core.utils.cache_fragments_to_zarr","title":"<code>cache_fragments_to_zarr(np_fragments, n_fragments, tmpdir=None)</code>","text":"<p>Cache numpy fragments array to zarr format for efficient memory management.</p> <p>This function converts a numpy array of recording fragments to a zarr array stored in a temporary location. This allows better memory management and garbage collection by avoiding keeping large numpy arrays in memory for extended periods.</p> <p>Parameters:</p> Name Type Description Default <code>np_fragments</code> <code>ndarray</code> <p>Numpy array of shape (n_fragments, n_samples, n_channels) containing the recording fragments to cache.</p> required <code>n_fragments</code> <code>int</code> <p>Number of fragments to cache (allows for subset caching).</p> required <code>tmpdir</code> <code>str</code> <p>Directory path for temporary zarr storage. If None, uses get_temp_directory(). Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[str, Array]</code> <p>tuple[str, zarr.Array]: A tuple containing: - str: Path to the temporary zarr file - zarr.Array: The zarr array object for accessing cached data</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If zarr is not available</p> Source code in <code>src/neurodent/core/utils.py</code> <pre><code>def cache_fragments_to_zarr(\n    np_fragments: np.ndarray, n_fragments: int, tmpdir: Optional[str] = None\n) -&gt; tuple[str, \"zarr.Array\"]:\n    \"\"\"\n    Cache numpy fragments array to zarr format for efficient memory management.\n\n    This function converts a numpy array of recording fragments to a zarr array stored\n    in a temporary location. This allows better memory management and garbage collection\n    by avoiding keeping large numpy arrays in memory for extended periods.\n\n    Args:\n        np_fragments (np.ndarray): Numpy array of shape (n_fragments, n_samples, n_channels)\n            containing the recording fragments to cache.\n        n_fragments (int): Number of fragments to cache (allows for subset caching).\n        tmpdir (str, optional): Directory path for temporary zarr storage. If None,\n            uses get_temp_directory(). Defaults to None.\n\n    Returns:\n        tuple[str, zarr.Array]: A tuple containing:\n            - str: Path to the temporary zarr file\n            - zarr.Array: The zarr array object for accessing cached data\n\n    Raises:\n        ImportError: If zarr is not available\n    \"\"\"\n    try:\n        import zarr\n    except ImportError:\n        raise ImportError(\"zarr package is required for fragment caching\")\n\n    if tmpdir is None:\n        tmpdir = get_temp_directory()\n\n    # Generate unique temporary path\n    tmppath = os.path.join(tmpdir, f\"temp_{os.urandom(24).hex()}.zarr\")\n\n    logging.debug(f\"Caching numpy array with zarr in {tmppath}\")\n\n    # Create Zarr array with optimal settings for fragment-wise access\n    chunk_size = min(100, n_fragments)  # Cap at 100 fragments per chunk\n    zarr_array = zarr.open(\n        tmppath,\n        mode=\"w\",\n        shape=np_fragments.shape,\n        chunks=(\n            chunk_size,\n            -1,  # No chunking along timestamp dimension\n            -1,  # No chunking along channel dimension\n        ),\n        dtype=np_fragments.dtype,\n        compressor=zarr.Blosc(cname=\"lz4\", clevel=3, shuffle=zarr.Blosc.SHUFFLE),  # Fast compression\n    )\n    zarr_array[:n_fragments] = np_fragments[:n_fragments]\n\n    # Log debug properties of the zarr array\n    total_memory_bytes = zarr_array.nbytes\n    total_memory_mb = total_memory_bytes / (1024 * 1024)\n    total_memory_gb = total_memory_mb / 1024\n\n    logging.debug(f\"  - Total memory footprint: {total_memory_mb:.2f} MB, {total_memory_gb:.3f} GB\")\n    logging.debug(f\"  - Zarr array shape: {zarr_array.shape}\")\n    logging.debug(f\"  - Zarr array chunks: {zarr_array.chunks}\")\n\n    return tmppath, zarr_array\n</code></pre>"},{"location":"reference/utilities/#neurodent.core.utils.convert_colpath_to_rowpath","title":"<code>convert_colpath_to_rowpath(rowdir_path, col_path, gzip=True, aspath=True)</code>","text":"<p>Convert a ColMajor file path to its corresponding RowMajor file path.</p> <p>This function transforms file paths from column-major format to row-major format, which is used when converting between different data storage layouts in Neurodent.</p> <p>Parameters:</p> Name Type Description Default <code>rowdir_path</code> <code>str | Path</code> <p>Directory path where the RowMajor file should be located.</p> required <code>col_path</code> <code>str | Path</code> <p>Path to the ColMajor file to be converted. Must contain 'ColMajor' in the path.</p> required <code>gzip</code> <code>bool</code> <p>If True, append '.npy.gz' extension. If False, append '.bin'. Defaults to True.</p> <code>True</code> <code>aspath</code> <code>bool</code> <p>If True, return as Path object. If False, return as string. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>str | Path</code> <p>str | Path: The converted RowMajor file path, either as string or Path object based on aspath parameter.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If 'ColMajor' is not found in col_path.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; convert_colpath_to_rowpath(\"/data/row/\", \"/data/col/file_ColMajor_001.bin\")\nPosixPath('/data/row/file_RowMajor_001.npy.gz')\n&gt;&gt;&gt; convert_colpath_to_rowpath(\"/data/row/\", \"/data/col/file_ColMajor_001.bin\", gzip=False)\nPosixPath('/data/row/file_RowMajor_001.bin')\n&gt;&gt;&gt; convert_colpath_to_rowpath(\"/data/row/\", \"/data/col/file_ColMajor_001.bin\", aspath=False)\n'/data/row/file_RowMajor_001.npy.gz'\n</code></pre> Source code in <code>src/neurodent/core/utils.py</code> <pre><code>def convert_colpath_to_rowpath(\n    rowdir_path: str | Path, col_path: str | Path, gzip: bool = True, aspath: bool = True\n) -&gt; str | Path:\n    \"\"\"\n    Convert a ColMajor file path to its corresponding RowMajor file path.\n\n    This function transforms file paths from column-major format to row-major format,\n    which is used when converting between different data storage layouts in Neurodent.\n\n    Args:\n        rowdir_path (str | Path): Directory path where the RowMajor file should be located.\n        col_path (str | Path): Path to the ColMajor file to be converted. Must contain 'ColMajor' in the path.\n        gzip (bool, optional): If True, append '.npy.gz' extension. If False, append '.bin'. Defaults to True.\n        aspath (bool, optional): If True, return as Path object. If False, return as string. Defaults to True.\n\n    Returns:\n        str | Path: The converted RowMajor file path, either as string or Path object based on aspath parameter.\n\n    Raises:\n        ValueError: If 'ColMajor' is not found in col_path.\n\n    Examples:\n        &gt;&gt;&gt; convert_colpath_to_rowpath(\"/data/row/\", \"/data/col/file_ColMajor_001.bin\")\n        PosixPath('/data/row/file_RowMajor_001.npy.gz')\n        &gt;&gt;&gt; convert_colpath_to_rowpath(\"/data/row/\", \"/data/col/file_ColMajor_001.bin\", gzip=False)\n        PosixPath('/data/row/file_RowMajor_001.bin')\n        &gt;&gt;&gt; convert_colpath_to_rowpath(\"/data/row/\", \"/data/col/file_ColMajor_001.bin\", aspath=False)\n        '/data/row/file_RowMajor_001.npy.gz'\n    \"\"\"\n    # TODO it would make more sense to not have a rowdir_path aparameter, since this is outside the scope of the function\n    if \"ColMajor\" not in col_path:\n        raise ValueError(f\"Expected 'ColMajor' in col_path: {col_path}\")\n\n    out = Path(rowdir_path) / f\"{get_file_stem(Path(col_path).name).replace('ColMajor', 'RowMajor')}\"\n    if gzip:\n        out = str(out) + \".npy.gz\"\n    else:\n        out = str(out) + \".bin\"\n    return Path(out) if aspath else out\n</code></pre>"},{"location":"reference/utilities/#neurodent.core.utils.convert_units_to_multiplier","title":"<code>convert_units_to_multiplier(current_units, target_units='\u00b5V')</code>","text":"<p>Convert between different voltage units and return the multiplication factor.</p> <p>This function calculates the conversion factor needed to transform values from one voltage unit to another (e.g., from mV to \u00b5V).</p> <p>Parameters:</p> Name Type Description Default <code>current_units</code> <code>str</code> <p>The current unit of the values. Must be one of: '\u00b5V', 'mV', 'V', 'nV'.</p> required <code>target_units</code> <code>str</code> <p>The target unit to convert to. Defaults to '\u00b5V'. Must be one of: '\u00b5V', 'mV', 'V', 'nV'.</p> <code>'\u00b5V'</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The multiplication factor to convert from current_units to target_units. To convert values, multiply your data by this factor.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If current_units or target_units are not supported.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; convert_units_to_multiplier(\"mV\", \"\u00b5V\")\n1000.0\n&gt;&gt;&gt; convert_units_to_multiplier(\"V\", \"mV\")\n1000.0\n&gt;&gt;&gt; convert_units_to_multiplier(\"\u00b5V\", \"V\")\n1e-06\n</code></pre> Source code in <code>src/neurodent/core/utils.py</code> <pre><code>def convert_units_to_multiplier(current_units: str, target_units: str = \"\u00b5V\") -&gt; float:\n    \"\"\"\n    Convert between different voltage units and return the multiplication factor.\n\n    This function calculates the conversion factor needed to transform values\n    from one voltage unit to another (e.g., from mV to \u00b5V).\n\n    Args:\n        current_units (str): The current unit of the values. Must be one of: '\u00b5V', 'mV', 'V', 'nV'.\n        target_units (str, optional): The target unit to convert to. Defaults to '\u00b5V'.\n            Must be one of: '\u00b5V', 'mV', 'V', 'nV'.\n\n    Returns:\n        float: The multiplication factor to convert from current_units to target_units.\n            To convert values, multiply your data by this factor.\n\n    Raises:\n        AssertionError: If current_units or target_units are not supported.\n\n    Examples:\n        &gt;&gt;&gt; convert_units_to_multiplier(\"mV\", \"\u00b5V\")\n        1000.0\n        &gt;&gt;&gt; convert_units_to_multiplier(\"V\", \"mV\")\n        1000.0\n        &gt;&gt;&gt; convert_units_to_multiplier(\"\u00b5V\", \"V\")\n        1e-06\n    \"\"\"\n    units_to_mult = {\"\u00b5V\": 1e-6, \"mV\": 1e-3, \"V\": 1, \"nV\": 1e-9}\n\n    assert current_units in units_to_mult.keys(), f\"No valid current unit called '{current_units}' found\"\n    assert target_units in units_to_mult.keys(), f\"No valid target unit called '{target_units}' found\"\n\n    return units_to_mult[current_units] / units_to_mult[target_units]\n</code></pre>"},{"location":"reference/utilities/#neurodent.core.utils.extract_mne_unit_info","title":"<code>extract_mne_unit_info(raw_info)</code>","text":"<p>Extract unit information from MNE Raw info object.</p> <p>Parameters:</p> Name Type Description Default <code>raw_info</code> <code>dict</code> <p>MNE Raw.info object containing channel information</p> required <p>Returns:</p> Type Description <code>tuple[str | None, float | None]</code> <p>tuple[str | None, float | None]: (unit_name, mult_to_uV) where unit_name                             is the consistent unit across all channels                             and mult_to_uV is the conversion factor to \u00b5V</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If channel units are inconsistent across channels</p> Source code in <code>src/neurodent/core/utils.py</code> <pre><code>def extract_mne_unit_info(raw_info: dict) -&gt; tuple[str | None, float | None]:\n    \"\"\"Extract unit information from MNE Raw info object.\n\n    Args:\n        raw_info (dict): MNE Raw.info object containing channel information\n\n    Returns:\n        tuple[str | None, float | None]: (unit_name, mult_to_uV) where unit_name\n                                        is the consistent unit across all channels\n                                        and mult_to_uV is the conversion factor to \u00b5V\n\n    Raises:\n        ValueError: If channel units are inconsistent across channels\n    \"\"\"\n    if mne is None or FIFF is None:\n        logging.warning(\"MNE not available, cannot extract unit information\")\n        return None, None\n\n    if \"chs\" not in raw_info or not raw_info[\"chs\"]:\n        logging.warning(\"No channel information found in MNE Raw.info, using default units\")\n        return None, None\n\n    # Extract unit information from all channels\n    channel_units = []\n    unit_muls = []\n\n    for ch_info in raw_info[\"chs\"]:\n        ch_name = ch_info.get(\"ch_name\", \"unknown\")\n        unit = ch_info.get(\"unit\", None)\n        unit_mul = ch_info.get(\"unit_mul\", None)\n\n        if unit is not None and unit_mul is not None:\n            channel_units.append((ch_name, unit, unit_mul))\n            unit_muls.append(unit_mul)\n\n    if not channel_units:\n        logging.warning(\"No unit information found in any channels, using default units\")\n        return None, None\n\n    # Check for consistency in unit values\n    unique_units = set(unit for _, unit, _ in channel_units)\n    unique_unit_muls = set(unit_muls)\n\n    if len(unique_units) &gt; 1:\n        unit_details = [(ch, unit, mul) for ch, unit, mul in channel_units]\n        raise ValueError(\n            f\"Inconsistent units across channels. Found different unit values: {unique_units}. \"\n            f\"Channel details: {unit_details}\"\n        )\n\n    if len(unique_unit_muls) &gt; 1:\n        unit_details = [(ch, unit, mul) for ch, unit, mul in channel_units]\n        raise ValueError(\n            f\"Inconsistent unit multipliers across channels. Found different unit_mul values: {unique_unit_muls}. \"\n            f\"Channel details: {unit_details}\"\n        )\n\n    # Get the consistent unit values\n    unit_code = list(unique_units)[0]\n    unit_mul = list(unique_unit_muls)[0]\n\n    # Convert MNE unit codes to string representation using FIFF constants\n    # Based on MNE FIFF constants documentation\n    unit_str = None\n    if hasattr(FIFF, \"FIFF_UNIT_V\") and unit_code == FIFF.FIFF_UNIT_V:\n        unit_str = \"V\"\n    elif hasattr(FIFF, \"FIFF_UNIT_T\") and unit_code == FIFF.FIFF_UNIT_T:\n        unit_str = \"T\"  # Tesla - MEG magnetometer\n    elif hasattr(FIFF, \"FIFF_UNIT_T_M\") and unit_code == FIFF.FIFF_UNIT_T_M:\n        unit_str = \"T/m\"  # Tesla/meter - MEG gradiometer\n    else:\n        logging.warning(f\"Unknown MNE unit code {unit_code}, using default units\")\n        return None, None\n\n    # Convert unit multipliers using FIFF constants\n    multiplier = None\n    if hasattr(FIFF, \"FIFF_UNITM_NONE\") and unit_mul == FIFF.FIFF_UNITM_NONE:\n        multiplier = 1.0\n    elif hasattr(FIFF, \"FIFF_UNITM_MU\") and unit_mul == FIFF.FIFF_UNITM_MU:\n        multiplier = 1e-6  # micro\n    elif hasattr(FIFF, \"FIFF_UNITM_M\") and unit_mul == FIFF.FIFF_UNITM_M:\n        multiplier = 1e-3  # milli\n    elif hasattr(FIFF, \"FIFF_UNITM_N\") and unit_mul == FIFF.FIFF_UNITM_N:\n        multiplier = 1e-9  # nano\n    elif hasattr(FIFF, \"FIFF_UNITM_P\") and unit_mul == FIFF.FIFF_UNITM_P:\n        multiplier = 1e-12  # pico\n    elif hasattr(FIFF, \"FIFF_UNITM_F\") and unit_mul == FIFF.FIFF_UNITM_F:\n        multiplier = 1e-15  # femto\n    else:\n        # Fallback to numerical interpretation if FIFF constants not available\n        mul_mapping = {\n            0: 1.0,  # FIFF_UNITM_NONE\n            -3: 1e-3,  # FIFF_UNITM_M (milli)\n            -6: 1e-6,  # FIFF_UNITM_MU (micro)\n            -9: 1e-9,  # FIFF_UNITM_N (nano)\n            -12: 1e-12,  # FIFF_UNITM_P (pico)\n            -15: 1e-15,  # FIFF_UNITM_F (femto)\n        }\n        multiplier = mul_mapping.get(unit_mul)\n        if multiplier is None:\n            logging.warning(f\"Unknown MNE unit multiplier {unit_mul}, using default units\")\n            return None, None\n\n    # For EEG data (voltage units), compute the final unit and conversion factor\n    if unit_str == \"V\":\n        # Apply the MNE multiplier to get the actual unit\n        if multiplier == 1e-6:\n            final_unit = \"\u00b5V\"\n        elif multiplier == 1e-3:\n            final_unit = \"mV\"\n        elif multiplier == 1.0:\n            final_unit = \"V\"\n        elif multiplier == 1e-9:\n            final_unit = \"nV\"\n        else:\n            logging.warning(f\"Unusual voltage unit multiplier {multiplier}, treating as V\")\n            final_unit = \"V\"\n\n        # Convert to \u00b5V multiplier using existing utility\n        try:\n            mult_to_uV = convert_units_to_multiplier(final_unit, \"\u00b5V\")\n            logging.info(f\"Extracted MNE units: {final_unit} -&gt; mult_to_uV = {mult_to_uV}\")\n            return final_unit, mult_to_uV\n        except (ValueError, AssertionError) as e:\n            logging.warning(f\"Failed to convert units {final_unit}: {e}\")\n            return None, None\n    else:\n        # Non-voltage units (MEG, etc.) - don't convert to \u00b5V\n        logging.info(f\"Non-voltage units detected: {unit_str}, not converting to \u00b5V\")\n        return None, None\n</code></pre>"},{"location":"reference/utilities/#neurodent.core.utils.filepath_to_index","title":"<code>filepath_to_index(filepath)</code>","text":"<p>Extract the index number from a filepath.</p> <p>This function extracts the last number found in a filepath after removing common suffixes and file extensions. For example, from \"/path/to/data_ColMajor_001.bin\" it returns 1.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p>Path to the file to extract index from.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The extracted index number.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; filepath_to_index(\"/path/to/data_ColMajor_001.bin\")\n1\n&gt;&gt;&gt; filepath_to_index(\"/path/to/data_2023_015_ColMajor.bin\")\n15\n&gt;&gt;&gt; filepath_to_index(\"/path/to/data_Meta_010.json\")\n10\n</code></pre> Source code in <code>src/neurodent/core/utils.py</code> <pre><code>def filepath_to_index(filepath) -&gt; int:\n    \"\"\"\n    Extract the index number from a filepath.\n\n    This function extracts the last number found in a filepath after removing common suffixes\n    and file extensions. For example, from \"/path/to/data_ColMajor_001.bin\" it returns 1.\n\n    Args:\n        filepath (str | Path): Path to the file to extract index from.\n\n    Returns:\n        int: The extracted index number.\n\n    Examples:\n        &gt;&gt;&gt; filepath_to_index(\"/path/to/data_ColMajor_001.bin\")\n        1\n        &gt;&gt;&gt; filepath_to_index(\"/path/to/data_2023_015_ColMajor.bin\")\n        15\n        &gt;&gt;&gt; filepath_to_index(\"/path/to/data_Meta_010.json\")\n        10\n    \"\"\"\n    fpath = str(filepath)\n    for suffix in [\"_RowMajor\", \"_ColMajor\", \"_Meta\"]:\n        fpath = fpath.replace(suffix, \"\")\n\n    # Remove only the actual file extension, not dots within the filename\n    path_obj = Path(fpath)\n    if path_obj.suffix:\n        fpath = str(path_obj.with_suffix(\"\"))\n\n    fname = Path(fpath).name\n    fname = re.split(r\"\\D+\", fname)\n    fname = list(filter(None, fname))\n    return int(fname[-1])\n</code></pre>"},{"location":"reference/utilities/#neurodent.core.utils.get_cache_status_message","title":"<code>get_cache_status_message(cache_path, use_cached)</code>","text":"<p>Generate a descriptive message about cache usage for logging.</p> Source code in <code>src/neurodent/core/utils.py</code> <pre><code>def get_cache_status_message(cache_path: Union[str, Path], use_cached: bool) -&gt; str:\n    \"\"\"Generate a descriptive message about cache usage for logging.\"\"\"\n    cache_path = Path(cache_path)\n\n    if use_cached:\n        return f\"Using cached intermediate: {cache_path.name}\"\n    else:\n        return f\"Regenerating intermediate: {cache_path.name}\"\n</code></pre>"},{"location":"reference/utilities/#neurodent.core.utils.get_file_stem","title":"<code>get_file_stem(filepath)</code>","text":"<p>Get the true stem for files, handling double extensions like .npy.gz.</p> Source code in <code>src/neurodent/core/utils.py</code> <pre><code>def get_file_stem(filepath: Union[str, Path]) -&gt; str:\n    \"\"\"Get the true stem for files, handling double extensions like .npy.gz.\"\"\"\n    filepath = Path(filepath)\n    name = filepath.name\n\n    return name.split(\".\")[0]\n</code></pre>"},{"location":"reference/utilities/#neurodent.core.utils.get_temp_directory","title":"<code>get_temp_directory()</code>","text":"<p>Get the current temporary directory used by Neurodent.</p> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Path object representing the current temporary directory.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; temp_dir = get_temp_directory()\n&gt;&gt;&gt; print(f\"Current temp directory: {temp_dir}\")\nCurrent temp directory: /tmp/neurodent_temp\n</code></pre> <p>Raises:</p> Type Description <code>KeyError</code> <p>If TMPDIR environment variable is not set.</p> Source code in <code>src/neurodent/core/utils.py</code> <pre><code>def get_temp_directory() -&gt; Path:\n    \"\"\"\n    Get the current temporary directory used by Neurodent.\n\n    Returns:\n        Path: Path object representing the current temporary directory.\n\n    Examples:\n        &gt;&gt;&gt; temp_dir = get_temp_directory()\n        &gt;&gt;&gt; print(f\"Current temp directory: {temp_dir}\")\n        Current temp directory: /tmp/neurodent_temp\n\n    Raises:\n        KeyError: If TMPDIR environment variable is not set.\n    \"\"\"\n    return Path(os.environ[\"TMPDIR\"])\n</code></pre>"},{"location":"reference/utilities/#neurodent.core.utils.is_day","title":"<code>is_day(dt, sunrise=6, sunset=18)</code>","text":"<p>Check if a datetime object is during the day.</p> <p>Parameters:</p> Name Type Description Default <code>dt</code> <code>datetime</code> <p>Datetime object to check</p> required <code>sunrise</code> <code>int</code> <p>Sunrise hour (0-23). Defaults to 6.</p> <code>6</code> <code>sunset</code> <code>int</code> <p>Sunset hour (0-23). Defaults to 18.</p> <code>18</code> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the datetime is during the day, False otherwise</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If dt is not a datetime object</p> Source code in <code>src/neurodent/core/utils.py</code> <pre><code>def is_day(dt: datetime, sunrise=6, sunset=18):\n    \"\"\"\n    Check if a datetime object is during the day.\n\n    Args:\n        dt (datetime): Datetime object to check\n        sunrise (int, optional): Sunrise hour (0-23). Defaults to 6.\n        sunset (int, optional): Sunset hour (0-23). Defaults to 18.\n\n    Returns:\n        bool: True if the datetime is during the day, False otherwise\n\n    Raises:\n        TypeError: If dt is not a datetime object\n    \"\"\"\n    if not isinstance(dt, datetime):\n        raise TypeError(f\"Expected datetime object, got {type(dt).__name__}\")\n    return sunrise &lt;= dt.hour &lt; sunset\n</code></pre>"},{"location":"reference/utilities/#neurodent.core.utils.log_transform","title":"<code>log_transform(rec, **kwargs)</code>","text":"<p>Log transform the signal</p> <p>Parameters:</p> Name Type Description Default <code>rec</code> <code>ndarray</code> <p>The signal to log transform.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: ln(rec + 1)</p> Source code in <code>src/neurodent/core/utils.py</code> <pre><code>def log_transform(rec: np.ndarray, **kwargs) -&gt; np.ndarray:\n    \"\"\"Log transform the signal\n\n    Args:\n        rec (np.ndarray): The signal to log transform.\n\n    Returns:\n        np.ndarray: ln(rec + 1)\n    \"\"\"\n    if rec is not None:\n        return np.log(rec + 1)\n    else:\n        return None\n</code></pre>"},{"location":"reference/utilities/#neurodent.core.utils.nanaverage","title":"<code>nanaverage(A, weights, axis=-1)</code>","text":"<p>Compute weighted average of an array, ignoring NaN values.</p> <p>This function computes a weighted average along the specified axis while properly handling NaN values by masking them out of the calculation.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>ndarray</code> <p>Input array containing the values to average.</p> required <code>weights</code> <code>ndarray</code> <p>Array of weights corresponding to the values in A. Must be broadcastable with A along the specified axis.</p> required <code>axis</code> <code>int</code> <p>Axis along which to compute the average. Defaults to -1 (last axis).</p> <code>-1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Weighted average with NaN values properly handled. If all values along an axis are NaN, the result will be NaN for that position.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; A = np.array([[1.0, 2.0, np.nan], [4.0, np.nan, 6.0]])\n&gt;&gt;&gt; weights = np.array([1, 2, 1])\n&gt;&gt;&gt; nanaverage(A, weights, axis=1)\narray([1.66666667, 5.        ])\n</code></pre> Note <p>Be careful with zero or negative weights as they may produce unexpected results. The function uses numpy's masked array functionality for robust NaN handling.</p> Source code in <code>src/neurodent/core/utils.py</code> <pre><code>def nanaverage(A: np.ndarray, weights: np.ndarray, axis: int = -1) -&gt; np.ndarray:\n    \"\"\"\n    Compute weighted average of an array, ignoring NaN values.\n\n    This function computes a weighted average along the specified axis while\n    properly handling NaN values by masking them out of the calculation.\n\n    Args:\n        A (np.ndarray): Input array containing the values to average.\n        weights (np.ndarray): Array of weights corresponding to the values in A.\n            Must be broadcastable with A along the specified axis.\n        axis (int, optional): Axis along which to compute the average. Defaults to -1 (last axis).\n\n    Returns:\n        np.ndarray: Weighted average with NaN values properly handled. If all values\n            along an axis are NaN, the result will be NaN for that position.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; A = np.array([[1.0, 2.0, np.nan], [4.0, np.nan, 6.0]])\n        &gt;&gt;&gt; weights = np.array([1, 2, 1])\n        &gt;&gt;&gt; nanaverage(A, weights, axis=1)\n        array([1.66666667, 5.        ])\n\n    Note:\n        Be careful with zero or negative weights as they may produce unexpected results.\n        The function uses numpy's masked array functionality for robust NaN handling.\n    \"\"\"\n    masked = np.ma.masked_array(A, np.isnan(A))\n    avg = np.ma.average(masked, axis=axis, weights=weights)\n\n    # Handle case where np.ma.average returns a scalar instead of masked array\n    if np.ma.is_masked(avg):\n        return avg.filled(np.nan)\n    else:\n        # avg is a scalar or regular array, convert to array and handle NaN\n        result = np.asarray(avg)\n        return np.where(np.isfinite(result), result, np.nan)\n</code></pre>"},{"location":"reference/utilities/#neurodent.core.utils.nanmean_series_of_np","title":"<code>nanmean_series_of_np(x, axis=0)</code>","text":"<p>Efficiently compute NaN-aware mean of a pandas Series containing numpy arrays.</p> <p>This function is optimized for computing the mean across a Series where each element is a numpy array. It uses different strategies based on the size of the Series for optimal performance.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Series</code> <p>Series containing numpy arrays as elements.</p> required <code>axis</code> <code>int</code> <p>Axis along which to compute the mean. Defaults to 0. - axis=0: Mean across the Series elements (most common) - axis=1: Mean within each array element</p> <code>0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array containing the computed means with NaN values properly handled.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; # Create a Series of numpy arrays\n&gt;&gt;&gt; arrays = [np.array([1.0, 2.0, np.nan]),\n...           np.array([4.0, np.nan, 6.0]),\n...           np.array([7.0, 8.0, 9.0])]\n&gt;&gt;&gt; series = pd.Series(arrays)\n&gt;&gt;&gt; nanmean_series_of_np(series)\narray([4. , 5. , 7.5])\n</code></pre> Performance Notes <ul> <li>For Series with more than 1000 elements containing numpy arrays,   uses <code>np.stack()</code> for better performance</li> <li>Falls back to list conversion for smaller Series or mixed types</li> <li>Handles shape mismatches gracefully by falling back to the slower method</li> </ul> Source code in <code>src/neurodent/core/utils.py</code> <pre><code>def nanmean_series_of_np(x: pd.Series, axis: int = 0) -&gt; np.ndarray:\n    \"\"\"\n    Efficiently compute NaN-aware mean of a pandas Series containing numpy arrays.\n\n    This function is optimized for computing the mean across a Series where each element\n    is a numpy array. It uses different strategies based on the size of the Series\n    for optimal performance.\n\n    Args:\n        x (pd.Series): Series containing numpy arrays as elements.\n        axis (int, optional): Axis along which to compute the mean. Defaults to 0.\n            - axis=0: Mean across the Series elements (most common)\n            - axis=1: Mean within each array element\n\n    Returns:\n        np.ndarray: Array containing the computed means with NaN values properly handled.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; # Create a Series of numpy arrays\n        &gt;&gt;&gt; arrays = [np.array([1.0, 2.0, np.nan]),\n        ...           np.array([4.0, np.nan, 6.0]),\n        ...           np.array([7.0, 8.0, 9.0])]\n        &gt;&gt;&gt; series = pd.Series(arrays)\n        &gt;&gt;&gt; nanmean_series_of_np(series)\n        array([4. , 5. , 7.5])\n\n    Performance Notes:\n        - For Series with more than 1000 elements containing numpy arrays,\n          uses `np.stack()` for better performance\n        - Falls back to list conversion for smaller Series or mixed types\n        - Handles shape mismatches gracefully by falling back to the slower method\n    \"\"\"\n    # logging.debug(f\"Unique shapes in x: {set(np.shape(item) for item in x)}\")\n\n    if len(x) &gt; 1000:\n        try:\n            if isinstance(x.iloc[0], np.ndarray):\n                xmean: np.ndarray = np.nanmean(np.stack(x.values, axis=0), axis=axis)\n                return xmean\n        except (ValueError, TypeError):\n            pass\n\n    xmean: np.ndarray = np.nanmean(np.array(list(x)), axis=axis)\n    return xmean\n</code></pre>"},{"location":"reference/utilities/#neurodent.core.utils.parse_chname_to_abbrev","title":"<code>parse_chname_to_abbrev(channel_name, assume_from_number=False, strict_matching=True)</code>","text":"<p>Parses the channel name to get the abbreviation.</p> <p>Parameters:</p> Name Type Description Default <code>channel_name</code> <code>str</code> <p>Name of the channel.</p> required <code>assume_from_number</code> <code>bool</code> <p>If True, assume the abbreviation based on the last number in the channel name when normal parsing fails. Defaults to False.</p> <code>False</code> <code>strict_matching</code> <code>bool</code> <p>If True, ensures the input matches exactly one L/R alias and one channel alias. If False, allows multiple matches and uses longest. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Abbreviation of the channel name.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When channel_name cannot be parsed or contains ambiguous matches in strict mode.</p> <code>KeyError</code> <p>When assume_from_number=True but the detected number is not a valid channel ID.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; parse_chname_to_abbrev(\"left Aud\")\n'LAud'\n&gt;&gt;&gt; parse_chname_to_abbrev(\"Right VIS\")\n'RVis'\n&gt;&gt;&gt; parse_chname_to_abbrev(\"channel_9\", assume_from_number=True)\n'LAud'\n&gt;&gt;&gt; parse_chname_to_abbrev(\"LRAud\", strict_matching=False)  # Would work in non-strict mode\n'LAud'  # Uses longest L/R match\n</code></pre> Source code in <code>src/neurodent/core/utils.py</code> <pre><code>def parse_chname_to_abbrev(channel_name: str, assume_from_number=False, strict_matching=True) -&gt; str:\n    \"\"\"\n    Parses the channel name to get the abbreviation.\n\n    Args:\n        channel_name (str): Name of the channel.\n        assume_from_number (bool, optional): If True, assume the abbreviation based on the last number\n            in the channel name when normal parsing fails. Defaults to False.\n        strict_matching (bool, optional): If True, ensures the input matches exactly one L/R alias and\n            one channel alias. If False, allows multiple matches and uses longest. Defaults to True.\n\n    Returns:\n        str: Abbreviation of the channel name.\n\n    Raises:\n        ValueError: When channel_name cannot be parsed or contains ambiguous matches in strict mode.\n        KeyError: When assume_from_number=True but the detected number is not a valid channel ID.\n\n    Examples:\n        &gt;&gt;&gt; parse_chname_to_abbrev(\"left Aud\")\n        'LAud'\n        &gt;&gt;&gt; parse_chname_to_abbrev(\"Right VIS\")\n        'RVis'\n        &gt;&gt;&gt; parse_chname_to_abbrev(\"channel_9\", assume_from_number=True)\n        'LAud'\n        &gt;&gt;&gt; parse_chname_to_abbrev(\"LRAud\", strict_matching=False)  # Would work in non-strict mode\n        'LAud'  # Uses longest L/R match\n    \"\"\"\n    if channel_name in constants.DEFAULT_ID_TO_NAME.values():\n        logging.debug(f\"{channel_name} is already an abbreviation\")\n        return channel_name\n\n    try:\n        lr = _get_key_from_match_values(channel_name, constants.LR_ALIASES, strict_matching)\n        chname = _get_key_from_match_values(channel_name, constants.CHNAME_ALIASES, strict_matching)\n    except ValueError as e:\n        if assume_from_number:\n            logging.warning(f\"{channel_name} does not match name aliases. Assuming alias from number in channel name.\")\n            nums = re.findall(r\"\\d+\", channel_name)\n\n            if not nums:\n                raise ValueError(\n                    f\"Expected to find a number in channel name '{channel_name}' when assume_from_number=True, but no numbers were found.\"\n                )\n\n            num = int(nums[-1])\n            if num not in constants.DEFAULT_ID_TO_NAME:\n                available_ids = sorted(constants.DEFAULT_ID_TO_NAME.keys())\n                raise KeyError(\n                    f\"Channel number {num} found in '{channel_name}' is not a valid channel ID. Available channel IDs: {available_ids}\"\n                )\n\n            return constants.DEFAULT_ID_TO_NAME[num]\n        else:\n            raise e\n\n    return lr + chname\n</code></pre>"},{"location":"reference/utilities/#neurodent.core.utils.parse_path_to_animalday","title":"<code>parse_path_to_animalday(filepath, animal_param=(0, None), day_sep=None, mode='concat', **day_parse_kwargs)</code>","text":"<p>Parses the filename of a binfolder to get the animalday identifier (animal id, genotype, and day).</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str | Path</code> <p>Filepath of the binfolder.</p> required <code>animal_param</code> <code>tuple[int, str] | str | list[str]</code> <p>Parameter specifying how to parse the animal ID: tuple[int, str]: (index, separator) for simple split and index str: regex pattern to extract ID list[str]: list of possible animal IDs to match against</p> <code>(0, None)</code> <code>day_sep</code> <code>str</code> <p>Separator for day in filename. Defaults to None.</p> <code>None</code> <code>mode</code> <code>Literal['nest', 'concat', 'base', 'noday']</code> <p>Mode to parse the filename. Defaults to 'concat'. 'nest': Extracts genotype/animal from parent directory name and date from filename        e.g. \"/WT_A10/recording_2023-04-01.\" 'concat': Extracts all info from filename, expects genotype_animal_date format          e.g. \"/WT_A10_2023-04-01.\" 'base': Same as concat 'noday': Extracts only genotype and animal ID, uses default date         e.g. \"/WT_A10_recording.*\"</p> <code>'concat'</code> <code>**day_parse_kwargs</code> <p>Additional keyword arguments to pass to parse_str_to_day function.                Common options include parse_params dict for dateutil.parser.parse.</p> <code>{}</code> <p>Returns:</p> Type Description <p>dict[str, str]: Dictionary with keys \"animal\", \"genotype\", \"day\", and \"animalday\" (concatenated). Example: {\"animal\": \"A10\", \"genotype\": \"WT\", \"day\": \"Apr-01-2023\", \"animalday\": \"A10 WT Apr-01-2023\"}</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If mode is invalid or required components cannot be extracted</p> <code>TypeError</code> <p>If filepath is not str or Path</p> Source code in <code>src/neurodent/core/utils.py</code> <pre><code>def parse_path_to_animalday(\n    filepath: str | Path,\n    animal_param: tuple[int, str] | str | list[str] = (0, None),\n    day_sep: str | None = None,\n    mode: Literal[\"nest\", \"concat\", \"base\", \"noday\"] = \"concat\",\n    **day_parse_kwargs,\n):\n    \"\"\"\n    Parses the filename of a binfolder to get the animalday identifier (animal id, genotype, and day).\n\n    Args:\n        filepath (str | Path): Filepath of the binfolder.\n        animal_param (tuple[int, str] | str | list[str], optional): Parameter specifying how to parse the animal ID:\n            tuple[int, str]: (index, separator) for simple split and index\n            str: regex pattern to extract ID\n            list[str]: list of possible animal IDs to match against\n        day_sep (str, optional): Separator for day in filename. Defaults to None.\n        mode (Literal['nest', 'concat', 'base', 'noday'], optional): Mode to parse the filename. Defaults to 'concat'.\n            'nest': Extracts genotype/animal from parent directory name and date from filename\n                   e.g. \"/WT_A10/recording_2023-04-01.*\"\n            'concat': Extracts all info from filename, expects genotype_animal_date format\n                     e.g. \"/WT_A10_2023-04-01.*\"\n            'base': Same as concat\n            'noday': Extracts only genotype and animal ID, uses default date\n                    e.g. \"/WT_A10_recording.*\"\n        **day_parse_kwargs: Additional keyword arguments to pass to parse_str_to_day function.\n                           Common options include parse_params dict for dateutil.parser.parse.\n\n    Returns:\n        dict[str, str]: Dictionary with keys \"animal\", \"genotype\", \"day\", and \"animalday\" (concatenated).\n            Example: {\"animal\": \"A10\", \"genotype\": \"WT\", \"day\": \"Apr-01-2023\", \"animalday\": \"A10 WT Apr-01-2023\"}\n\n    Raises:\n        ValueError: If mode is invalid or required components cannot be extracted\n        TypeError: If filepath is not str or Path\n    \"\"\"\n    filepath = Path(filepath)\n    match mode:\n        case \"nest\":\n            geno = parse_str_to_genotype(filepath.parent.name)\n            animid = parse_str_to_animal(filepath.parent.name, animal_param=animal_param)\n            day = parse_str_to_day(filepath.name, sep=day_sep, **day_parse_kwargs).strftime(\"%b-%d-%Y\")\n        case \"concat\" | \"base\":\n            geno = parse_str_to_genotype(filepath.name)\n            animid = parse_str_to_animal(filepath.name, animal_param=animal_param)\n            day = parse_str_to_day(filepath.name, sep=day_sep, **day_parse_kwargs).strftime(\"%b-%d-%Y\")\n        case \"noday\":\n            geno = parse_str_to_genotype(filepath.name)\n            animid = parse_str_to_animal(filepath.name, animal_param=animal_param)\n            day = constants.DEFAULT_DAY.strftime(\"%b-%d-%Y\")\n        case _:\n            raise ValueError(f\"Invalid mode: {mode}\")\n    return {\n        \"animal\": animid,\n        \"genotype\": geno,\n        \"day\": day,\n        \"animalday\": f\"{animid} {geno} {day}\",\n    }\n</code></pre>"},{"location":"reference/utilities/#neurodent.core.utils.parse_str_to_animal","title":"<code>parse_str_to_animal(string, animal_param=(0, None))</code>","text":"<p>Parses the filename of a binfolder to get the animal id.</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <code>str</code> <p>String to parse.</p> required <code>animal_param</code> <code>tuple[int, str] | str | list[str]</code> <p>Parameter specifying how to parse the animal ID: tuple[int, str]: (index, separator) for simple split and index. Not recommended for inconsistent naming conventions. str: regex pattern to extract ID. Most general use case. If multiple matches are found, returns the first match. list[str]: list of possible animal IDs to match against. Returns first match in list order, case-sensitive, ignoring empty strings.</p> <code>(0, None)</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Animal id.</p> <p>Examples:</p>"},{"location":"reference/utilities/#neurodent.core.utils.parse_str_to_animal--tuple-format-index-separator","title":"Tuple format: (index, separator)","text":"<pre><code>&gt;&gt;&gt; parse_str_to_animal(\"WT_A10_2023-01-01_data.bin\", (1, \"_\"))\n'A10'\n&gt;&gt;&gt; parse_str_to_animal(\"A10_WT_recording.bin\", (0, \"_\"))\n'A10'\n</code></pre>"},{"location":"reference/utilities/#neurodent.core.utils.parse_str_to_animal--regex-pattern-format","title":"Regex pattern format","text":"<pre><code>&gt;&gt;&gt; parse_str_to_animal(\"WT_A10_2023-01-01_data.bin\", r\"A\\d+\")\n'A10'\n&gt;&gt;&gt; parse_str_to_animal(\"subject_123_data.bin\", r\"\\d+\")\n'123'\n</code></pre>"},{"location":"reference/utilities/#neurodent.core.utils.parse_str_to_animal--list-format-possible-ids-to-match","title":"List format: possible IDs to match","text":"<pre><code>&gt;&gt;&gt; parse_str_to_animal(\"WT_A10_2023-01-01_data.bin\", [\"A10\", \"A11\", \"A12\"])\n'A10'\n&gt;&gt;&gt; parse_str_to_animal(\"WT_A10_data.bin\", [\"B15\", \"C20\"])  # No match\nValueError: No matching ID found in WT_A10_data.bin from possible IDs: ['B15', 'C20']\n</code></pre> Source code in <code>src/neurodent/core/utils.py</code> <pre><code>def parse_str_to_animal(string: str, animal_param: tuple[int, str] | str | list[str] = (0, None)) -&gt; str:\n    \"\"\"\n    Parses the filename of a binfolder to get the animal id.\n\n    Args:\n        string (str): String to parse.\n        animal_param: Parameter specifying how to parse the animal ID:\n            tuple[int, str]: (index, separator) for simple split and index. Not recommended for inconsistent naming conventions.\n            str: regex pattern to extract ID. Most general use case. If multiple matches are found, returns the first match.\n            list[str]: list of possible animal IDs to match against. Returns first match in list order, case-sensitive, ignoring empty strings.\n\n    Returns:\n        str: Animal id.\n\n    Examples:\n        # Tuple format: (index, separator)\n        &gt;&gt;&gt; parse_str_to_animal(\"WT_A10_2023-01-01_data.bin\", (1, \"_\"))\n        'A10'\n        &gt;&gt;&gt; parse_str_to_animal(\"A10_WT_recording.bin\", (0, \"_\"))\n        'A10'\n\n        # Regex pattern format\n        &gt;&gt;&gt; parse_str_to_animal(\"WT_A10_2023-01-01_data.bin\", r\"A\\\\d+\")\n        'A10'\n        &gt;&gt;&gt; parse_str_to_animal(\"subject_123_data.bin\", r\"\\\\d+\")\n        '123'\n\n        # List format: possible IDs to match\n        &gt;&gt;&gt; parse_str_to_animal(\"WT_A10_2023-01-01_data.bin\", [\"A10\", \"A11\", \"A12\"])\n        'A10'\n        &gt;&gt;&gt; parse_str_to_animal(\"WT_A10_data.bin\", [\"B15\", \"C20\"])  # No match\n        ValueError: No matching ID found in WT_A10_data.bin from possible IDs: ['B15', 'C20']\n    \"\"\"\n    if isinstance(animal_param, tuple):\n        index, sep = animal_param\n        animid = string.split(sep)\n        return animid[index]\n    elif isinstance(animal_param, str):\n        pattern = animal_param\n        match = re.search(pattern, string)\n        if match:\n            return match.group()\n        raise ValueError(f\"No match found for pattern {pattern} in string {string}\")\n    elif isinstance(animal_param, list):\n        possible_ids = animal_param\n        for id in possible_ids:\n            # Skip empty or whitespace-only strings\n            if id and id.strip() and id in string:\n                return id\n        raise ValueError(f\"No matching ID found in {string} from possible IDs: {possible_ids}\")\n    else:\n        raise ValueError(f\"Invalid animal_param type: {type(animal_param)}\")\n</code></pre>"},{"location":"reference/utilities/#neurodent.core.utils.parse_str_to_day","title":"<code>parse_str_to_day(string, sep=None, parse_params=None, parse_mode='split', date_patterns=None)</code>","text":"<p>Parses the filename of a binfolder to get the day.</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <code>str</code> <p>String to parse.</p> required <code>sep</code> <code>str</code> <p>Separator to split string by. If None, split by whitespace. Defaults to None.</p> <code>None</code> <code>parse_params</code> <code>dict</code> <p>Parameters to pass to dateutil.parser.parse. Defaults to {'fuzzy':True}.</p> <code>None</code> <code>parse_mode</code> <code>Literal['full', 'split', 'window', 'all']</code> <p>Mode for parsing the string. Defaults to \"split\". \"full\": Try parsing the entire cleaned string only \"split\": Try parsing individual tokens only \"window\": Try parsing sliding windows of tokens (2-4 tokens) only \"all\": Use all three approaches in the order \"full\", \"split\", \"window</p> <code>'split'</code> <code>date_patterns</code> <code>list[tuple[str, str]]</code> <p>List of (regex_pattern, strptime_format) tuples to try before falling back to token-based parsing. This allows users to specify exact formats to handle ambiguous cases like MM/DD/YYYY vs DD/MM/YYYY. Only used in \"split\" and \"all\" modes. Defaults to None (no regex patterns).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>datetime</code> <code>datetime</code> <p>Datetime object corresponding to the day of the binfolder.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no valid date token is found in the string.</p> <code>TypeError</code> <p>If date_patterns is not a list of tuples.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Handle ambiguous date formats with explicit patterns\n&gt;&gt;&gt; patterns = [(r'(19\\d{2}|20\\d{2})-(\\d{1,2})-(\\d{1,2})', '%Y-%m-%d')]\n&gt;&gt;&gt; parse_str_to_day('2001_2023-07-04_data', date_patterns=patterns)\ndatetime.datetime(2023, 7, 4, 0, 0)\n</code></pre> <pre><code>&gt;&gt;&gt; # European format pattern\n&gt;&gt;&gt; patterns = [(r'(\\d{1,2})/(\\d{1,2})/(19\\d{2}|20\\d{2})', '%d/%m/%Y')]\n&gt;&gt;&gt; parse_str_to_day('04/07/2023_data', date_patterns=patterns)\ndatetime.datetime(2023, 7, 4, 0, 0)  # July 4th, not April 7th\n</code></pre> Note <p>When date_patterns is provided, users have full control over date interpretation. Without date_patterns, the function falls back to token-based parsing which may be ambiguous for formats like MM/DD/YYYY vs DD/MM/YYYY.</p> Source code in <code>src/neurodent/core/utils.py</code> <pre><code>def parse_str_to_day(\n    string: str,\n    sep: str = None,\n    parse_params: dict = None,\n    parse_mode: Literal[\"full\", \"split\", \"window\", \"all\"] = \"split\",\n    date_patterns: list[tuple[str, str]] = None,\n) -&gt; datetime:\n    \"\"\"\n    Parses the filename of a binfolder to get the day.\n\n    Args:\n        string (str): String to parse.\n        sep (str, optional): Separator to split string by. If None, split by whitespace. Defaults to None.\n        parse_params (dict, optional): Parameters to pass to dateutil.parser.parse. Defaults to {'fuzzy':True}.\n        parse_mode (Literal[\"full\", \"split\", \"window\", \"all\"], optional): Mode for parsing the string. Defaults to \"split\".\n            \"full\": Try parsing the entire cleaned string only\n            \"split\": Try parsing individual tokens only\n            \"window\": Try parsing sliding windows of tokens (2-4 tokens) only\n            \"all\": Use all three approaches in the order \"full\", \"split\", \"window\n        date_patterns (list[tuple[str, str]], optional): List of (regex_pattern, strptime_format) tuples\n            to try before falling back to token-based parsing. This allows users to specify\n            exact formats to handle ambiguous cases like MM/DD/YYYY vs DD/MM/YYYY.\n            Only used in \"split\" and \"all\" modes. Defaults to None (no regex patterns).\n\n    Returns:\n        datetime: Datetime object corresponding to the day of the binfolder.\n\n    Raises:\n        ValueError: If no valid date token is found in the string.\n        TypeError: If date_patterns is not a list of tuples.\n\n    Examples:\n        &gt;&gt;&gt; # Handle ambiguous date formats with explicit patterns\n        &gt;&gt;&gt; patterns = [(r'(19\\d{2}|20\\d{2})-(\\d{1,2})-(\\d{1,2})', '%Y-%m-%d')]\n        &gt;&gt;&gt; parse_str_to_day('2001_2023-07-04_data', date_patterns=patterns)\n        datetime.datetime(2023, 7, 4, 0, 0)\n\n        &gt;&gt;&gt; # European format pattern\n        &gt;&gt;&gt; patterns = [(r'(\\d{1,2})/(\\d{1,2})/(19\\d{2}|20\\d{2})', '%d/%m/%Y')]\n        &gt;&gt;&gt; parse_str_to_day('04/07/2023_data', date_patterns=patterns)\n        datetime.datetime(2023, 7, 4, 0, 0)  # July 4th, not April 7th\n\n    Note:\n        When date_patterns is provided, users have full control over date interpretation.\n        Without date_patterns, the function falls back to token-based parsing which may\n        be ambiguous for formats like MM/DD/YYYY vs DD/MM/YYYY.\n    \"\"\"\n    if parse_params is None:\n        parse_params = {\"fuzzy\": True}\n    elif not isinstance(parse_params, dict):\n        raise TypeError(\"parse_params must be a dictionary\")\n\n    # Validate date_patterns\n    if date_patterns is not None:\n        if not isinstance(date_patterns, list):\n            raise TypeError(\"date_patterns must be a list of (regex_pattern, strptime_format) tuples\")\n        for i, pattern_tuple in enumerate(date_patterns):\n            if not isinstance(pattern_tuple, tuple) or len(pattern_tuple) != 2:\n                raise TypeError(f\"date_patterns[{i}] must be a tuple of (regex_pattern, strptime_format)\")\n            if not isinstance(pattern_tuple[0], str) or not isinstance(pattern_tuple[1], str):\n                raise TypeError(f\"date_patterns[{i}] must contain string elements\")\n\n    # Validate parse_mode\n    valid_modes = [\"full\", \"split\", \"window\", \"all\"]\n    if parse_mode not in valid_modes:\n        raise ValueError(f\"Invalid parse_mode: {parse_mode}. Must be one of {valid_modes}\")\n\n    clean_str = _clean_str_for_date(string)\n\n    # Only use user-provided regex patterns for \"split\" and \"all\" modes\n    if date_patterns and parse_mode in [\"split\", \"all\"]:\n        date_result = _try_user_regex_patterns(clean_str, date_patterns)\n        if date_result is not None:\n            return date_result\n        else:\n            # Warn when patterns are provided but none match, falling back to token parsing\n            warnings.warn(\n                f\"No user-provided date patterns matched '{clean_str}'. \"\n                f\"Falling back to token-based parsing which may be ambiguous.\",\n                UserWarning,\n            )\n\n    # Fallback to original token-based approach\n    # Try parsing based on the specified mode\n    if parse_mode in [\"full\", \"all\"]:\n        # Pass 1: Try parsing the entire cleaned string\n        try:\n            date = dateutil.parser.parse(clean_str, default=constants.DEFAULT_DAY, **parse_params)\n            if date.year &gt; 1980:\n                return date\n        except ParserError:\n            pass\n\n    if parse_mode in [\"split\", \"all\"]:\n        # Pass 2: Try individual tokens\n        tokens = clean_str.split(sep)\n        if len(tokens) == 1:\n            warnings.warn(\"Only 1 string token found. Did you mean to use a different separator or parse_mode='all'?\")\n        for token in tokens:\n            try:\n                # logging.debug(f'token: {token}')\n                date = dateutil.parser.parse(token, default=constants.DEFAULT_DAY, **parse_params)\n                if date.year &lt;= 1980:\n                    continue\n                return date\n            except ParserError:\n                continue\n\n    if parse_mode in [\"window\", \"all\"]:\n        # Pass 3: Try sliding window of tokens\n        tokens = clean_str.split(sep)\n        if len(tokens) == 1:\n            warnings.warn(\"Only 1 string token found. Did you mean to use a different separator or parse_mode='all'?\")\n        for window_size in range(2, min(5, len(tokens) + 1)):\n            for i in range(len(tokens) - window_size + 1):\n                grouped = \" \".join(tokens[i : i + window_size])\n                try:\n                    date = dateutil.parser.parse(grouped, default=constants.DEFAULT_DAY, **parse_params)\n                    if date.year &lt;= 1980:\n                        continue\n                    return date\n                except ParserError:\n                    continue\n\n    raise ValueError(f\"No valid date token found in string: {string}\")\n</code></pre>"},{"location":"reference/utilities/#neurodent.core.utils.parse_str_to_genotype","title":"<code>parse_str_to_genotype(string, strict_matching=False)</code>","text":"<p>Parses the filename of a binfolder to get the genotype.</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <code>str</code> <p>String to parse.</p> required <code>strict_matching</code> <code>bool</code> <p>If True, ensures the input matches exactly one genotype. If False, allows overlapping matches and uses longest. Defaults to False for backward compatibility.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Genotype.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When string cannot be parsed or contains ambiguous matches in strict mode.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; parse_str_to_genotype(\"WT_A10_data\")\n'WT'\n&gt;&gt;&gt; parse_str_to_genotype(\"WT_KO_comparison\", strict_matching=True)  # Would raise error\nValueError: Ambiguous match...\n&gt;&gt;&gt; parse_str_to_genotype(\"WT_KO_comparison\", strict_matching=False)  # Uses longest match\n'WT'  # or 'KO' depending on which alias is longer\n</code></pre> Source code in <code>src/neurodent/core/utils.py</code> <pre><code>def parse_str_to_genotype(string: str, strict_matching: bool = False) -&gt; str:\n    \"\"\"\n    Parses the filename of a binfolder to get the genotype.\n\n    Args:\n        string (str): String to parse.\n        strict_matching (bool, optional): If True, ensures the input matches exactly one genotype.\n            If False, allows overlapping matches and uses longest. Defaults to False for\n            backward compatibility.\n\n    Returns:\n        str: Genotype.\n\n    Raises:\n        ValueError: When string cannot be parsed or contains ambiguous matches in strict mode.\n\n    Examples:\n        &gt;&gt;&gt; parse_str_to_genotype(\"WT_A10_data\")\n        'WT'\n        &gt;&gt;&gt; parse_str_to_genotype(\"WT_KO_comparison\", strict_matching=True)  # Would raise error\n        ValueError: Ambiguous match...\n        &gt;&gt;&gt; parse_str_to_genotype(\"WT_KO_comparison\", strict_matching=False)  # Uses longest match\n        'WT'  # or 'KO' depending on which alias is longer\n    \"\"\"\n    return _get_key_from_match_values(string, constants.GENOTYPE_ALIASES, strict_matching)\n</code></pre>"},{"location":"reference/utilities/#neurodent.core.utils.parse_truncate","title":"<code>parse_truncate(truncate)</code>","text":"<p>Parse the truncate parameter to determine how many characters to truncate.</p> <p>If truncate is a boolean, returns 10 if True and 0 if False. If truncate is an integer, returns that integer value directly.</p> <p>Parameters:</p> Name Type Description Default <code>truncate</code> <code>int | bool</code> <p>If bool, True=10 chars and False=0 chars.                   If int, specifies exact number of chars.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of characters to truncate (0 means no truncation)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If truncate is not a boolean or integer</p> Source code in <code>src/neurodent/core/utils.py</code> <pre><code>def parse_truncate(truncate: int | bool) -&gt; int:\n    \"\"\"\n    Parse the truncate parameter to determine how many characters to truncate.\n\n    If truncate is a boolean, returns 10 if True and 0 if False.\n    If truncate is an integer, returns that integer value directly.\n\n    Args:\n        truncate (int | bool): If bool, True=10 chars and False=0 chars.\n                              If int, specifies exact number of chars.\n\n    Returns:\n        int: Number of characters to truncate (0 means no truncation)\n\n    Raises:\n        ValueError: If truncate is not a boolean or integer\n    \"\"\"\n    if isinstance(truncate, bool):\n        return 10 if truncate else 0\n    elif isinstance(truncate, int):\n        return truncate\n    else:\n        raise ValueError(f\"Invalid truncate value: {truncate}\")\n</code></pre>"},{"location":"reference/utilities/#neurodent.core.utils.set_temp_directory","title":"<code>set_temp_directory(path)</code>","text":"<p>Set the temporary directory for Neurodent operations.</p> <p>This function configures the temporary directory used by Neurodent for intermediate files and operations. The directory will be created if it doesn't exist.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to the temporary directory. Will be created if it doesn't exist.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; set_temp_directory(\"/tmp/neurodent_temp\")\n&gt;&gt;&gt; set_temp_directory(Path.home() / \"neurodent_workspace\" / \"temp\")\n</code></pre> Note <p>This function modifies the TMPDIR environment variable, which affects the behavior of other temporary file operations in the process.</p> Source code in <code>src/neurodent/core/utils.py</code> <pre><code>def set_temp_directory(path: str | Path) -&gt; None:\n    \"\"\"\n    Set the temporary directory for Neurodent operations.\n\n    This function configures the temporary directory used by Neurodent for intermediate\n    files and operations. The directory will be created if it doesn't exist.\n\n    Args:\n        path (str | Path): Path to the temporary directory. Will be created if it doesn't exist.\n\n    Examples:\n        &gt;&gt;&gt; set_temp_directory(\"/tmp/neurodent_temp\")\n        &gt;&gt;&gt; set_temp_directory(Path.home() / \"neurodent_workspace\" / \"temp\")\n\n    Note:\n        This function modifies the TMPDIR environment variable, which affects\n        the behavior of other temporary file operations in the process.\n    \"\"\"\n    path = Path(path)\n    if not path.exists():\n        path.mkdir(parents=True, exist_ok=True)\n    os.environ[\"TMPDIR\"] = str(path)\n    logging.info(f\"Temporary directory set to {path}\")\n</code></pre>"},{"location":"reference/utilities/#neurodent.core.utils.should_use_cache_unified","title":"<code>should_use_cache_unified(cache_path, source_paths, cache_policy)</code>","text":"<p>Unified cache decision logic for all intermediate files.</p> <p>Parameters:</p> Name Type Description Default <code>cache_path</code> <code>Union[str, Path]</code> <p>Path to the cache file</p> required <code>source_paths</code> <code>list[Union[str, Path]]</code> <p>List of source file paths to check timestamps against</p> required <code>cache_policy</code> <code>Literal['auto', 'always', 'force_regenerate']</code> <p>Caching policy: - \"auto\": Use cache if exists and newer than sources, regenerate with logging if missing/invalid - \"always\": Use cache if exists, raise error if missing/invalid - \"force_regenerate\": Always regenerate and overwrite existing cache</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if cache should be used, False if should regenerate</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If cache_policy is invalid</p> Source code in <code>src/neurodent/core/utils.py</code> <pre><code>def should_use_cache_unified(\n    cache_path: Union[str, Path],\n    source_paths: list[Union[str, Path]],\n    cache_policy: Literal[\"auto\", \"always\", \"force_regenerate\"],\n) -&gt; bool:\n    \"\"\"Unified cache decision logic for all intermediate files.\n\n    Args:\n        cache_path: Path to the cache file\n        source_paths: List of source file paths to check timestamps against\n        cache_policy: Caching policy:\n            - \"auto\": Use cache if exists and newer than sources, regenerate with logging if missing/invalid\n            - \"always\": Use cache if exists, raise error if missing/invalid\n            - \"force_regenerate\": Always regenerate and overwrite existing cache\n\n    Returns:\n        bool: True if cache should be used, False if should regenerate\n\n    Raises:\n        ValueError: If cache_policy is invalid\n    \"\"\"\n    if cache_policy == \"force_regenerate\":\n        return False\n    elif cache_policy == \"always\":\n        return Path(cache_path).exists()\n    elif cache_policy == \"auto\":\n        return should_use_cached_file(cache_path, source_paths, \"auto\")\n    else:\n        raise ValueError(f\"Invalid cache_policy: {cache_policy}. Must be one of: auto, always, force_regenerate\")\n</code></pre>"},{"location":"reference/utilities/#neurodent.core.utils.should_use_cached_file","title":"<code>should_use_cached_file(cache_path, source_paths, use_cached='auto')</code>","text":"<p>Determine whether to use a cached intermediate file based on caching policy and file timestamps.</p> <p>Parameters:</p> Name Type Description Default <code>cache_path</code> <code>Union[str, Path]</code> <p>Path to the cached intermediate file</p> required <code>source_paths</code> <code>list[Union[str, Path]]</code> <p>List of source file paths that the cache depends on</p> required <code>use_cached</code> <code>Literal['auto', 'always', 'never', 'error']</code> <p>Caching policy - \"auto\": Use cached if exists and newer than all sources (default) - \"always\": Always use cached if it exists - \"never\": Never use cached (always regenerate) - \"error\": Raise error if cached doesn't exist</p> <code>'auto'</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if cached file should be used, False if it should be regenerated</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>When use_cached=\"error\" and cache doesn't exist</p> <code>ValueError</code> <p>For invalid use_cached values</p> Source code in <code>src/neurodent/core/utils.py</code> <pre><code>def should_use_cached_file(\n    cache_path: Union[str, Path],\n    source_paths: list[Union[str, Path]],\n    use_cached: Literal[\"auto\", \"always\", \"never\", \"error\"] = \"auto\",\n) -&gt; bool:\n    \"\"\"\n    Determine whether to use a cached intermediate file based on caching policy and file timestamps.\n\n    Args:\n        cache_path: Path to the cached intermediate file\n        source_paths: List of source file paths that the cache depends on\n        use_cached: Caching policy\n            - \"auto\": Use cached if exists and newer than all sources (default)\n            - \"always\": Always use cached if it exists\n            - \"never\": Never use cached (always regenerate)\n            - \"error\": Raise error if cached doesn't exist\n\n    Returns:\n        bool: True if cached file should be used, False if it should be regenerated\n\n    Raises:\n        FileNotFoundError: When use_cached=\"error\" and cache doesn't exist\n        ValueError: For invalid use_cached values\n    \"\"\"\n    cache_path = Path(cache_path)\n    source_paths = [Path(p) for p in source_paths]\n\n    if use_cached == \"never\":\n        return False\n    elif use_cached == \"error\":\n        if not cache_path.exists():\n            raise FileNotFoundError(f\"Cache file required but not found: {cache_path}\")\n        return True\n    elif use_cached == \"always\":\n        return cache_path.exists()\n    elif use_cached == \"auto\":\n        if not cache_path.exists():\n            return False\n\n        # Check if cache is newer than all source files\n        cache_mtime = cache_path.stat().st_mtime\n\n        for source_path in source_paths:\n            if not source_path.exists():\n                continue  # Skip missing source files\n            if source_path.stat().st_mtime &gt; cache_mtime:\n                logging.info(f\"Cache {cache_path.name} is older than {source_path.name}, regenerating\")\n                return False\n\n        logging.info(f\"Using cached intermediate file: {cache_path.name}\")\n        return True\n    else:\n        raise ValueError(f\"Invalid use_cached value: {use_cached}\")\n</code></pre>"},{"location":"reference/utilities/#neurodent.core.utils.sort_dataframe_by_plot_order","title":"<code>sort_dataframe_by_plot_order(df, df_sort_order=None)</code>","text":"<p>Sort DataFrame columns according to predefined orders.</p>"},{"location":"reference/utilities/#neurodent.core.utils.sort_dataframe_by_plot_order--parameters","title":"Parameters","text":"<p>df : pd.DataFrame     DataFrame to sort df_sort_order : dict     Dictionary mapping column names to the order of the values in the column.</p>"},{"location":"reference/utilities/#neurodent.core.utils.sort_dataframe_by_plot_order--returns","title":"Returns","text":"<p>pd.DataFrame     Sorted DataFrame</p>"},{"location":"reference/utilities/#neurodent.core.utils.sort_dataframe_by_plot_order--raises","title":"Raises","text":"<p>ValueError     If df_sort_order is not a valid dictionary or contains invalid categories</p> Source code in <code>src/neurodent/core/utils.py</code> <pre><code>def sort_dataframe_by_plot_order(df: pd.DataFrame, df_sort_order: Optional[dict] = None) -&gt; pd.DataFrame:\n    \"\"\"\n    Sort DataFrame columns according to predefined orders.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        DataFrame to sort\n    df_sort_order : dict\n        Dictionary mapping column names to the order of the values in the column.\n\n    Returns\n    -------\n    pd.DataFrame\n        Sorted DataFrame\n\n    Raises\n    ------\n    ValueError\n        If df_sort_order is not a valid dictionary or contains invalid categories\n    \"\"\"\n    if df_sort_order is None:\n        df_sort_order = constants.DF_SORT_ORDER.copy()\n    elif not isinstance(df_sort_order, dict):\n        raise ValueError(\"df_sort_order must be a dictionary\")\n\n    if df.empty:\n        return df.copy()\n\n    for col, categories in df_sort_order.items():\n        if not isinstance(categories, (list, tuple)):\n            raise ValueError(f\"Categories for column '{col}' must be a list or tuple\")\n\n    columns_to_sort = [col for col in df.columns if col in df_sort_order]\n    df_sorted = df.copy()\n\n    if not columns_to_sort:\n        return df_sorted\n\n    for col in columns_to_sort:\n        categories = df_sort_order[col]\n\n        # Check for values not in predefined categories\n        unique_values = set(df_sorted[col].dropna().unique())\n        missing_values = unique_values - set(categories)\n\n        if missing_values:\n            raise ValueError(\n                f\"Column '{col}' contains values not in sort order dictionary: {missing_values}. Add them to plot_order in ExperimentPlotter init.\"\n            )\n\n        # Filter categories to only include those that exist in the DataFrame\n        existing_categories = [cat for cat in categories if cat in unique_values]\n\n        df_sorted[col] = pd.Categorical(df_sorted[col], categories=existing_categories, ordered=True)\n\n    df_sorted = df_sorted.sort_values(columns_to_sort)\n    # REVIEW since \"sex\" is not inherently part of the pipeline (add ad-hoc), this could be a feature worth sorting\n    # But this might mean rewriting the data loading pipeline, file-reading, etc.\n    # Maybe a dictionary corresponding to animal/id -&gt; sex would be good enough, instead of reading it in from filenames\n    # which would be difficult since name conventions are not standardized\n\n    return df_sorted\n</code></pre>"},{"location":"reference/utilities/#neurodent.core.utils.validate_timestamps","title":"<code>validate_timestamps(timestamps, gap_threshold_seconds=60)</code>","text":"<p>Validate that timestamps are in chronological order and check for large gaps.</p> <p>Parameters:</p> Name Type Description Default <code>timestamps</code> <code>list[datetime]</code> <p>List of timestamps to validate</p> required <code>gap_threshold_seconds</code> <code>float</code> <p>Threshold in seconds for warning about large gaps. Defaults to 60.</p> <code>60</code> <p>Returns:</p> Type Description <code>list[datetime]</code> <p>list[datetime]: The validated timestamps in chronological order</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no valid timestamps are provided</p> Source code in <code>src/neurodent/core/utils.py</code> <pre><code>def validate_timestamps(timestamps: list[datetime], gap_threshold_seconds: float = 60) -&gt; list[datetime]:\n    \"\"\"\n    Validate that timestamps are in chronological order and check for large gaps.\n\n    Args:\n        timestamps (list[datetime]): List of timestamps to validate\n        gap_threshold_seconds (float, optional): Threshold in seconds for warning about large gaps. Defaults to 60.\n\n    Returns:\n        list[datetime]: The validated timestamps in chronological order\n\n    Raises:\n        ValueError: If no valid timestamps are provided\n    \"\"\"\n    if not timestamps:\n        raise ValueError(\"No timestamps provided for validation\")\n\n    valid_timestamps = [ts for ts in timestamps if ts is not None]\n    if len(valid_timestamps) &lt; len(timestamps):\n        warnings.warn(f\"Found {len(timestamps) - len(valid_timestamps)} None timestamps that were filtered out\")\n\n    if not valid_timestamps:\n        raise ValueError(\"No valid timestamps found (all were None)\")\n\n    # Check chronological order\n    sorted_timestamps = sorted(valid_timestamps)\n    if valid_timestamps != sorted_timestamps:\n        warnings.warn(\"Timestamps are not in chronological order. This may cause issues with the data.\")\n\n    # Check for large gaps between consecutive timestamps\n    for i in range(1, len(valid_timestamps)):\n        gap = valid_timestamps[i] - valid_timestamps[i - 1]\n        gap_seconds = gap.total_seconds()\n\n        if gap_seconds &gt; gap_threshold_seconds:\n            warnings.warn(\n                f\"Large gap detected between timestamps: {gap} exceeds threshold of {gap_threshold_seconds} seconds\"\n            )\n\n    return valid_timestamps\n</code></pre>"},{"location":"reference/core/analysis/","title":"Analysis","text":""},{"location":"reference/core/analysis/#neurodent.core.analysis.LongRecordingAnalyzer","title":"<code>LongRecordingAnalyzer</code>","text":"Source code in <code>src/neurodent/core/analysis.py</code> <pre><code>class LongRecordingAnalyzer:\n    def __init__(self, longrecording: core.LongRecordingOrganizer, fragment_len_s=10, apply_notch_filter=True) -&gt; None:\n        assert isinstance(longrecording, core.LongRecordingOrganizer)\n\n        self.LongRecording = longrecording\n        self.fragment_len_s = fragment_len_s\n        self.n_fragments = longrecording.get_num_fragments(fragment_len_s)\n        self.channel_names = longrecording.channel_names\n        self.n_channels = longrecording.meta.n_channels\n        self.mult_to_uV = longrecording.meta.mult_to_uV\n        self.f_s = int(longrecording.LongRecording.get_sampling_frequency())\n        self.apply_notch_filter = apply_notch_filter\n\n    def get_fragment_rec(self, index) -&gt; \"si.BaseRecording\":\n        \"\"\"Get window at index as a spikeinterface recording object\n\n        Args:\n            index (int): Index of time window\n\n        Returns:\n            si.BaseRecording: spikeinterface recording object with optional notch filtering applied\n        \"\"\"\n        if si is None:\n            raise ImportError(\"spikeinterface is required for get_fragment_rec\")\n\n        rec = self.LongRecording.get_fragment(self.fragment_len_s, index)\n\n        if self.apply_notch_filter and spre is not None:\n            rec = spre.notch_filter(rec, freq=constants.LINE_FREQ)\n\n        return rec\n\n    def get_fragment_np(self, index, recobj=None) -&gt; np.ndarray:\n        \"\"\"Get window at index as a numpy array object\n\n        Args:\n            index (int): Index of time window\n            recobj (si.BaseRecording, optional): If not None, uses this recording object to get the numpy array. Defaults to None.\n\n        Returns:\n            np.ndarray: Numpy array with dimensions (N, M), N = number of samples, M = number of channels. Values in uV\n        \"\"\"\n        if si is not None:\n            assert isinstance(recobj, si.BaseRecording) or recobj is None\n        if recobj is None:\n            return self.get_fragment_rec(index).get_traces(\n                return_scaled=True\n            )  # (num_samples, num_channels), in units uV\n        else:\n            return recobj.get_traces(return_scaled=True)\n\n    def get_fragment_mne(self, index, recobj=None) -&gt; np.ndarray:\n        \"\"\"Get window at index as a numpy array object, formatted for ease of use with MNE functions\n\n        Args:\n            index (int): Index of time window\n            recobj (si.BaseRecording, optional): If not None, uses this recording object to get the numpy array. Defaults to None.\n\n        Returns:\n            np.ndarray: Numpy array with dimensions (1, M, N), M = number of channels, N = number of samples. 1st dimension corresponds\n             to number of epochs, which there is only 1 in a window. Values in uV\n        \"\"\"\n        rec = self.get_fragment_np(index, recobj=recobj)[..., np.newaxis]\n        return np.transpose(rec, (2, 1, 0))  # (1 epoch, num_channels, num_samples)\n\n    def get_file_end(self, index, **kwargs):\n        tstart, tend = self.convert_idx_to_timebound(index)\n        for tfile in self.LongRecording.cumulative_file_durations:\n            if tstart &lt;= tfile &lt; tend:\n                return tfile - tstart\n        return None\n\n    def compute_rms(self, index, **kwargs):\n        \"\"\"Compute average root mean square amplitude\n\n        Args:\n            index (int): Index of time window\n\n        Returns:\n            result: np.ndarray with shape (1, M), M = number of channels\n        \"\"\"\n        rec = self.get_fragment_np(index)\n        return FragmentAnalyzer.compute_rms(rec=rec, **kwargs)\n\n    def compute_logrms(self, index, **kwargs):\n        \"\"\"Compute the log of the root mean square amplitude\"\"\"\n        rec = self.get_fragment_np(index)\n        return FragmentAnalyzer.compute_logrms(rec=rec, **kwargs)\n\n    def compute_ampvar(self, index, **kwargs):\n        \"\"\"Compute average amplitude variance\n\n        Args:\n            index (int): Index of time window\n\n        Returns:\n            result: np.ndarray with shape (1, M), M = number of channels\n        \"\"\"\n        rec = self.get_fragment_np(index)\n        return FragmentAnalyzer.compute_ampvar(rec=rec, **kwargs)\n\n    def compute_logampvar(self, index, **kwargs):\n        \"\"\"Compute the log of the amplitude variance\"\"\"\n        rec = self.get_fragment_np(index)\n        return FragmentAnalyzer.compute_logampvar(rec=rec, **kwargs)\n\n    def compute_psd(self, index, welch_bin_t=1, notch_filter=True, multitaper=False, **kwargs):\n        \"\"\"Compute PSD (power spectral density)\n\n        Args:\n            index (int): Index of time window\n            welch_bin_t (float, optional): Length of time bins to use in Welch's method, in seconds. Defaults to 1.\n            notch_filter (bool, optional): If True, applies notch filter at line frequency. Defaults to True.\n            multitaper (bool, optional): If True, uses multitaper method instead of Welch's method. Defaults to False.\n\n        Returns:\n            f (np.ndarray): Array of sample frequencies\n            psd (np.ndarray): Array of PSD values at sample frequencies. (X, M), X = number of sample frequencies, M = number of channels.\n            If sample window length is too short, PSD is interpolated\n        \"\"\"\n        rec = self.get_fragment_np(index)\n\n        f, psd = FragmentAnalyzer.compute_psd(\n            rec=rec, f_s=self.f_s, welch_bin_t=welch_bin_t, notch_filter=notch_filter, multitaper=multitaper, **kwargs\n        )\n\n        if index == self.n_fragments - 1 and self.n_fragments &gt; 1:\n            f_prev, _ = self.compute_psd(index - 1, welch_bin_t, notch_filter, multitaper)\n            psd = Akima1DInterpolator(f, psd, axis=0, extrapolate=True)(f_prev)\n            f = f_prev\n\n        return f, psd\n\n    def compute_psdband(\n        self,\n        index,\n        welch_bin_t=1,\n        notch_filter=True,\n        bands: list[tuple[float, float]] = constants.FREQ_BANDS,\n        multitaper=False,\n        **kwargs,\n    ):\n        \"\"\"Compute power spectral density of the signal for each frequency band.\n\n        Args:\n            index (int): Index of time window\n            welch_bin_t (float, optional): Length of time bins to use in Welch's method, in seconds. Defaults to 1.\n            notch_filter (bool, optional): If True, applies notch filter at line frequency. Defaults to True.\n            bands (list[tuple[float, float]], optional): List of frequency bands to compute PSD for. Defaults to constants.FREQ_BANDS.\n            multitaper (bool, optional): If True, uses multitaper method instead of Welch's method. Defaults to False.\n\n        Returns:\n            dict: Dictionary mapping band names to PSD values for each channel\n        \"\"\"\n\n        rec = self.get_fragment_np(index)\n\n        return FragmentAnalyzer.compute_psdband(\n            rec=rec,\n            f_s=self.f_s,\n            welch_bin_t=welch_bin_t,\n            notch_filter=notch_filter,\n            bands=bands,\n            multitaper=multitaper,\n            **kwargs,\n        )\n\n    def compute_logpsdband(\n        self,\n        index,\n        welch_bin_t=1,\n        notch_filter=True,\n        bands: list[tuple[float, float]] = constants.FREQ_BANDS,\n        multitaper=False,\n        **kwargs,\n    ):\n        \"\"\"Compute the log of the power spectral density of the signal for each frequency band.\"\"\"\n        rec = self.get_fragment_np(index)\n\n        return FragmentAnalyzer.compute_logpsdband(\n            rec=rec,\n            f_s=self.f_s,\n            welch_bin_t=welch_bin_t,\n            notch_filter=notch_filter,\n            bands=bands,\n            multitaper=multitaper,\n            **kwargs,\n        )\n\n    def compute_psdtotal(\n        self,\n        index,\n        welch_bin_t=1,\n        notch_filter=True,\n        band: tuple[float, float] = constants.FREQ_BAND_TOTAL,\n        multitaper=False,\n        **kwargs,\n    ):\n        \"\"\"Compute total power over PSD (power spectral density) plot within a specified frequency band\n\n        Args:\n            index (int): Index of time window\n            welch_bin_t (float, optional): Length of time bins to use in Welch's method, in seconds. Defaults to 1.\n            notch_filter (bool, optional): If True, applies notch filter at line frequency. Defaults to True.\n            band (tuple[float, float], optional): Frequency band to calculate over. Defaults to constants.FREQ_BAND_TOTAL.\n            multitaper (bool, optional): If True, uses multitaper method instead of Welch's method. Defaults to False.\n\n        Returns:\n            psdtotal (np.ndarray): (M,) long array, M = number of channels. Each value corresponds to sum total of PSD in that band at that channel\n        \"\"\"\n        rec = self.get_fragment_np(index)\n\n        return FragmentAnalyzer.compute_psdtotal(\n            rec=rec,\n            f_s=self.f_s,\n            welch_bin_t=welch_bin_t,\n            notch_filter=notch_filter,\n            band=band,\n            multitaper=multitaper,\n            **kwargs,\n        )\n\n    def compute_logpsdtotal(\n        self,\n        index,\n        welch_bin_t=1,\n        notch_filter=True,\n        band: tuple[float, float] = constants.FREQ_BAND_TOTAL,\n        multitaper=False,\n        **kwargs,\n    ):\n        \"\"\"Compute the log of the total power over PSD (power spectral density) plot within a specified frequency band\"\"\"\n        rec = self.get_fragment_np(index)\n\n        return FragmentAnalyzer.compute_logpsdtotal(\n            rec=rec,\n            f_s=self.f_s,\n            welch_bin_t=welch_bin_t,\n            notch_filter=notch_filter,\n            band=band,\n            multitaper=multitaper,\n            **kwargs,\n        )\n\n    def compute_psdfrac(\n        self,\n        index,\n        welch_bin_t=1,\n        notch_filter=True,\n        bands: list[tuple[float, float]] = constants.FREQ_BANDS,\n        total_band: tuple[float, float] = constants.FREQ_BAND_TOTAL,\n        multitaper=False,\n        **kwargs,\n    ):\n        \"\"\"Compute the power spectral density in each band as a fraction of the total power.\"\"\"\n        rec = self.get_fragment_np(index)\n\n        return FragmentAnalyzer.compute_psdfrac(\n            rec=rec,\n            f_s=self.f_s,\n            welch_bin_t=welch_bin_t,\n            notch_filter=notch_filter,\n            bands=bands,\n            total_band=total_band,\n            multitaper=multitaper,\n            **kwargs,\n        )\n\n    def compute_logpsdfrac(\n        self,\n        index,\n        welch_bin_t=1,\n        notch_filter=True,\n        bands: list[tuple[float, float]] = constants.FREQ_BANDS,\n        total_band: tuple[float, float] = constants.FREQ_BAND_TOTAL,\n        multitaper=False,\n        **kwargs,\n    ):\n        \"\"\"Compute the log of the power spectral density in each band as a fraction of the total power.\"\"\"\n        rec = self.get_fragment_np(index)\n\n        return FragmentAnalyzer.compute_logpsdfrac(\n            rec=rec,\n            f_s=self.f_s,\n            welch_bin_t=welch_bin_t,\n            notch_filter=notch_filter,\n            bands=bands,\n            total_band=total_band,\n            multitaper=multitaper,\n            **kwargs,\n        )\n\n    def compute_psdslope(\n        self,\n        index,\n        welch_bin_t=1,\n        notch_filter=True,\n        band: tuple[float, float] = constants.FREQ_BAND_TOTAL,\n        multitaper=False,\n        **kwargs,\n    ):\n        \"\"\"Compute the slope of the power spectral density of the signal.\n\n        Args:\n            index (int): Index of time window\n            welch_bin_t (float, optional): Length of time bins to use in Welch's method, in seconds. Defaults to 1.\n            notch_filter (bool, optional): If True, applies notch filter at line frequency. Defaults to True.\n            band (tuple[float, float], optional): Frequency band to calculate over. Defaults to constants.FREQ_BAND_TOTAL.\n            multitaper (bool, optional): If True, uses multitaper method instead of Welch's method. Defaults to False.\n\n        Returns:\n            np.ndarray: Array of shape (M,2) where M is number of channels. Each row contains [slope, intercept] of log-log fit.\n        \"\"\"\n        rec = self.get_fragment_np(index)\n\n        return FragmentAnalyzer.compute_psdslope(\n            rec=rec,\n            f_s=self.f_s,\n            welch_bin_t=welch_bin_t,\n            notch_filter=notch_filter,\n            band=band,\n            multitaper=multitaper,\n            **kwargs,\n        )\n\n    def convert_idx_to_timebound(self, index: int) -&gt; tuple[float, float]:\n        \"\"\"Convert fragment index to timebound (start time, end time)\n\n        Args:\n            index (int): Fragment index\n\n        Returns:\n            tuple[float, float]: Timebound in seconds\n        \"\"\"\n        frag_len_idx = round(self.fragment_len_s * self.f_s)\n        startidx = frag_len_idx * index\n        endidx = min(frag_len_idx * (index + 1), self.LongRecording.LongRecording.get_num_frames())\n        return (startidx / self.f_s, endidx / self.f_s)\n\n    def compute_cohere(\n        self,\n        index,\n        freq_res: float = 1,\n        mode: Literal[\"cwt_morlet\", \"multitaper\"] = \"multitaper\",\n        geomspace: bool = False,\n        cwt_n_cycles_max: float = 7.0,\n        mt_bandwidth: float = 4.0,\n        downsamp_q: int = 4,\n        epsilon: float = 1e-2,\n        **kwargs,\n    ) -&gt; np.ndarray:\n        rec = self.get_fragment_np(index)\n        return FragmentAnalyzer.compute_cohere(\n            rec=rec,\n            f_s=self.f_s,\n            freq_res=freq_res,\n            mode=mode,\n            geomspace=geomspace,\n            cwt_n_cycles_max=cwt_n_cycles_max,\n            mt_bandwidth=mt_bandwidth,\n            downsamp_q=downsamp_q,\n            epsilon=epsilon,\n            **kwargs,\n        )\n\n    def compute_zcohere(self, index, z_epsilon: float = 1e-6, **kwargs) -&gt; np.ndarray:\n        \"\"\"Compute the Fisher z-transformed coherence of the signal.\n\n        Args:\n            index (int): Index of time window\n            z_epsilon (float): Small value to prevent arctanh(1) = inf. Values are clipped to [-1+z_epsilon, 1-z_epsilon]\n            **kwargs: Additional arguments passed to compute_zcohere\n        \"\"\"\n        rec = self.get_fragment_np(index)\n        return FragmentAnalyzer.compute_zcohere(rec=rec, f_s=self.f_s, z_epsilon=z_epsilon, **kwargs)\n\n    def compute_imcoh(self, index, **kwargs) -&gt; np.ndarray:\n        rec = self.get_fragment_np(index)\n        return FragmentAnalyzer.compute_imcoh(rec=rec, f_s=self.f_s, **kwargs)\n\n    def compute_zimcoh(self, index, z_epsilon: float = 1e-6, **kwargs) -&gt; np.ndarray:\n        rec = self.get_fragment_np(index)\n        return FragmentAnalyzer.compute_zimcoh(rec=rec, f_s=self.f_s, z_epsilon=z_epsilon, **kwargs)\n\n    def compute_pcorr(self, index, lower_triag=False, **kwargs) -&gt; np.ndarray:\n        rec = self.get_fragment_np(index)\n        return FragmentAnalyzer.compute_pcorr(rec=rec, f_s=self.f_s, lower_triag=lower_triag, **kwargs)\n\n    def compute_zpcorr(self, index, z_epsilon: float = 1e-6, **kwargs) -&gt; np.ndarray:\n        \"\"\"Compute the Fisher z-transformed Pearson correlation coefficient of the signal.\n\n        Args:\n            index (int): Index of time window\n            z_epsilon (float): Small value to prevent arctanh(1) = inf. Values are clipped to [-1+z_epsilon, 1-z_epsilon]\n            **kwargs: Additional arguments passed to compute_zpcorr\n        \"\"\"\n        rec = self.get_fragment_np(index)\n        return FragmentAnalyzer.compute_zpcorr(rec=rec, f_s=self.f_s, z_epsilon=z_epsilon, **kwargs)\n\n    def compute_nspike(self, index, **kwargs):\n        rec = self.get_fragment_np(index)\n        return FragmentAnalyzer.compute_nspike(rec=rec, f_s=self.f_s, **kwargs)\n\n    def compute_lognspike(self, index, **kwargs):\n        rec = self.get_fragment_np(index)\n        return FragmentAnalyzer.compute_lognspike(rec=rec, f_s=self.f_s, **kwargs)\n</code></pre>"},{"location":"reference/core/analysis/#neurodent.core.analysis.LongRecordingAnalyzer.compute_ampvar","title":"<code>compute_ampvar(index, **kwargs)</code>","text":"<p>Compute average amplitude variance</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of time window</p> required <p>Returns:</p> Name Type Description <code>result</code> <p>np.ndarray with shape (1, M), M = number of channels</p> Source code in <code>src/neurodent/core/analysis.py</code> <pre><code>def compute_ampvar(self, index, **kwargs):\n    \"\"\"Compute average amplitude variance\n\n    Args:\n        index (int): Index of time window\n\n    Returns:\n        result: np.ndarray with shape (1, M), M = number of channels\n    \"\"\"\n    rec = self.get_fragment_np(index)\n    return FragmentAnalyzer.compute_ampvar(rec=rec, **kwargs)\n</code></pre>"},{"location":"reference/core/analysis/#neurodent.core.analysis.LongRecordingAnalyzer.compute_logampvar","title":"<code>compute_logampvar(index, **kwargs)</code>","text":"<p>Compute the log of the amplitude variance</p> Source code in <code>src/neurodent/core/analysis.py</code> <pre><code>def compute_logampvar(self, index, **kwargs):\n    \"\"\"Compute the log of the amplitude variance\"\"\"\n    rec = self.get_fragment_np(index)\n    return FragmentAnalyzer.compute_logampvar(rec=rec, **kwargs)\n</code></pre>"},{"location":"reference/core/analysis/#neurodent.core.analysis.LongRecordingAnalyzer.compute_logpsdband","title":"<code>compute_logpsdband(index, welch_bin_t=1, notch_filter=True, bands=constants.FREQ_BANDS, multitaper=False, **kwargs)</code>","text":"<p>Compute the log of the power spectral density of the signal for each frequency band.</p> Source code in <code>src/neurodent/core/analysis.py</code> <pre><code>def compute_logpsdband(\n    self,\n    index,\n    welch_bin_t=1,\n    notch_filter=True,\n    bands: list[tuple[float, float]] = constants.FREQ_BANDS,\n    multitaper=False,\n    **kwargs,\n):\n    \"\"\"Compute the log of the power spectral density of the signal for each frequency band.\"\"\"\n    rec = self.get_fragment_np(index)\n\n    return FragmentAnalyzer.compute_logpsdband(\n        rec=rec,\n        f_s=self.f_s,\n        welch_bin_t=welch_bin_t,\n        notch_filter=notch_filter,\n        bands=bands,\n        multitaper=multitaper,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/core/analysis/#neurodent.core.analysis.LongRecordingAnalyzer.compute_logpsdfrac","title":"<code>compute_logpsdfrac(index, welch_bin_t=1, notch_filter=True, bands=constants.FREQ_BANDS, total_band=constants.FREQ_BAND_TOTAL, multitaper=False, **kwargs)</code>","text":"<p>Compute the log of the power spectral density in each band as a fraction of the total power.</p> Source code in <code>src/neurodent/core/analysis.py</code> <pre><code>def compute_logpsdfrac(\n    self,\n    index,\n    welch_bin_t=1,\n    notch_filter=True,\n    bands: list[tuple[float, float]] = constants.FREQ_BANDS,\n    total_band: tuple[float, float] = constants.FREQ_BAND_TOTAL,\n    multitaper=False,\n    **kwargs,\n):\n    \"\"\"Compute the log of the power spectral density in each band as a fraction of the total power.\"\"\"\n    rec = self.get_fragment_np(index)\n\n    return FragmentAnalyzer.compute_logpsdfrac(\n        rec=rec,\n        f_s=self.f_s,\n        welch_bin_t=welch_bin_t,\n        notch_filter=notch_filter,\n        bands=bands,\n        total_band=total_band,\n        multitaper=multitaper,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/core/analysis/#neurodent.core.analysis.LongRecordingAnalyzer.compute_logpsdtotal","title":"<code>compute_logpsdtotal(index, welch_bin_t=1, notch_filter=True, band=constants.FREQ_BAND_TOTAL, multitaper=False, **kwargs)</code>","text":"<p>Compute the log of the total power over PSD (power spectral density) plot within a specified frequency band</p> Source code in <code>src/neurodent/core/analysis.py</code> <pre><code>def compute_logpsdtotal(\n    self,\n    index,\n    welch_bin_t=1,\n    notch_filter=True,\n    band: tuple[float, float] = constants.FREQ_BAND_TOTAL,\n    multitaper=False,\n    **kwargs,\n):\n    \"\"\"Compute the log of the total power over PSD (power spectral density) plot within a specified frequency band\"\"\"\n    rec = self.get_fragment_np(index)\n\n    return FragmentAnalyzer.compute_logpsdtotal(\n        rec=rec,\n        f_s=self.f_s,\n        welch_bin_t=welch_bin_t,\n        notch_filter=notch_filter,\n        band=band,\n        multitaper=multitaper,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/core/analysis/#neurodent.core.analysis.LongRecordingAnalyzer.compute_logrms","title":"<code>compute_logrms(index, **kwargs)</code>","text":"<p>Compute the log of the root mean square amplitude</p> Source code in <code>src/neurodent/core/analysis.py</code> <pre><code>def compute_logrms(self, index, **kwargs):\n    \"\"\"Compute the log of the root mean square amplitude\"\"\"\n    rec = self.get_fragment_np(index)\n    return FragmentAnalyzer.compute_logrms(rec=rec, **kwargs)\n</code></pre>"},{"location":"reference/core/analysis/#neurodent.core.analysis.LongRecordingAnalyzer.compute_psd","title":"<code>compute_psd(index, welch_bin_t=1, notch_filter=True, multitaper=False, **kwargs)</code>","text":"<p>Compute PSD (power spectral density)</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of time window</p> required <code>welch_bin_t</code> <code>float</code> <p>Length of time bins to use in Welch's method, in seconds. Defaults to 1.</p> <code>1</code> <code>notch_filter</code> <code>bool</code> <p>If True, applies notch filter at line frequency. Defaults to True.</p> <code>True</code> <code>multitaper</code> <code>bool</code> <p>If True, uses multitaper method instead of Welch's method. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>f</code> <code>ndarray</code> <p>Array of sample frequencies</p> <code>psd</code> <code>ndarray</code> <p>Array of PSD values at sample frequencies. (X, M), X = number of sample frequencies, M = number of channels.</p> <p>If sample window length is too short, PSD is interpolated</p> Source code in <code>src/neurodent/core/analysis.py</code> <pre><code>def compute_psd(self, index, welch_bin_t=1, notch_filter=True, multitaper=False, **kwargs):\n    \"\"\"Compute PSD (power spectral density)\n\n    Args:\n        index (int): Index of time window\n        welch_bin_t (float, optional): Length of time bins to use in Welch's method, in seconds. Defaults to 1.\n        notch_filter (bool, optional): If True, applies notch filter at line frequency. Defaults to True.\n        multitaper (bool, optional): If True, uses multitaper method instead of Welch's method. Defaults to False.\n\n    Returns:\n        f (np.ndarray): Array of sample frequencies\n        psd (np.ndarray): Array of PSD values at sample frequencies. (X, M), X = number of sample frequencies, M = number of channels.\n        If sample window length is too short, PSD is interpolated\n    \"\"\"\n    rec = self.get_fragment_np(index)\n\n    f, psd = FragmentAnalyzer.compute_psd(\n        rec=rec, f_s=self.f_s, welch_bin_t=welch_bin_t, notch_filter=notch_filter, multitaper=multitaper, **kwargs\n    )\n\n    if index == self.n_fragments - 1 and self.n_fragments &gt; 1:\n        f_prev, _ = self.compute_psd(index - 1, welch_bin_t, notch_filter, multitaper)\n        psd = Akima1DInterpolator(f, psd, axis=0, extrapolate=True)(f_prev)\n        f = f_prev\n\n    return f, psd\n</code></pre>"},{"location":"reference/core/analysis/#neurodent.core.analysis.LongRecordingAnalyzer.compute_psdband","title":"<code>compute_psdband(index, welch_bin_t=1, notch_filter=True, bands=constants.FREQ_BANDS, multitaper=False, **kwargs)</code>","text":"<p>Compute power spectral density of the signal for each frequency band.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of time window</p> required <code>welch_bin_t</code> <code>float</code> <p>Length of time bins to use in Welch's method, in seconds. Defaults to 1.</p> <code>1</code> <code>notch_filter</code> <code>bool</code> <p>If True, applies notch filter at line frequency. Defaults to True.</p> <code>True</code> <code>bands</code> <code>list[tuple[float, float]]</code> <p>List of frequency bands to compute PSD for. Defaults to constants.FREQ_BANDS.</p> <code>FREQ_BANDS</code> <code>multitaper</code> <code>bool</code> <p>If True, uses multitaper method instead of Welch's method. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <p>Dictionary mapping band names to PSD values for each channel</p> Source code in <code>src/neurodent/core/analysis.py</code> <pre><code>def compute_psdband(\n    self,\n    index,\n    welch_bin_t=1,\n    notch_filter=True,\n    bands: list[tuple[float, float]] = constants.FREQ_BANDS,\n    multitaper=False,\n    **kwargs,\n):\n    \"\"\"Compute power spectral density of the signal for each frequency band.\n\n    Args:\n        index (int): Index of time window\n        welch_bin_t (float, optional): Length of time bins to use in Welch's method, in seconds. Defaults to 1.\n        notch_filter (bool, optional): If True, applies notch filter at line frequency. Defaults to True.\n        bands (list[tuple[float, float]], optional): List of frequency bands to compute PSD for. Defaults to constants.FREQ_BANDS.\n        multitaper (bool, optional): If True, uses multitaper method instead of Welch's method. Defaults to False.\n\n    Returns:\n        dict: Dictionary mapping band names to PSD values for each channel\n    \"\"\"\n\n    rec = self.get_fragment_np(index)\n\n    return FragmentAnalyzer.compute_psdband(\n        rec=rec,\n        f_s=self.f_s,\n        welch_bin_t=welch_bin_t,\n        notch_filter=notch_filter,\n        bands=bands,\n        multitaper=multitaper,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/core/analysis/#neurodent.core.analysis.LongRecordingAnalyzer.compute_psdfrac","title":"<code>compute_psdfrac(index, welch_bin_t=1, notch_filter=True, bands=constants.FREQ_BANDS, total_band=constants.FREQ_BAND_TOTAL, multitaper=False, **kwargs)</code>","text":"<p>Compute the power spectral density in each band as a fraction of the total power.</p> Source code in <code>src/neurodent/core/analysis.py</code> <pre><code>def compute_psdfrac(\n    self,\n    index,\n    welch_bin_t=1,\n    notch_filter=True,\n    bands: list[tuple[float, float]] = constants.FREQ_BANDS,\n    total_band: tuple[float, float] = constants.FREQ_BAND_TOTAL,\n    multitaper=False,\n    **kwargs,\n):\n    \"\"\"Compute the power spectral density in each band as a fraction of the total power.\"\"\"\n    rec = self.get_fragment_np(index)\n\n    return FragmentAnalyzer.compute_psdfrac(\n        rec=rec,\n        f_s=self.f_s,\n        welch_bin_t=welch_bin_t,\n        notch_filter=notch_filter,\n        bands=bands,\n        total_band=total_band,\n        multitaper=multitaper,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/core/analysis/#neurodent.core.analysis.LongRecordingAnalyzer.compute_psdslope","title":"<code>compute_psdslope(index, welch_bin_t=1, notch_filter=True, band=constants.FREQ_BAND_TOTAL, multitaper=False, **kwargs)</code>","text":"<p>Compute the slope of the power spectral density of the signal.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of time window</p> required <code>welch_bin_t</code> <code>float</code> <p>Length of time bins to use in Welch's method, in seconds. Defaults to 1.</p> <code>1</code> <code>notch_filter</code> <code>bool</code> <p>If True, applies notch filter at line frequency. Defaults to True.</p> <code>True</code> <code>band</code> <code>tuple[float, float]</code> <p>Frequency band to calculate over. Defaults to constants.FREQ_BAND_TOTAL.</p> <code>FREQ_BAND_TOTAL</code> <code>multitaper</code> <code>bool</code> <p>If True, uses multitaper method instead of Welch's method. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>np.ndarray: Array of shape (M,2) where M is number of channels. Each row contains [slope, intercept] of log-log fit.</p> Source code in <code>src/neurodent/core/analysis.py</code> <pre><code>def compute_psdslope(\n    self,\n    index,\n    welch_bin_t=1,\n    notch_filter=True,\n    band: tuple[float, float] = constants.FREQ_BAND_TOTAL,\n    multitaper=False,\n    **kwargs,\n):\n    \"\"\"Compute the slope of the power spectral density of the signal.\n\n    Args:\n        index (int): Index of time window\n        welch_bin_t (float, optional): Length of time bins to use in Welch's method, in seconds. Defaults to 1.\n        notch_filter (bool, optional): If True, applies notch filter at line frequency. Defaults to True.\n        band (tuple[float, float], optional): Frequency band to calculate over. Defaults to constants.FREQ_BAND_TOTAL.\n        multitaper (bool, optional): If True, uses multitaper method instead of Welch's method. Defaults to False.\n\n    Returns:\n        np.ndarray: Array of shape (M,2) where M is number of channels. Each row contains [slope, intercept] of log-log fit.\n    \"\"\"\n    rec = self.get_fragment_np(index)\n\n    return FragmentAnalyzer.compute_psdslope(\n        rec=rec,\n        f_s=self.f_s,\n        welch_bin_t=welch_bin_t,\n        notch_filter=notch_filter,\n        band=band,\n        multitaper=multitaper,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/core/analysis/#neurodent.core.analysis.LongRecordingAnalyzer.compute_psdtotal","title":"<code>compute_psdtotal(index, welch_bin_t=1, notch_filter=True, band=constants.FREQ_BAND_TOTAL, multitaper=False, **kwargs)</code>","text":"<p>Compute total power over PSD (power spectral density) plot within a specified frequency band</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of time window</p> required <code>welch_bin_t</code> <code>float</code> <p>Length of time bins to use in Welch's method, in seconds. Defaults to 1.</p> <code>1</code> <code>notch_filter</code> <code>bool</code> <p>If True, applies notch filter at line frequency. Defaults to True.</p> <code>True</code> <code>band</code> <code>tuple[float, float]</code> <p>Frequency band to calculate over. Defaults to constants.FREQ_BAND_TOTAL.</p> <code>FREQ_BAND_TOTAL</code> <code>multitaper</code> <code>bool</code> <p>If True, uses multitaper method instead of Welch's method. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>psdtotal</code> <code>ndarray</code> <p>(M,) long array, M = number of channels. Each value corresponds to sum total of PSD in that band at that channel</p> Source code in <code>src/neurodent/core/analysis.py</code> <pre><code>def compute_psdtotal(\n    self,\n    index,\n    welch_bin_t=1,\n    notch_filter=True,\n    band: tuple[float, float] = constants.FREQ_BAND_TOTAL,\n    multitaper=False,\n    **kwargs,\n):\n    \"\"\"Compute total power over PSD (power spectral density) plot within a specified frequency band\n\n    Args:\n        index (int): Index of time window\n        welch_bin_t (float, optional): Length of time bins to use in Welch's method, in seconds. Defaults to 1.\n        notch_filter (bool, optional): If True, applies notch filter at line frequency. Defaults to True.\n        band (tuple[float, float], optional): Frequency band to calculate over. Defaults to constants.FREQ_BAND_TOTAL.\n        multitaper (bool, optional): If True, uses multitaper method instead of Welch's method. Defaults to False.\n\n    Returns:\n        psdtotal (np.ndarray): (M,) long array, M = number of channels. Each value corresponds to sum total of PSD in that band at that channel\n    \"\"\"\n    rec = self.get_fragment_np(index)\n\n    return FragmentAnalyzer.compute_psdtotal(\n        rec=rec,\n        f_s=self.f_s,\n        welch_bin_t=welch_bin_t,\n        notch_filter=notch_filter,\n        band=band,\n        multitaper=multitaper,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/core/analysis/#neurodent.core.analysis.LongRecordingAnalyzer.compute_rms","title":"<code>compute_rms(index, **kwargs)</code>","text":"<p>Compute average root mean square amplitude</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of time window</p> required <p>Returns:</p> Name Type Description <code>result</code> <p>np.ndarray with shape (1, M), M = number of channels</p> Source code in <code>src/neurodent/core/analysis.py</code> <pre><code>def compute_rms(self, index, **kwargs):\n    \"\"\"Compute average root mean square amplitude\n\n    Args:\n        index (int): Index of time window\n\n    Returns:\n        result: np.ndarray with shape (1, M), M = number of channels\n    \"\"\"\n    rec = self.get_fragment_np(index)\n    return FragmentAnalyzer.compute_rms(rec=rec, **kwargs)\n</code></pre>"},{"location":"reference/core/analysis/#neurodent.core.analysis.LongRecordingAnalyzer.compute_zcohere","title":"<code>compute_zcohere(index, z_epsilon=1e-06, **kwargs)</code>","text":"<p>Compute the Fisher z-transformed coherence of the signal.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of time window</p> required <code>z_epsilon</code> <code>float</code> <p>Small value to prevent arctanh(1) = inf. Values are clipped to [-1+z_epsilon, 1-z_epsilon]</p> <code>1e-06</code> <code>**kwargs</code> <p>Additional arguments passed to compute_zcohere</p> <code>{}</code> Source code in <code>src/neurodent/core/analysis.py</code> <pre><code>def compute_zcohere(self, index, z_epsilon: float = 1e-6, **kwargs) -&gt; np.ndarray:\n    \"\"\"Compute the Fisher z-transformed coherence of the signal.\n\n    Args:\n        index (int): Index of time window\n        z_epsilon (float): Small value to prevent arctanh(1) = inf. Values are clipped to [-1+z_epsilon, 1-z_epsilon]\n        **kwargs: Additional arguments passed to compute_zcohere\n    \"\"\"\n    rec = self.get_fragment_np(index)\n    return FragmentAnalyzer.compute_zcohere(rec=rec, f_s=self.f_s, z_epsilon=z_epsilon, **kwargs)\n</code></pre>"},{"location":"reference/core/analysis/#neurodent.core.analysis.LongRecordingAnalyzer.compute_zpcorr","title":"<code>compute_zpcorr(index, z_epsilon=1e-06, **kwargs)</code>","text":"<p>Compute the Fisher z-transformed Pearson correlation coefficient of the signal.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of time window</p> required <code>z_epsilon</code> <code>float</code> <p>Small value to prevent arctanh(1) = inf. Values are clipped to [-1+z_epsilon, 1-z_epsilon]</p> <code>1e-06</code> <code>**kwargs</code> <p>Additional arguments passed to compute_zpcorr</p> <code>{}</code> Source code in <code>src/neurodent/core/analysis.py</code> <pre><code>def compute_zpcorr(self, index, z_epsilon: float = 1e-6, **kwargs) -&gt; np.ndarray:\n    \"\"\"Compute the Fisher z-transformed Pearson correlation coefficient of the signal.\n\n    Args:\n        index (int): Index of time window\n        z_epsilon (float): Small value to prevent arctanh(1) = inf. Values are clipped to [-1+z_epsilon, 1-z_epsilon]\n        **kwargs: Additional arguments passed to compute_zpcorr\n    \"\"\"\n    rec = self.get_fragment_np(index)\n    return FragmentAnalyzer.compute_zpcorr(rec=rec, f_s=self.f_s, z_epsilon=z_epsilon, **kwargs)\n</code></pre>"},{"location":"reference/core/analysis/#neurodent.core.analysis.LongRecordingAnalyzer.convert_idx_to_timebound","title":"<code>convert_idx_to_timebound(index)</code>","text":"<p>Convert fragment index to timebound (start time, end time)</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Fragment index</p> required <p>Returns:</p> Type Description <code>tuple[float, float]</code> <p>tuple[float, float]: Timebound in seconds</p> Source code in <code>src/neurodent/core/analysis.py</code> <pre><code>def convert_idx_to_timebound(self, index: int) -&gt; tuple[float, float]:\n    \"\"\"Convert fragment index to timebound (start time, end time)\n\n    Args:\n        index (int): Fragment index\n\n    Returns:\n        tuple[float, float]: Timebound in seconds\n    \"\"\"\n    frag_len_idx = round(self.fragment_len_s * self.f_s)\n    startidx = frag_len_idx * index\n    endidx = min(frag_len_idx * (index + 1), self.LongRecording.LongRecording.get_num_frames())\n    return (startidx / self.f_s, endidx / self.f_s)\n</code></pre>"},{"location":"reference/core/analysis/#neurodent.core.analysis.LongRecordingAnalyzer.get_fragment_mne","title":"<code>get_fragment_mne(index, recobj=None)</code>","text":"<p>Get window at index as a numpy array object, formatted for ease of use with MNE functions</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of time window</p> required <code>recobj</code> <code>BaseRecording</code> <p>If not None, uses this recording object to get the numpy array. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Numpy array with dimensions (1, M, N), M = number of channels, N = number of samples. 1st dimension corresponds to number of epochs, which there is only 1 in a window. Values in uV</p> Source code in <code>src/neurodent/core/analysis.py</code> <pre><code>def get_fragment_mne(self, index, recobj=None) -&gt; np.ndarray:\n    \"\"\"Get window at index as a numpy array object, formatted for ease of use with MNE functions\n\n    Args:\n        index (int): Index of time window\n        recobj (si.BaseRecording, optional): If not None, uses this recording object to get the numpy array. Defaults to None.\n\n    Returns:\n        np.ndarray: Numpy array with dimensions (1, M, N), M = number of channels, N = number of samples. 1st dimension corresponds\n         to number of epochs, which there is only 1 in a window. Values in uV\n    \"\"\"\n    rec = self.get_fragment_np(index, recobj=recobj)[..., np.newaxis]\n    return np.transpose(rec, (2, 1, 0))  # (1 epoch, num_channels, num_samples)\n</code></pre>"},{"location":"reference/core/analysis/#neurodent.core.analysis.LongRecordingAnalyzer.get_fragment_np","title":"<code>get_fragment_np(index, recobj=None)</code>","text":"<p>Get window at index as a numpy array object</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of time window</p> required <code>recobj</code> <code>BaseRecording</code> <p>If not None, uses this recording object to get the numpy array. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Numpy array with dimensions (N, M), N = number of samples, M = number of channels. Values in uV</p> Source code in <code>src/neurodent/core/analysis.py</code> <pre><code>def get_fragment_np(self, index, recobj=None) -&gt; np.ndarray:\n    \"\"\"Get window at index as a numpy array object\n\n    Args:\n        index (int): Index of time window\n        recobj (si.BaseRecording, optional): If not None, uses this recording object to get the numpy array. Defaults to None.\n\n    Returns:\n        np.ndarray: Numpy array with dimensions (N, M), N = number of samples, M = number of channels. Values in uV\n    \"\"\"\n    if si is not None:\n        assert isinstance(recobj, si.BaseRecording) or recobj is None\n    if recobj is None:\n        return self.get_fragment_rec(index).get_traces(\n            return_scaled=True\n        )  # (num_samples, num_channels), in units uV\n    else:\n        return recobj.get_traces(return_scaled=True)\n</code></pre>"},{"location":"reference/core/analysis/#neurodent.core.analysis.LongRecordingAnalyzer.get_fragment_rec","title":"<code>get_fragment_rec(index)</code>","text":"<p>Get window at index as a spikeinterface recording object</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of time window</p> required <p>Returns:</p> Type Description <code>BaseRecording</code> <p>si.BaseRecording: spikeinterface recording object with optional notch filtering applied</p> Source code in <code>src/neurodent/core/analysis.py</code> <pre><code>def get_fragment_rec(self, index) -&gt; \"si.BaseRecording\":\n    \"\"\"Get window at index as a spikeinterface recording object\n\n    Args:\n        index (int): Index of time window\n\n    Returns:\n        si.BaseRecording: spikeinterface recording object with optional notch filtering applied\n    \"\"\"\n    if si is None:\n        raise ImportError(\"spikeinterface is required for get_fragment_rec\")\n\n    rec = self.LongRecording.get_fragment(self.fragment_len_s, index)\n\n    if self.apply_notch_filter and spre is not None:\n        rec = spre.notch_filter(rec, freq=constants.LINE_FREQ)\n\n    return rec\n</code></pre>"},{"location":"reference/core/core/","title":"Core Classes","text":""},{"location":"reference/core/core/#neurodent.core.core.DDFBinaryMetadata","title":"<code>DDFBinaryMetadata</code>","text":"Source code in <code>src/neurodent/core/core.py</code> <pre><code>class DDFBinaryMetadata:\n    def __init__(\n        self,\n        metadata_path: str | Path | None,\n        *,\n        n_channels: int | None = None,\n        f_s: float | None = None,\n        dt_end: datetime | None = None,\n        channel_names: list[str] | None = None,\n        V_units: str | None = None,\n        mult_to_uV: float | None = None,\n    ) -&gt; None:\n        \"\"\"Initialize DDFBinaryMetadata either from a file path or direct parameters.\n\n        Args:\n            metadata_path (str, optional): Path to metadata CSV file. If provided, other parameters are ignored.\n            n_channels (int, optional): Number of channels\n            f_s (float, optional): Sampling frequency in Hz\n            dt_end (datetime, optional): End datetime of recording\n            channel_names (list, optional): List of channel names\n            V_units (str, optional): Voltage units (e.g., '\u00b5V', 'mV', 'V')\n            mult_to_uV (float, optional): Multiplication factor to convert to microvolts\n        \"\"\"\n        if metadata_path is not None:\n            self._init_from_path(metadata_path)\n        else:\n            self._init_from_params(n_channels, f_s, dt_end, channel_names, V_units, mult_to_uV)\n\n    def _init_from_path(self, metadata_path):\n        self.metadata_path = metadata_path\n        self.metadata_df = pd.read_csv(metadata_path)\n        if self.metadata_df.empty:\n            raise ValueError(f\"Metadata file is empty: {metadata_path}\")\n\n        self.n_channels = len(self.metadata_df.index)\n        self.f_s = self.__getsinglecolval(\n            \"SampleRate\"\n        )  # NOTE this may not be the same as LongRecording (Recording object) f_s, which the name should reflect\n        self.V_units = self.__getsinglecolval(\"Units\")\n        self.mult_to_uV = convert_units_to_multiplier(self.V_units)\n        self.precision = self.__getsinglecolval(\"Precision\")\n\n        if \"LastEdit\" in self.metadata_df.keys():\n            self.dt_end = datetime.fromisoformat(self.__getsinglecolval(\"LastEdit\"))\n        else:\n            self.dt_end = None\n            logging.warning(\"No LastEdit column provided in metadata. dt_end set to None\")\n\n        self.channel_names = self.metadata_df[\"ProbeInfo\"].tolist()\n\n    def _init_from_params(self, n_channels, f_s, dt_end, channel_names, V_units=None, mult_to_uV=None):\n        if None in (n_channels, f_s, channel_names):\n            raise ValueError(\"All parameters must be provided when not using metadata_path\")\n\n        self.metadata_path = None\n        self.metadata_df = None\n        self.n_channels = n_channels\n        self.f_s = f_s  # NOTE see above note about f_s\n        self.V_units = V_units\n        self.mult_to_uV = mult_to_uV\n        self.precision = None\n        self.dt_end = dt_end\n\n        if not isinstance(channel_names, list):\n            raise ValueError(\"channel_names must be a list\")\n\n        self.channel_names = channel_names\n\n    def __getsinglecolval(self, colname):\n        vals = self.metadata_df.loc[:, colname]\n        if len(np.unique(vals)) &gt; 1:\n            warnings.warn(f\"Not all {colname}s are equal!\")\n        if vals.size == 0:\n            return None\n        return vals.iloc[0]\n\n    def to_dict(self) -&gt; dict:\n        \"\"\"Convert DDFBinaryMetadata to a dictionary for JSON serialization.\"\"\"\n        return {\n            \"metadata_path\": str(self.metadata_path) if self.metadata_path else None,\n            \"n_channels\": self.n_channels,\n            \"f_s\": self.f_s,\n            \"V_units\": self.V_units,\n            \"mult_to_uV\": self.mult_to_uV,\n            \"precision\": self.precision,\n            \"dt_end\": self.dt_end.isoformat() if self.dt_end else None,\n            \"channel_names\": self.channel_names,\n        }\n\n    @classmethod\n    def from_dict(cls, data: dict) -&gt; \"DDFBinaryMetadata\":\n        \"\"\"Create DDFBinaryMetadata from a dictionary (from JSON deserialization).\"\"\"\n        dt_end = datetime.fromisoformat(data[\"dt_end\"]) if data[\"dt_end\"] else None\n\n        return cls(\n            metadata_path=None,  # We're reconstructing from cached data\n            n_channels=data[\"n_channels\"],\n            f_s=data[\"f_s\"],\n            dt_end=dt_end,\n            channel_names=data[\"channel_names\"],\n            V_units=data.get(\"V_units\"),\n            mult_to_uV=data.get(\"mult_to_uV\"),\n        )\n\n    def to_json(self, file_path: Path) -&gt; None:\n        \"\"\"Save DDFBinaryMetadata to a JSON file.\"\"\"\n        with open(file_path, \"w\") as f:\n            json.dump(self.to_dict(), f, indent=2)\n\n    @classmethod\n    def from_json(cls, file_path: Path) -&gt; \"DDFBinaryMetadata\":\n        \"\"\"Load DDFBinaryMetadata from a JSON file.\"\"\"\n        with open(file_path, \"r\") as f:\n            data = json.load(f)\n\n        # Reconstruct the object, preserving additional fields that were serialized\n        instance = cls.from_dict(data)\n\n        # Set additional fields that might not be in from_dict\n        instance.V_units = data.get(\"V_units\")\n        instance.mult_to_uV = data.get(\"mult_to_uV\")\n        instance.precision = data.get(\"precision\")\n\n        return instance\n\n    def update_sampling_rate(self, new_f_s: float) -&gt; None:\n        \"\"\"Update the sampling rate in this metadata object.\n\n        This should be called when the associated recording is resampled.\n        \"\"\"\n        old_f_s = self.f_s\n        self.f_s = new_f_s\n        logging.info(f\"Updated DDFBinaryMetadata sampling rate from {old_f_s} Hz to {new_f_s} Hz\")\n</code></pre>"},{"location":"reference/core/core/#neurodent.core.core.DDFBinaryMetadata.__init__","title":"<code>__init__(metadata_path, *, n_channels=None, f_s=None, dt_end=None, channel_names=None, V_units=None, mult_to_uV=None)</code>","text":"<p>Initialize DDFBinaryMetadata either from a file path or direct parameters.</p> <p>Parameters:</p> Name Type Description Default <code>metadata_path</code> <code>str</code> <p>Path to metadata CSV file. If provided, other parameters are ignored.</p> required <code>n_channels</code> <code>int</code> <p>Number of channels</p> <code>None</code> <code>f_s</code> <code>float</code> <p>Sampling frequency in Hz</p> <code>None</code> <code>dt_end</code> <code>datetime</code> <p>End datetime of recording</p> <code>None</code> <code>channel_names</code> <code>list</code> <p>List of channel names</p> <code>None</code> <code>V_units</code> <code>str</code> <p>Voltage units (e.g., '\u00b5V', 'mV', 'V')</p> <code>None</code> <code>mult_to_uV</code> <code>float</code> <p>Multiplication factor to convert to microvolts</p> <code>None</code> Source code in <code>src/neurodent/core/core.py</code> <pre><code>def __init__(\n    self,\n    metadata_path: str | Path | None,\n    *,\n    n_channels: int | None = None,\n    f_s: float | None = None,\n    dt_end: datetime | None = None,\n    channel_names: list[str] | None = None,\n    V_units: str | None = None,\n    mult_to_uV: float | None = None,\n) -&gt; None:\n    \"\"\"Initialize DDFBinaryMetadata either from a file path or direct parameters.\n\n    Args:\n        metadata_path (str, optional): Path to metadata CSV file. If provided, other parameters are ignored.\n        n_channels (int, optional): Number of channels\n        f_s (float, optional): Sampling frequency in Hz\n        dt_end (datetime, optional): End datetime of recording\n        channel_names (list, optional): List of channel names\n        V_units (str, optional): Voltage units (e.g., '\u00b5V', 'mV', 'V')\n        mult_to_uV (float, optional): Multiplication factor to convert to microvolts\n    \"\"\"\n    if metadata_path is not None:\n        self._init_from_path(metadata_path)\n    else:\n        self._init_from_params(n_channels, f_s, dt_end, channel_names, V_units, mult_to_uV)\n</code></pre>"},{"location":"reference/core/core/#neurodent.core.core.DDFBinaryMetadata.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create DDFBinaryMetadata from a dictionary (from JSON deserialization).</p> Source code in <code>src/neurodent/core/core.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict) -&gt; \"DDFBinaryMetadata\":\n    \"\"\"Create DDFBinaryMetadata from a dictionary (from JSON deserialization).\"\"\"\n    dt_end = datetime.fromisoformat(data[\"dt_end\"]) if data[\"dt_end\"] else None\n\n    return cls(\n        metadata_path=None,  # We're reconstructing from cached data\n        n_channels=data[\"n_channels\"],\n        f_s=data[\"f_s\"],\n        dt_end=dt_end,\n        channel_names=data[\"channel_names\"],\n        V_units=data.get(\"V_units\"),\n        mult_to_uV=data.get(\"mult_to_uV\"),\n    )\n</code></pre>"},{"location":"reference/core/core/#neurodent.core.core.DDFBinaryMetadata.from_json","title":"<code>from_json(file_path)</code>  <code>classmethod</code>","text":"<p>Load DDFBinaryMetadata from a JSON file.</p> Source code in <code>src/neurodent/core/core.py</code> <pre><code>@classmethod\ndef from_json(cls, file_path: Path) -&gt; \"DDFBinaryMetadata\":\n    \"\"\"Load DDFBinaryMetadata from a JSON file.\"\"\"\n    with open(file_path, \"r\") as f:\n        data = json.load(f)\n\n    # Reconstruct the object, preserving additional fields that were serialized\n    instance = cls.from_dict(data)\n\n    # Set additional fields that might not be in from_dict\n    instance.V_units = data.get(\"V_units\")\n    instance.mult_to_uV = data.get(\"mult_to_uV\")\n    instance.precision = data.get(\"precision\")\n\n    return instance\n</code></pre>"},{"location":"reference/core/core/#neurodent.core.core.DDFBinaryMetadata.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert DDFBinaryMetadata to a dictionary for JSON serialization.</p> Source code in <code>src/neurodent/core/core.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Convert DDFBinaryMetadata to a dictionary for JSON serialization.\"\"\"\n    return {\n        \"metadata_path\": str(self.metadata_path) if self.metadata_path else None,\n        \"n_channels\": self.n_channels,\n        \"f_s\": self.f_s,\n        \"V_units\": self.V_units,\n        \"mult_to_uV\": self.mult_to_uV,\n        \"precision\": self.precision,\n        \"dt_end\": self.dt_end.isoformat() if self.dt_end else None,\n        \"channel_names\": self.channel_names,\n    }\n</code></pre>"},{"location":"reference/core/core/#neurodent.core.core.DDFBinaryMetadata.to_json","title":"<code>to_json(file_path)</code>","text":"<p>Save DDFBinaryMetadata to a JSON file.</p> Source code in <code>src/neurodent/core/core.py</code> <pre><code>def to_json(self, file_path: Path) -&gt; None:\n    \"\"\"Save DDFBinaryMetadata to a JSON file.\"\"\"\n    with open(file_path, \"w\") as f:\n        json.dump(self.to_dict(), f, indent=2)\n</code></pre>"},{"location":"reference/core/core/#neurodent.core.core.DDFBinaryMetadata.update_sampling_rate","title":"<code>update_sampling_rate(new_f_s)</code>","text":"<p>Update the sampling rate in this metadata object.</p> <p>This should be called when the associated recording is resampled.</p> Source code in <code>src/neurodent/core/core.py</code> <pre><code>def update_sampling_rate(self, new_f_s: float) -&gt; None:\n    \"\"\"Update the sampling rate in this metadata object.\n\n    This should be called when the associated recording is resampled.\n    \"\"\"\n    old_f_s = self.f_s\n    self.f_s = new_f_s\n    logging.info(f\"Updated DDFBinaryMetadata sampling rate from {old_f_s} Hz to {new_f_s} Hz\")\n</code></pre>"},{"location":"reference/core/core/#neurodent.core.core.LongRecordingOrganizer","title":"<code>LongRecordingOrganizer</code>","text":"Source code in <code>src/neurodent/core/core.py</code> <pre><code>class LongRecordingOrganizer:\n    def __init__(\n        self,\n        base_folder_path,\n        mode: Literal[\"bin\", \"si\", \"mne\", None] = \"bin\",\n        truncate: Union[bool, int] = False,\n        cache_policy: Literal[\"auto\", \"always\", \"force_regenerate\"] = \"auto\",\n        multiprocess_mode: Literal[\"dask\", \"serial\"] = \"serial\",\n        extract_func: Union[Callable[..., \"si.BaseRecording\"], Callable[..., mne.io.Raw]] = None,\n        input_type: Literal[\"folder\", \"file\", \"files\"] = \"folder\",\n        file_pattern: str = None,\n        manual_datetimes: datetime | list[datetime] = None,\n        datetimes_are_start: bool = True,\n        n_jobs: int = 1,\n        **kwargs,\n    ):\n        \"\"\"Construct a long recording from binary files or EDF files.\n\n        Args:\n            base_folder_path (str): Path to the base folder containing the data files.\n            mode (Literal['bin', 'si', 'mne', None]): Mode to load data in. Defaults to 'bin'.\n            truncate (Union[bool, int], optional): If True, truncate data to first 10 files.\n                If an integer, truncate data to the first n files. Defaults to False.\n            overwrite_rowbins (bool, optional): If True, overwrite existing row-major binary files. Defaults to False.\n            multiprocess_mode (Literal['dask', 'serial'], optional): Processing mode for parallel operations. Defaults to 'serial'.\n            extract_func (Callable, optional): Function to extract data when using 'si' or 'mne' mode. Required for those modes.\n            input_type (Literal['folder', 'file', 'files'], optional): Type of input to load. Defaults to 'folder'.\n            file_pattern (str, optional): Pattern to match files when using 'file' or 'files' input type. Defaults to '*'.\n            manual_datetimes (datetime | list[datetime], optional): Manual timestamps for the recording.\n                For 'bin' mode: if datetime, used as global start/end time; if list, one timestamp per file.\n                For 'si'/'mne' modes: if datetime, used as start/end of entire recording; if list, one per input file.\n            datetimes_are_start (bool, optional): If True, manual_datetimes are treated as start times.\n                If False, treated as end times. Defaults to True.\n            n_jobs (int, optional): Number of jobs for MNE resampling operations. Defaults to 1 for safety.\n                Set to -1 for automatic parallel detection, or &gt;1 for specific job count.\n            **kwargs: Additional arguments passed to the data loading functions.\n\n        Raises:\n            ValueError: If no data files are found, if the folder contains mixed file types,\n                or if manual time parameters are invalid.\n        \"\"\"\n\n        self.base_folder_path = Path(base_folder_path)\n\n        self.n_truncate = parse_truncate(truncate)\n        self.truncate = True if self.n_truncate &gt; 0 else False\n        if self.truncate:\n            warnings.warn(f\"LongRecording will be truncated to the first {self.n_truncate} files\")\n\n        # Store manual time parameters for validation\n        self.manual_datetimes = manual_datetimes\n        self.datetimes_are_start = datetimes_are_start\n\n        # Store n_jobs parameter for MNE operations\n        self.n_jobs = n_jobs\n\n        # Validate manual time parameters\n        self._validate_manual_time_params()\n\n        # Initialize core attributes\n        self.meta = None\n        self.channel_names = None\n        self.LongRecording = None\n        self.temppaths = []\n        self.file_durations = []\n        self.cumulative_file_durations = []\n        self.bad_channel_names = []\n\n        # Load data if mode is specified\n        if mode is not None:\n            self.detect_and_load_data(\n                mode=mode,\n                cache_policy=cache_policy,\n                multiprocess_mode=multiprocess_mode,\n                extract_func=extract_func,\n                input_type=input_type,\n                file_pattern=file_pattern,\n                **kwargs,\n            )\n\n    def detect_and_load_data(\n        self,\n        mode: Literal[\"bin\", \"si\", \"mne\", None] = \"bin\",\n        cache_policy: Literal[\"auto\", \"always\", \"force_regenerate\"] = \"auto\",\n        multiprocess_mode: Literal[\"dask\", \"serial\"] = \"serial\",\n        extract_func: Union[Callable[..., \"si.BaseRecording\"], Callable[..., mne.io.Raw]] = None,\n        input_type: Literal[\"folder\", \"file\", \"files\"] = \"folder\",\n        file_pattern: str = None,\n        **kwargs,\n    ):\n        \"\"\"Load in recording based on mode.\"\"\"\n\n        if mode == \"bin\":\n            # Binary file pipeline\n            self.convert_colbins_rowbins_to_rec(\n                cache_policy=cache_policy,\n                multiprocess_mode=multiprocess_mode,\n            )\n        elif mode == \"si\":\n            # EDF file pipeline\n            self.convert_file_with_si_to_recording(\n                extract_func=extract_func,\n                input_type=input_type,\n                file_pattern=file_pattern,\n                cache_policy=cache_policy,\n                **kwargs,\n            )\n        elif mode == \"mne\":\n            # MNE file pipeline\n            self.convert_file_with_mne_to_recording(\n                extract_func=extract_func,\n                input_type=input_type,\n                file_pattern=file_pattern,\n                cache_policy=cache_policy,\n                n_jobs=self.n_jobs,\n                **kwargs,\n            )\n        elif mode is None:\n            pass\n        else:\n            raise ValueError(f\"Invalid mode: {mode}\")\n\n    def prepare_colbins_rowbins_metas(self):\n        self.colbin_folder_path = self.base_folder_path\n        self.rowbin_folder_path = self.base_folder_path\n\n        self.__update_colbins_rowbins_metas()\n        self.__check_colbins_rowbins_metas_folders_exist()\n        self.__check_colbins_rowbins_metas_not_empty()\n\n        self.meta = DDFBinaryMetadata(self.metas[0])\n        self.__metadata_objects = [DDFBinaryMetadata(x) for x in self.metas]\n        self._validate_metadata_consistency(self.__metadata_objects)\n\n        self.channel_names = self.meta.channel_names\n\n        # Initialize file_end_datetimes from CSV metadata (will be overridden later if manual times provided)\n        file_end_datetimes = [x.dt_end for x in self.__metadata_objects]\n        if all(x is None for x in file_end_datetimes):\n            # If no CSV times available, manual times will be required later\n            self.file_end_datetimes = file_end_datetimes\n        else:\n            self.file_end_datetimes = file_end_datetimes\n            logging.info(\n                f\"CSV metadata timestamps: {len([x for x in file_end_datetimes if x is not None])} of {len(file_end_datetimes)} files have timestamps\"\n            )\n\n    def _truncate_file_list(\n        self, files: list[Union[str, Path]], ref_list: list[Union[str, Path]] = None\n    ) -&gt; list[Union[str, Path]]:\n        \"\"\"Unified method to truncate any list of files.\n\n        Args:\n            files: List of files to truncate\n            ref_list: Optional list of files to maintain relationships between. Only stems will be compared.\n        \"\"\"\n\n        if not ref_list:\n            if not self.truncate or len(files) &lt;= self.n_truncate:\n                return files\n\n            # Sort and truncate primary files\n            truncated = sorted(files)[: self.n_truncate]\n            return truncated\n        else:\n            # Get a subset of files that match with ref_list\n            ref_list_stems = [get_file_stem(f) for f in ref_list]\n            files = [f for f in files if get_file_stem(f) in ref_list_stems]\n            return files\n\n    def __update_colbins_rowbins_metas(self):\n        self.colbins = glob.glob(str(self.colbin_folder_path / \"*_ColMajor.bin\"))\n        self.rowbins = glob.glob(str(self.rowbin_folder_path / \"*_RowMajor.npy.gz\"))\n        self.metas = glob.glob(str(self.colbin_folder_path / \"*_Meta.csv\"))\n\n        self.colbins.sort(key=filepath_to_index)\n        self.rowbins.sort(key=filepath_to_index)\n        self.metas.sort(key=filepath_to_index)\n\n        logging.debug(\n            f\"Before prune: {len(self.colbins)} colbins, {len(self.rowbins)} rowbins, {len(self.metas)} metas\"\n        )\n        self.__prune_empty_files()\n        logging.debug(f\"After prune: {len(self.colbins)} colbins, {len(self.rowbins)} rowbins, {len(self.metas)} metas\")\n        if len(self.colbins) != len(self.metas):\n            logging.warning(\"Number of column-major and metadata files do not match\")\n\n        metadatas = [DDFBinaryMetadata(x) for x in self.metas]\n        for meta in metadatas:\n            # if metadata file is empty, remove it and the corresponding column-major and row-major files\n            if meta.metadata_df.empty:\n                searchstr = Path(meta.metadata_path).name.replace(\"_Meta\", \"\")\n                self.colbins = [x for x in self.colbins if searchstr + \"_ColMajor.bin\" not in x]\n                self.rowbins = [x for x in self.rowbins if searchstr + \"_RowMajor.npy.gz\" not in x]\n                self.metas = [x for x in self.metas if searchstr + \"_Meta.csv\" not in x]\n\n        # if truncate is True, truncate the lists\n        if self.truncate:\n            self.colbins = self._truncate_file_list(self.colbins)\n            self.rowbins = self._truncate_file_list(\n                self.rowbins, ref_list=[x.replace(\"_ColMajor.bin\", \"_RowMajor.npy.gz\") for x in self.colbins]\n            )\n            self.metas = self._truncate_file_list(\n                self.metas, ref_list=[x.replace(\"_ColMajor.bin\", \"_Meta.csv\") for x in self.colbins]\n            )\n\n    def __prune_empty_files(self):\n        # if the column-major file is empty, remove the corresponding row-major and metadata files\n        colbins = self.colbins.copy()\n        for i, e in enumerate(colbins):\n            if Path(e).stat().st_size == 0:\n                name = Path(e).name.replace(\"_ColMajor.bin\", \"\")\n                logging.debug(f\"Removing {name}\")\n                self.colbins.remove(e)\n                self.rowbins = [x for x in self.rowbins if name + \"_RowMajor.npy.gz\" not in x]\n                self.metas = [x for x in self.metas if name + \"_Meta.csv\" not in x]\n        # remove None values\n        self.colbins = [x for x in self.colbins if x is not None]\n        self.rowbins = [x for x in self.rowbins if x is not None]\n        self.metas = [x for x in self.metas if x is not None]\n\n    def __check_colbins_rowbins_metas_folders_exist(self):\n        if not self.colbin_folder_path.exists():\n            raise FileNotFoundError(f\"Column-major binary files folder not found: {self.colbin_folder_path}\")\n        if not self.rowbin_folder_path.exists():\n            logging.warning(f\"Row-major binary files folder not found: {self.rowbin_folder_path}\")\n        if not self.metas:\n            raise FileNotFoundError(f\"Metadata files folder not found: {self.metas}\")\n\n    def __check_colbins_rowbins_metas_not_empty(self):\n        if not self.colbins:\n            raise ValueError(\"No column-major binary files found\")\n        if not self.rowbins:\n            warnings.warn(\"No row-major binary files found. Convert with convert_colbins_to_rowbins()\")\n        if not self.metas:\n            raise ValueError(\"No metadata files found\")\n\n    def _validate_metadata_consistency(self, metadatas: list[DDFBinaryMetadata]):\n        meta0 = metadatas[0]\n        # attributes = ['f_s', 'n_channels', 'precision', 'V_units', 'channel_names']\n        attributes = [\"n_channels\", \"precision\", \"V_units\", \"channel_names\"]\n        for attr in attributes:\n            if not all([getattr(meta0, attr) == getattr(x, attr) for x in metadatas]):\n                unequal_values = [getattr(x, attr) for x in metadatas if getattr(x, attr) != getattr(meta0, attr)]\n                logging.error(\n                    f\"Inconsistent {attr} values across metadata files: {getattr(meta0, attr)} != {unequal_values}\"\n                )\n                raise ValueError(f\"Metadata files inconsistent at attribute {attr}\")\n        return\n\n    def convert_colbins_rowbins_to_rec(\n        self,\n        overwrite_rowbins: bool = False,\n        multiprocess_mode: Literal[\"dask\", \"serial\"] = \"serial\",\n        cache_policy: Literal[\"auto\", \"always\", \"force_regenerate\"] = \"auto\",\n    ):\n        self.prepare_colbins_rowbins_metas()\n        self.convert_colbins_to_rowbins(overwrite=overwrite_rowbins, multiprocess_mode=multiprocess_mode)\n        self.convert_rowbins_to_rec(multiprocess_mode=multiprocess_mode, cache_policy=cache_policy)\n        # Now that file_durations are available, finalize timestamps\n        self.finalize_file_timestamps()\n\n    def convert_colbins_to_rowbins(self, overwrite=False, multiprocess_mode: Literal[\"dask\", \"serial\"] = \"serial\"):\n        \"\"\"\n        Convert column-major binary files to row-major binary files, and save them in the rowbin_folder_path.\n\n        Args:\n            overwrite (bool, optional): If True, overwrite existing row-major binary files. Defaults to True.\n            multiprocess_mode (Literal['dask', 'serial'], optional): If 'dask', use dask to convert the files in parallel.\n                If 'serial', convert the files in serial. Defaults to 'serial'.\n        \"\"\"\n\n        # if overwrite, regenerate regardless of existence\n        # else, read them (they exist) or make them (they don't exist)\n        # there is no error condition, and rowbins will be recreated regardless of choice\n\n        logging.info(f\"Converting {len(self.colbins)} column-major binary files to row-major format\")\n        if overwrite:\n            logging.info(\"Overwrite flag set - regenerating all row-major files\")\n        else:\n            logging.info(\"Overwrite flag not set - only generating missing row-major files\")\n\n        delayed = []\n        for i, e in enumerate(self.colbins):\n            if convert_colpath_to_rowpath(self.rowbin_folder_path, e, aspath=False) not in self.rowbins or overwrite:\n                logging.info(f\"Converting {e}\")\n                match multiprocess_mode:\n                    case \"dask\":\n                        delayed.append(\n                            dask.delayed(convert_ddfcolbin_to_ddfrowbin)(\n                                self.rowbin_folder_path, e, self.meta, save_gzip=True\n                            )\n                        )\n                    case \"serial\":\n                        convert_ddfcolbin_to_ddfrowbin(self.rowbin_folder_path, e, self.meta, save_gzip=True)\n                    case _:\n                        raise ValueError(f\"Invalid multiprocess_mode: {multiprocess_mode}\")\n\n        if multiprocess_mode == \"dask\":\n            # Run all conversions in parallel\n            dask.compute(*delayed)\n\n        self.__update_colbins_rowbins_metas()\n\n    def convert_rowbins_to_rec(\n        self,\n        multiprocess_mode: Literal[\"dask\", \"serial\"] = \"serial\",\n        cache_policy: Literal[\"auto\", \"always\", \"force_regenerate\"] = \"auto\",\n    ):\n        \"\"\"\n        Convert row-major binary files to SpikeInterface Recording structure.\n\n        Args:\n            multiprocess_mode (Literal['dask', 'serial'], optional): If 'dask', use dask to convert the files in parallel.\n                If 'serial', convert the files in serial. Defaults to 'serial'.\n            cache_policy (Literal): Caching policy for intermediate files (default: \"auto\")\n                - \"auto\": Use cached files if exist and newer than sources, regenerate with logging if missing/invalid\n                - \"always\": Use cached files if exist, raise error if missing/invalid\n                - \"force_regenerate\": Always regenerate files, overwrite existing cache\n        \"\"\"\n        if si is None:\n            raise ImportError(\"SpikeInterface is required for convert_rowbins_to_rec\")\n        if len(self.rowbins) &lt; len(self.colbins):\n            warnings.warn(\n                f\"{len(self.colbins)} column-major files found, but only {len(self.rowbins)} row-major files found. Some column-major files may be missing.\"\n            )\n        elif len(self.rowbins) &gt; len(self.colbins):\n            warnings.warn(\n                f\"{len(self.rowbins)} row-major files found, but only {len(self.colbins)} column-major files found. Some row-major files will be ignored.\"\n            )\n\n        recs = []\n        t_cumulative = 0\n        self.temppaths = []\n\n        match multiprocess_mode:\n            case \"dask\":\n                # Compute all conversions in parallel\n                delayed_results = []\n                for i, e in enumerate(self.rowbins):\n                    delayed_results.append((i, dask.delayed(_convert_ddfrowbin_to_si_no_resample)(e, self.meta)))\n                computed_results = dask.compute(*delayed_results)\n\n                # Reconstruct results in the correct order\n                results = [None] * len(self.rowbins)\n                for i, result in computed_results:\n                    results[i] = result\n                logging.info(f\"self.rowbins: {[Path(x).name for x in self.rowbins]}\")\n\n            case \"serial\":\n                results = [_convert_ddfrowbin_to_si_no_resample(e, self.meta) for e in self.rowbins]\n            case _:\n                raise ValueError(f\"Invalid multiprocess_mode: {multiprocess_mode}\")\n\n        # Process results\n        for i, (rec, temppath) in enumerate(results):\n            recs.append(rec)\n            self.temppaths.append(temppath)\n\n            duration = rec.get_duration()\n            self.file_durations.append(duration)\n\n            t_cumulative += duration  # NOTE  use numpy cumsum later\n            self.cumulative_file_durations.append(t_cumulative)\n\n        if not recs:\n            raise ValueError(\"No recordings generated. Check that all row-major files are present and readable.\")\n        elif len(recs) &lt; len(self.rowbins):\n            logging.warning(f\"Only {len(recs)} recordings generated. Some row-major files may be missing.\")\n\n        # Concatenate recordings first\n        concatenated_recording = si.concatenate_recordings(recs).rename_channels(self.channel_names)\n\n        # Apply unified resampling to the concatenated recording\n        self.LongRecording: \"si.BaseRecording\" = self._apply_resampling(concatenated_recording)\n\n        # Debug logging for critical recording features\n        logging.info(f\"LongRecording created: {self}\")\n\n    def convert_file_with_si_to_recording(\n        self,\n        extract_func: Callable[..., \"si.BaseRecording\"],\n        input_type: Literal[\"folder\", \"file\", \"files\"] = \"folder\",\n        file_pattern: str = \"*\",\n        cache_policy: Literal[\"auto\", \"always\", \"force_regenerate\"] = \"auto\",\n        **kwargs,\n    ):\n        \"\"\"Create a SpikeInterface Recording from a folder, a single file, or multiple files.\n\n        This is a thin wrapper around ``extract_func`` that discovers inputs under\n        ``self.base_folder_path`` and builds a ``si.BaseRecording`` accordingly.\n\n        Modes:\n        - ``folder``: Passes ``self.base_folder_path`` directly to ``extract_func``.\n        - ``file``: Uses ``glob`` with ``file_pattern`` relative to ``self.base_folder_path``.\n          If multiple matches are found, the first match is used and a warning is issued.\n        - ``files``: Uses ``Path.glob`` with ``file_pattern`` under ``self.base_folder_path``,\n          optionally truncates via ``self._truncate_file_list(...)``, sorts the files, applies\n          ``extract_func`` to each file, and concatenates the resulting recordings via\n          ``si.concatenate_recordings``.\n\n        Args:\n            extract_func (Callable[..., \"si.BaseRecording\"]): Function that consumes a path\n                (folder or file path) and returns a ``si.BaseRecording``.\n            input_type (Literal['folder', 'file', 'files'], optional): How to discover inputs.\n                Defaults to ``'folder'``.\n            file_pattern (str, optional): Glob pattern used when ``input_type`` is ``'file'`` or\n                ``'files'``. Defaults to ``'*'``.\n            **kwargs: Additional keyword arguments forwarded to ``extract_func``.\n\n        Side Effects:\n            Sets ``self.LongRecording`` to the resulting recording and initializes ``self.meta``\n            based on that recording's properties.\n\n        Raises:\n            ValueError: If no files are found for the given ``file_pattern`` or ``input_type`` is invalid.\n        \"\"\"\n        if si is None:\n            raise ImportError(\"SpikeInterface is required for convert_file_with_si_to_recording\")\n        # Early validation and file discovery\n        if input_type == \"folder\":\n            # For single folder, validate that timestamps are provided\n            self._validate_timestamps_for_mode(\"si\", 1)\n            datafolder = self.base_folder_path\n            rec: \"si.BaseRecording\" = extract_func(datafolder, **kwargs)\n            n_processed_files = 1\n        elif input_type == \"file\":\n            # For single file, validate that timestamps are provided\n            self._validate_timestamps_for_mode(\"si\", 1)\n            datafiles = glob.glob(str(self.base_folder_path / file_pattern))\n            if len(datafiles) == 0:\n                raise ValueError(f\"No files found matching pattern: {file_pattern}\")\n            elif len(datafiles) &gt; 1:\n                warnings.warn(f\"Multiple files found matching pattern: {file_pattern}. Using first file.\")\n            datafile = datafiles[0]\n            rec: \"si.BaseRecording\" = extract_func(datafile, **kwargs)\n            n_processed_files = 1\n        elif input_type == \"files\":\n            datafiles = [str(x) for x in self.base_folder_path.glob(file_pattern)]\n            if len(datafiles) == 0:\n                raise ValueError(f\"No files found matching pattern: {file_pattern}\")\n            datafiles = self._truncate_file_list(datafiles)\n            # Validate timestamps early before slow processing\n            self._validate_timestamps_for_mode(\"si\", len(datafiles))\n            datafiles.sort()  # FIXME sort by index, or some other logic. Files may be out of order otherwise, messing up isday calculation\n            recs: list[\"si.BaseRecording\"] = [extract_func(x, **kwargs) for x in datafiles]\n            rec = si.concatenate_recordings(recs)\n            n_processed_files = len(datafiles)\n        else:\n            raise ValueError(f\"Invalid input_type: {input_type}\")\n\n        # Store number of processed files for timestamp handling\n        self._n_processed_files = n_processed_files\n\n        # Apply unified resampling to the recording\n        self.LongRecording = self._apply_resampling(rec)\n\n        # For SI mode, don't use confusing DEFAULT_DAY if we have manual timestamps\n        dt_end = None if self.manual_datetimes is not None else None  # Will be set by finalize_file_timestamps\n\n        self.meta = DDFBinaryMetadata(\n            None,\n            n_channels=self.LongRecording.get_num_channels(),\n            f_s=self.LongRecording.get_sampling_frequency(),\n            dt_end=dt_end,  # Will be properly set by finalize_file_timestamps\n            channel_names=self.LongRecording.get_channel_ids().tolist(),  # NOTE may potentially be a list of integers, which is undesirable. The ability to set names is available in the extractor function itself\n            # In the case this is integers, raise a warning and/or error, convert to string, and make a note that you may need to adjust parameters in si extractor\n        )\n        self.channel_names = self.meta.channel_names\n\n        # For si mode, handle multiple files or single file\n        if not hasattr(self, \"file_durations\") or not self.file_durations:\n            if hasattr(self, \"_n_processed_files\") and self._n_processed_files &gt; 1:\n                # Multiple files concatenated - estimate equal durations\n                total_duration = self.LongRecording.get_duration()\n                avg_duration = total_duration / self._n_processed_files\n                self.file_durations = [avg_duration] * self._n_processed_files\n            else:\n                # Single file or folder\n                self.file_durations = [self.LongRecording.get_duration()]\n            self.file_end_datetimes = []\n\n        # Apply manual timestamps if provided\n        self.finalize_file_timestamps()\n\n        # Debug logging for critical recording features\n        logging.debug(f\"LongRecording created via SI: {self}\")\n\n    def _load_and_process_mne_data(\n        self, extract_func, input_type, datafolder, datafile, datafiles, n_jobs, metadata_to_update=None, **kwargs\n    ) -&gt; mne.io.Raw:\n        \"\"\"Helper method to load and process MNE data from various input types.\"\"\"\n        # Load data based on input type\n        if input_type == \"folder\":\n            raw: mne.io.Raw = extract_func(datafolder, **kwargs)\n        elif input_type == \"file\":\n            raw: mne.io.Raw = extract_func(datafile, **kwargs)\n        elif input_type == \"files\":\n            logging.info(f\"Running extract_func on {len(datafiles)} files\")\n            raws: list[mne.io.Raw] = [extract_func(x, **kwargs) for x in datafiles]\n            logging.info(f\"Concatenating {len(raws)} raws\")\n            raw: mne.io.Raw = mne.concatenate_raws(raws)\n            del raws\n        else:\n            raise ValueError(f\"Invalid input_type: {input_type}\")\n\n        logging.info(f\"raw.info: {raw.info}\")\n\n        # Use user-specified n_jobs for MNE resampling, or default to 1\n        effective_n_jobs = n_jobs if n_jobs is not None else self.n_jobs\n        logging.info(\n            f\"Using n_jobs={effective_n_jobs} for MNE resampling (method param: {n_jobs}, instance: {self.n_jobs})\"\n        )\n\n        # Ensure data is preloaded for parallel processing\n        if not raw.preload:\n            logging.info(\"Preloading data\")\n            raw.load_data()\n\n        # Use optimal resampling method with power-of-2 padding for speed\n        original_sfreq = raw.info[\"sfreq\"]\n        if original_sfreq != constants.GLOBAL_SAMPLING_RATE:\n            logging.info(f\"Resampling from {original_sfreq} to {constants.GLOBAL_SAMPLING_RATE}\")\n            raw = raw.resample(constants.GLOBAL_SAMPLING_RATE, n_jobs=effective_n_jobs, npad=\"auto\", method=\"fft\")\n\n            # Update metadata to reflect the new sampling rate\n            if metadata_to_update is not None:\n                metadata_to_update.update_sampling_rate(constants.GLOBAL_SAMPLING_RATE)\n        else:\n            logging.info(\n                f\"Sampling frequency already matches {constants.GLOBAL_SAMPLING_RATE} Hz, no resampling needed\"\n            )\n\n        return raw\n\n    def _load_mne_data_no_resample(\n        self, extract_func, input_type, datafolder, datafile, datafiles, **kwargs\n    ) -&gt; mne.io.Raw:\n        \"\"\"Load MNE data without resampling for unified resampling pipeline.\n\n        This method loads and concatenates MNE data but skips resampling,\n        allowing the unified resampling to be applied after intermediate file creation.\n        \"\"\"\n        # Load data based on input type\n        if input_type == \"folder\":\n            raw: mne.io.Raw = extract_func(datafolder, **kwargs)\n        elif input_type == \"file\":\n            raw: mne.io.Raw = extract_func(datafile, **kwargs)\n        elif input_type == \"files\":\n            logging.info(f\"Running extract_func on {len(datafiles)} files\")\n            raws: list[mne.io.Raw] = [extract_func(x, **kwargs) for x in datafiles]\n            logging.info(f\"Concatenating {len(raws)} raws\")\n            raw: mne.io.Raw = mne.concatenate_raws(raws)\n            del raws\n        else:\n            raise ValueError(f\"Invalid input_type: {input_type}\")\n\n        logging.info(f\"raw.info: {raw.info}\")\n\n        # Ensure data is preloaded\n        if not raw.preload:\n            logging.info(\"Preloading data\")\n            raw.load_data()\n\n        # NOTE: No resampling applied here - will be handled by unified resampling after loading from cache\n        logging.info(\n            f\"Data loaded at original sampling rate ({raw.info['sfreq']} Hz) - resampling will be applied later\"\n        )\n\n        return raw\n\n    def _get_or_create_intermediate_file(\n        self,\n        fname,\n        source_paths,\n        cache_policy,\n        intermediate,\n        extract_func,\n        input_type,\n        datafolder,\n        datafile,\n        datafiles,\n        n_jobs,\n        **kwargs,\n    ):\n        \"\"\"Get cached intermediate file or create it if needed.\n\n        Returns:\n            tuple: (recording, raw_object, metadata) where:\n                - recording: SpikeInterface recording object\n                - raw_object: MNE Raw object (None if using cache)\n                - metadata: DDFBinaryMetadata object\n        \"\"\"\n        # Define metadata sidecar file path\n        meta_fname = fname.with_suffix(fname.suffix + \".meta.json\")\n\n        # Check cache policy and validate cache files\n        if cache_policy == \"force_regenerate\":\n            use_cache = False\n            logging.info(get_cache_status_message(fname, False))\n            logging.info(\"Cache policy 'force_regenerate': ignoring any existing cache\")\n        else:\n            # Check if both data and metadata cache files exist and are valid\n            data_cache_valid = should_use_cache_unified(fname, source_paths, cache_policy)\n            meta_cache_valid = meta_fname.exists() if data_cache_valid else False\n\n            # Handle cache validation based on policy\n            if not data_cache_valid or not meta_cache_valid:\n                if cache_policy == \"always\":\n                    # 'always' policy: raise error if cache missing/invalid\n                    missing_files = []\n                    if not data_cache_valid:\n                        missing_files.append(f\"intermediate file ({fname})\")\n                    if not meta_cache_valid:\n                        missing_files.append(f\"metadata sidecar ({meta_fname})\")\n                    raise FileNotFoundError(\n                        f\"Cache policy 'always' requires existing cache files, but missing: {', '.join(missing_files)}\"\n                    )\n                elif cache_policy == \"auto\":\n                    # 'auto' policy: log and regenerate if cache missing/invalid\n                    if not data_cache_valid:\n                        logging.info(f\"Intermediate file {fname} missing or outdated, regenerating\")\n                    if not meta_cache_valid:\n                        logging.info(f\"Metadata sidecar {meta_fname} missing, regenerating\")\n                    use_cache = False\n                else:\n                    use_cache = False\n            else:\n                use_cache = True\n\n            if use_cache:\n                logging.info(get_cache_status_message(fname, True))\n                logging.info(f\"Loading cached metadata from {meta_fname}\")\n\n                # Load metadata from sidecar file\n                try:\n                    metadata = DDFBinaryMetadata.from_json(meta_fname)\n                    logging.info(f\"Loaded cached metadata: {metadata.n_channels} channels, {metadata.f_s} Hz\")\n                except Exception as e:\n                    if cache_policy == \"always\":\n                        # 'always' policy: raise error if metadata invalid\n                        logging.error(\n                            f\"Cache policy 'always' requires valid metadata, but failed to load {meta_fname}: {e}\"\n                        )\n                        raise\n                    elif cache_policy == \"auto\":\n                        # 'auto' policy: log and regenerate if metadata invalid\n                        logging.info(f\"Failed to load cached metadata from {meta_fname}: {e}\")\n                        logging.info(\"Regenerating intermediate files due to invalid metadata\")\n                        use_cache = False\n\n        if use_cache:\n            # Load cached data file\n            if intermediate == \"edf\":\n                logging.info(\"Reading cached edf file\")\n                rec = se.read_edf(fname)\n                return rec, None, metadata  # No raw object when using cache\n\n            elif intermediate == \"bin\":\n                # Use metadata to reconstruct SpikeInterface parameters\n                params = {\n                    \"sampling_frequency\": metadata.f_s,\n                    \"num_channels\": metadata.n_channels,\n                    \"dtype\": \"float64\",  # We standardize on float64 for cached binary files\n                    \"gain_to_uV\": 1,\n                    \"offset_to_uV\": 0,\n                    \"time_axis\": 0,\n                    \"is_filtered\": False,\n                }\n\n                logging.info(f\"Reading from cached binary file {fname}\")\n                rec = se.read_binary(fname, **params)\n                return rec, None, metadata  # No raw object when using cache\n\n        else:\n            # Generate new intermediate files\n            logging.info(get_cache_status_message(fname, False))\n\n            # Create metadata object from raw info BEFORE resampling\n            # We need to load one file to get the original metadata\n            if input_type == \"folder\":\n                sample_raw = extract_func(datafolder, **kwargs)\n            elif input_type == \"file\":\n                sample_raw = extract_func(datafile, **kwargs)\n            elif input_type == \"files\":\n                sample_raw = extract_func(datafiles[0], **kwargs)\n            else:\n                raise ValueError(f\"Invalid input_type: {input_type}\")\n\n            # Create metadata from the original raw object (before resampling)\n            original_info = sample_raw.info\n\n            # Extract unit information from MNE Raw object\n            unit_str, mult_to_uv = extract_mne_unit_info(original_info)\n\n            metadata = DDFBinaryMetadata(\n                metadata_path=None,\n                n_channels=original_info[\"nchan\"],\n                f_s=original_info[\"sfreq\"],  # Original sampling rate\n                dt_end=None,  # Will be set later by finalize_file_timestamps\n                channel_names=original_info[\"ch_names\"],\n                V_units=unit_str,\n                mult_to_uV=mult_to_uv,\n            )\n            logging.info(f\"Created metadata from raw: {metadata.n_channels} channels, {metadata.f_s} Hz\")\n            if unit_str and mult_to_uv:\n                logging.info(f\"Extracted unit information: {unit_str} (mult_to_uV = {mult_to_uv})\")\n            else:\n                logging.warning(\"No unit information could be extracted from MNE Raw object\")\n\n            # Load data without resampling (resampling will be applied after intermediate file loading)\n            raw = self._load_mne_data_no_resample(extract_func, input_type, datafolder, datafile, datafiles, **kwargs)\n\n            # Create the intermediate file\n            if intermediate == \"edf\":\n                logging.info(f\"Exporting raw to {fname}\")\n                mne.export.export_raw(fname, raw=raw, fmt=\"edf\", overwrite=True)\n\n                logging.info(\"Reading edf file\")\n                rec = se.read_edf(fname)\n\n            elif intermediate == \"bin\":\n                # Get raw info for SpikeInterface parameters\n                raw_info = raw.info\n                params = {\n                    \"sampling_frequency\": raw_info[\"sfreq\"],\n                    \"num_channels\": raw_info[\"nchan\"],\n                    \"gain_to_uV\": 1,\n                    \"offset_to_uV\": 0,\n                    \"time_axis\": 0,\n                    \"is_filtered\": False,\n                }\n\n                logging.info(f\"Exporting raw to {fname}\")\n                data: np.ndarray = raw.get_data()  # (n channels, n samples)\n                data = data.T  # (n samples, n channels)\n                params[\"dtype\"] = data.dtype\n                logging.info(f\"Writing to {fname}\")\n                data.tofile(fname)\n\n                logging.info(f\"Reading from {fname}\")\n                rec = se.read_binary(fname, **params)\n\n            else:\n                raise ValueError(f\"Invalid intermediate: {intermediate}\")\n\n            # Save metadata sidecar file\n            logging.info(f\"Saving metadata to {meta_fname}\")\n            metadata.to_json(meta_fname)\n\n            return rec, raw, metadata\n\n    def convert_file_with_mne_to_recording(\n        self,\n        extract_func: Callable[..., mne.io.Raw],\n        input_type: Literal[\"folder\", \"file\", \"files\"] = \"folder\",\n        file_pattern: str = \"*\",\n        intermediate: Literal[\"edf\", \"bin\"] = \"edf\",\n        intermediate_name=None,\n        cache_policy: Literal[\"auto\", \"always\", \"force_regenerate\"] = \"auto\",\n        multiprocess_mode: Literal[\"dask\", \"serial\"] = \"serial\",\n        n_jobs: int = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Convert MNE-compatible files to SpikeInterface recording format with metadata caching.\n\n        Args:\n            extract_func (Callable): Function that takes a file path and returns mne.io.Raw object\n            input_type (Literal): Type of input - \"folder\", \"file\", or \"files\"\n            file_pattern (str): Glob pattern for file matching (default: \"*\")\n            intermediate (Literal): Intermediate format - \"edf\" or \"bin\" (default: \"edf\")\n            intermediate_name (str, optional): Custom name for intermediate file\n            cache_policy (Literal): Caching policy for intermediate and metadata files (default: \"auto\")\n                - \"auto\": Use cached files if both data and metadata exist and cache is newer than sources, regenerate with logging if missing/invalid\n                - \"always\": Use cached files if both data and metadata exist, raise error if missing/invalid\n                - \"force_regenerate\": Always regenerate files, overwrite existing cache\n            multiprocess_mode (Literal): Processing mode - \"dask\" or \"serial\" (default: \"serial\")\n            n_jobs (int, optional): Number of jobs for MNE resampling. If None (default),\n                                uses the instance n_jobs value. Set to -1 for automatic parallel\n                                detection, or &gt;1 for specific job count.\n            **kwargs: Additional arguments passed to extract_func\n\n        Note:\n            Creates two cache files: data file (e.g., file.edf) and metadata sidecar (e.g., file.edf.meta.json).\n            Both files must exist for cache to be used. Metadata preserves channel names, original\n            sampling rates, and other DDFBinaryMetadata fields across cache hits.\n        \"\"\"\n        if se is None:\n            raise ImportError(\"SpikeInterface is required for convert_file_with_mne_to_recording\")\n        # Early validation and file discovery\n        if input_type == \"folder\":\n            self._validate_timestamps_for_mode(\"mne\", 1)\n            datafolder = self.base_folder_path\n            datafile = None\n            datafiles = None\n            source_paths = [self.base_folder_path]\n            n_processed_files = 1\n\n        elif input_type == \"file\":\n            self._validate_timestamps_for_mode(\"mne\", 1)\n            datafiles = list(self.base_folder_path.glob(file_pattern))\n            if len(datafiles) == 0:\n                raise ValueError(f\"No files found matching pattern: {file_pattern}\")\n            elif len(datafiles) &gt; 1:\n                warnings.warn(f\"Multiple files found matching pattern: {file_pattern}. Using first file.\")\n            datafile = datafiles[0]\n            datafolder = None\n            source_paths = [datafile]\n            n_processed_files = 1\n\n        elif input_type == \"files\":\n            datafiles = list(self.base_folder_path.glob(file_pattern))\n            if len(datafiles) == 0:\n                raise ValueError(f\"No files found matching pattern: {file_pattern}\")\n            datafiles = self._truncate_file_list(datafiles)\n            self._validate_timestamps_for_mode(\"mne\", len(datafiles))\n            datafiles.sort()\n            datafolder = None\n            datafile = None\n            source_paths = datafiles\n            n_processed_files = len(datafiles)\n\n        else:\n            raise ValueError(f\"Invalid input_type: {input_type}\")\n\n        # Store number of processed files for timestamp handling\n        self._n_processed_files = n_processed_files\n\n        # Determine intermediate file path\n        intermediate_name = (\n            f\"{self.base_folder_path.name}_mne-to-rec\" if intermediate_name is None else intermediate_name\n        )\n        fname = self.base_folder_path / f\"{intermediate_name}.{intermediate}\"\n\n        # Get or create the intermediate file (this handles caching logic)\n        rec, _, metadata = self._get_or_create_intermediate_file(\n            fname=fname,\n            source_paths=source_paths,\n            cache_policy=cache_policy,\n            intermediate=intermediate,\n            extract_func=extract_func,\n            input_type=input_type,\n            datafolder=datafolder,\n            datafile=datafile,\n            datafiles=datafiles,\n            n_jobs=n_jobs,\n            **kwargs,\n        )\n\n        # Set metadata first so resampling can update it\n        self.meta = metadata\n\n        # Apply unified resampling to the loaded recording (this will update metadata sampling rate)\n        self.LongRecording = self._apply_resampling(rec)\n\n        # Update dt_end for manual timestamps (will be properly set by finalize_file_timestamps)\n        if self.manual_datetimes is not None:\n            self.meta.dt_end = None  # Will be set by finalize_file_timestamps\n        self.channel_names = self.meta.channel_names\n\n        # For mne mode, handle multiple files or single file\n        if not hasattr(self, \"file_durations\") or not self.file_durations:\n            if hasattr(self, \"_n_processed_files\") and self._n_processed_files &gt; 1:\n                # Multiple files concatenated - estimate equal durations\n                total_duration = self.LongRecording.get_duration()\n                avg_duration = total_duration / self._n_processed_files\n                self.file_durations = [avg_duration] * self._n_processed_files\n            else:\n                # Single file or folder\n                self.file_durations = [self.LongRecording.get_duration()]\n            self.file_end_datetimes = []\n\n        # Apply manual timestamps if provided\n        self.finalize_file_timestamps()\n\n        # Debug logging for critical recording features\n        logging.debug(f\"LongRecording created via MNE: {self}\")\n\n    def cleanup_rec(self):\n        try:\n            del self.LongRecording\n        except AttributeError:\n            logging.warning(\"LongRecording does not exist, probably deleted already\")\n        for tpath in self.temppaths:\n            Path.unlink(tpath)\n\n    def get_num_fragments(self, fragment_len_s):\n        frag_len_idx = self.__time_to_idx(fragment_len_s)\n        duration_idx = self.LongRecording.get_num_frames()\n        return math.ceil(duration_idx / frag_len_idx)\n\n    def __time_to_idx(self, time_s):\n        return self.LongRecording.time_to_sample_index(time_s)\n\n    def __idx_to_time(self, idx):\n        return self.LongRecording.sample_index_to_time(idx)\n\n    def get_fragment(self, fragment_len_s, fragment_idx):\n        startidx, endidx = self.__fragidx_to_startendind(fragment_len_s, fragment_idx)\n        return self.LongRecording.frame_slice(startidx, endidx)\n\n    def get_dur_fragment(self, fragment_len_s, fragment_idx):\n        startidx, endidx = self.__fragidx_to_startendind(fragment_len_s, fragment_idx)\n        return self.__idx_to_time(endidx) - self.__idx_to_time(startidx)\n\n    def get_datetime_fragment(self, fragment_len_s, fragment_idx):\n        \"\"\"\n        Get the datetime for a specific fragment using the timestamp mapper.\n\n        Args:\n            fragment_len_s (float): Length of each fragment in seconds\n            fragment_idx (int): Index of the fragment to get datetime for\n\n        Returns:\n            datetime: The datetime corresponding to the start of the fragment\n\n        Raises:\n            ValueError: If timestamp mapper is not initialized (only available in 'bin' mode)\n        \"\"\"\n        return TimestampMapper(self.file_end_datetimes, self.file_durations).get_fragment_timestamp(\n            fragment_idx, fragment_len_s\n        )\n\n    def __fragidx_to_startendind(self, fragment_len_s, fragment_idx):\n        \"\"\"Convert fragment index to start and end sample indices.\n\n        Args:\n            fragment_len_s (float): Length of each fragment in seconds\n            fragment_idx (int): Index of the fragment to get indices for\n\n        Returns:\n            tuple[int, int]: Start and end sample indices for the fragment. The end index is capped at the recording length.\n        \"\"\"\n        frag_len_idx = self.__time_to_idx(fragment_len_s)\n        startidx = frag_len_idx * fragment_idx\n        endidx = min(frag_len_idx * (fragment_idx + 1), self.LongRecording.get_num_frames())\n        return startidx, endidx\n\n    def convert_to_mne(self) -&gt; mne.io.RawArray:\n        \"\"\"Convert this LongRecording object to an MNE RawArray.\n\n        Returns:\n            mne.io.RawArray: The converted MNE RawArray\n        \"\"\"\n        data = self.LongRecording.get_traces(return_scaled=True)  # This gets data in (n_samples, n_channels) format\n        data = data.T  # Convert to (n_channels, n_samples) format for MNE\n\n        info = mne.create_info(\n            ch_names=self.channel_names, sfreq=self.LongRecording.get_sampling_frequency(), ch_types=\"eeg\"\n        )\n\n        return mne.io.RawArray(data=data, info=info)\n\n    def compute_bad_channels(\n        self, lof_threshold: float = None, limit_memory: bool = True, force_recompute: bool = False\n    ):\n        \"\"\"Compute bad channels using LOF analysis with unified score storage.\n\n        Args:\n            lof_threshold (float, optional): Threshold for determining bad channels from LOF scores.\n                                           If None, only computes/loads scores without setting bad_channel_names.\n            limit_memory (bool): Whether to reduce memory usage by decimation and float16.\n            force_recompute (bool): Whether to recompute LOF scores even if they exist.\n        \"\"\"\n        # Check if LOF scores already exist and are current\n        if not force_recompute and hasattr(self, \"lof_scores\") and self.lof_scores is not None:\n            logging.info(\"Using existing LOF scores\")\n        else:\n            # Compute new LOF scores\n            try:\n                scores = self._compute_lof_scores(limit_memory=limit_memory)\n                self.lof_scores = scores\n                logging.info(f\"Computed LOF scores for {len(scores)} channels\")\n            except Exception as e:\n                logging.error(f\"Failed to compute LOF scores for recording: {e}\")\n                raise\n\n        # Apply threshold if provided\n        if lof_threshold is not None:\n            self.apply_lof_threshold(lof_threshold)\n\n    def _compute_lof_scores(self, limit_memory: bool = True) -&gt; np.ndarray:\n        \"\"\"Compute raw LOF scores for all channels.\n\n        Args:\n            limit_memory (bool): Whether to reduce memory usage.\n\n        Returns:\n            np.ndarray: LOF scores for each channel.\n        \"\"\"\n        try:\n            nn = Natural_Neighbor()\n            rec = self.LongRecording\n\n            logging.debug(f\"Computing LOF scores for {rec.__str__()}\")\n            rec_np = rec.get_traces(return_scaled=True)  # (n_samples, n_channels)\n\n            if rec_np is None or rec_np.size == 0:\n                logging.error(\"Failed to get traces from recording - data is None or empty\")\n                raise ValueError(\"Recording traces are None or empty\")\n            logging.debug(f\"Got recording shape: {rec_np.shape}\")\n\n            if limit_memory:\n                rec_np = rec_np.astype(np.float16)\n                rec_np = decimate(rec_np, 10, axis=0)\n            logging.debug(f\"Decimated traces shape: {rec_np.shape}\")\n            rec_np = rec_np.T  # (n_channels, n_samples)\n            logging.debug(f\"Transposed traces shape: {rec_np.shape}\")\n\n            # Compute the optimal number of neighbors\n            nn.read(rec_np)\n            n_neighbors = nn.algorithm()\n            logging.info(f\"Computed n_neighbors for LOF computation: {n_neighbors}\")\n\n            # Initialize LocalOutlierFactor\n            # lof = LocalOutlierFactor(n_neighbors=n_neighbors, metric=\"minkowski\", p=2)\n            # distance_vector = pdist(rec_np, metric=\"seuclidean\")\n            distance_vector = pdist(rec_np, metric=\"euclidean\")\n            distance_matrix = squareform(distance_vector)\n            lof = LocalOutlierFactor(n_neighbors=n_neighbors, metric=\"precomputed\")\n            # lof = LocalOutlierFactor(n_neighbors=n_neighbors, metric=pdist, )\n\n            # Compute the outlier scores\n            logging.debug(\"Computing outlier scores\")\n            del nn\n            # lof.fit(rec_np)\n            lof.fit(distance_matrix)\n            del rec_np\n            scores = lof.negative_outlier_factor_ * -1\n            logging.info(f\"LOF computation successful: {len(scores)} channels\")\n            logging.debug(f\"LOF scores: {scores}\")\n\n            return scores\n\n        except Exception as e:\n            logging.error(f\"Failed to compute LOF scores: {e}\")\n            logging.error(\n                f\"Recording info: channels={getattr(self, 'channel_names', 'unknown')}, \"\n                f\"duration={getattr(rec, 'duration', 'unknown') if 'rec' in locals() else 'unknown'}\"\n            )\n            raise\n\n    def apply_lof_threshold(self, lof_threshold: float):\n        \"\"\"Apply threshold to existing LOF scores to determine bad channels.\n\n        Args:\n            lof_threshold (float): Threshold for determining bad channels.\n        \"\"\"\n        if not hasattr(self, \"lof_scores\") or self.lof_scores is None:\n            raise ValueError(\"LOF scores not available. Run compute_bad_channels() first.\")\n\n        is_inlier = self.lof_scores &lt; lof_threshold\n        self.bad_channel_names = [self.channel_names[i] for i in np.where(~is_inlier)[0]]\n        logging.info(f\"Applied threshold {lof_threshold}: bad_channel_names = {self.bad_channel_names}\")\n\n    def get_lof_scores(self) -&gt; dict:\n        \"\"\"Get LOF scores with channel names.\n\n        Returns:\n            dict: Dictionary mapping channel names to LOF scores.\n        \"\"\"\n        if not hasattr(self, \"lof_scores\") or self.lof_scores is None:\n            raise ValueError(\"LOF scores not available. Run compute_bad_channels() first.\")\n\n        return dict(zip(self.channel_names, self.lof_scores))\n\n    def _validate_manual_time_params(self):\n        \"\"\"Validate that manual time parameters are correctly specified.\"\"\"\n        if self.manual_datetimes is not None:\n            if not isinstance(self.manual_datetimes, (datetime, list, tuple)):\n                raise ValueError(\"manual_datetimes must be a datetime object or list of datetime objects\")\n\n    def _validate_timestamps_for_mode(self, mode: str, expected_n_files: int = None):\n        \"\"\"Validate that manual timestamps are provided when required for specific modes.\n\n        Args:\n            mode (str): The processing mode ('si', 'mne', or 'bin')\n            expected_n_files (int, optional): Expected number of files for validation\n\n        Raises:\n            ValueError: If timestamps are required but not provided or if count mismatch\n        \"\"\"\n        if mode in [\"si\", \"mne\"]:\n            if self.manual_datetimes is None:\n                raise ValueError(f\"manual_datetimes must be provided for {mode} mode when no CSV metadata is available\")\n\n            # If list provided and expected files known, validate length\n            if expected_n_files is not None and isinstance(self.manual_datetimes, list):\n                if len(self.manual_datetimes) != expected_n_files:\n                    raise ValueError(\n                        f\"manual_datetimes length ({len(self.manual_datetimes)}) must match \"\n                        f\"number of input files ({expected_n_files}) for {mode} mode\"\n                    )\n\n    def _compute_manual_file_datetimes(self, n_files: int, durations: list[float]) -&gt; list[datetime]:\n        \"\"\"Compute file end datetimes based on manual time specifications.\n\n        Args:\n            n_files (int): Number of files\n            durations (list[float]): Duration of each file in seconds\n\n        Returns:\n            list[datetime]: End datetime for each file\n\n        Raises:\n            ValueError: If manual_datetimes length doesn't match number of files\n        \"\"\"\n        if self.manual_datetimes is None:\n            return None\n\n        if isinstance(self.manual_datetimes, list):\n            # List of times provided - one per file\n            if len(self.manual_datetimes) != n_files:\n                raise ValueError(\n                    f\"manual_datetimes length ({len(self.manual_datetimes)}) must match number of files ({n_files})\"\n                )\n\n            # Convert start times to end times or vice versa\n            if self.datetimes_are_start:\n                # Convert start times to end times\n                file_end_datetimes = [\n                    start_time + timedelta(seconds=duration)\n                    for start_time, duration in zip(self.manual_datetimes, durations)\n                ]\n            else:\n                # Use as end times directly\n                file_end_datetimes = list(self.manual_datetimes)\n\n            # Check contiguity (warn instead of error)\n            self._validate_file_contiguity(file_end_datetimes, durations)\n\n            return file_end_datetimes\n\n        else:\n            # Single datetime provided - global start or end time\n            if self.datetimes_are_start:\n                # Global start time - compute cumulative end times\n                current_time = self.manual_datetimes\n                file_end_datetimes = []\n                for duration in durations:\n                    current_time += timedelta(seconds=duration)\n                    file_end_datetimes.append(current_time)\n                return file_end_datetimes\n            else:\n                # Global end time - work backwards\n                total_duration = sum(durations)\n                start_time = self.manual_datetimes - timedelta(seconds=total_duration)\n                current_time = start_time\n                file_end_datetimes = []\n                for duration in durations:\n                    current_time += timedelta(seconds=duration)\n                    file_end_datetimes.append(current_time)\n                return file_end_datetimes\n\n    def _validate_file_contiguity(self, file_end_datetimes: list[datetime], durations: list[float]):\n        \"\"\"Check that files are contiguous in time and warn if they're not.\n\n        Args:\n            file_end_datetimes (list[datetime]): End datetime for each file\n            durations (list[float]): Duration of each file in seconds\n        \"\"\"\n        if len(file_end_datetimes) &lt;= 1:\n            return  # Single file or no files - nothing to check\n\n        tolerance_seconds = 1.0  # Allow 1 second tolerance for rounding errors\n\n        for i in range(len(file_end_datetimes) - 1):\n            # Start time of next file should equal end time of current file\n            current_end = file_end_datetimes[i]\n            next_start = file_end_datetimes[i + 1] - timedelta(seconds=durations[i + 1])\n\n            gap_seconds = (next_start - current_end).total_seconds()\n            if gap_seconds &gt; tolerance_seconds:\n                warnings.warn(\n                    f\"Files may not be contiguous: gap of {gap_seconds:.2f}s between \"\n                    f\"file {i} (ends {current_end}) and file {i + 1} (starts {next_start}). \"\n                    f\"Tolerance is {tolerance_seconds}s.\"\n                )\n            elif gap_seconds &lt; -tolerance_seconds:\n                warnings.warn(\n                    f\"Files may overlap: negative gap of {gap_seconds:.2f}s between \"\n                    f\"file {i} (ends {current_end}) and file {i + 1} (starts {next_start}). \"\n                    f\"Tolerance is {tolerance_seconds}s.\"\n                )\n\n    def finalize_file_timestamps(self):\n        \"\"\"Finalize file timestamps using manual times if provided, otherwise validate CSV times.\"\"\"\n        logging.info(\"Finalizing file timestamps\")\n        if not hasattr(self, \"file_durations\") or not self.file_durations:\n            return  # No file durations available yet\n\n        manual_file_datetimes = self._compute_manual_file_datetimes(len(self.file_durations), self.file_durations)\n\n        if manual_file_datetimes is not None:\n            self.file_end_datetimes = manual_file_datetimes\n            logging.info(f\"Using manual timestamps: {len(manual_file_datetimes)} file end times specified\")\n        else:\n            # Check if CSV times are sufficient (only for bin mode)\n            if hasattr(self, \"file_end_datetimes\") and self.file_end_datetimes:\n                if all(x is None for x in self.file_end_datetimes):\n                    raise ValueError(\"No dates found in any metadata object and no manual times specified!\")\n                logging.info(\"Using CSV metadata timestamps\")\n            else:\n                # For si/mne modes, manual timestamps are required\n                raise ValueError(\"manual_datetimes must be provided when no CSV metadata is available!\")\n\n    def __str__(self):\n        \"\"\"Return a string representation of critical long recording features.\"\"\"\n        if not hasattr(self, \"LongRecording\") or self.LongRecording is None:\n            return \"LongRecordingOrganizer: No recording loaded yet\"\n\n        n_channels = self.LongRecording.get_num_channels()\n        sampling_freq = self.LongRecording.get_sampling_frequency()\n        total_duration = self.LongRecording.get_duration()\n\n        n_files = len(self.file_durations) if hasattr(self, \"file_durations\") and self.file_durations else 1\n\n        timestamp_info = \"No timestamps\"\n        if hasattr(self, \"file_end_datetimes\") and self.file_end_datetimes:\n            timestamp_coverage = len([x for x in self.file_end_datetimes if x is not None])\n            timestamp_info = f\"{timestamp_coverage}/{len(self.file_end_datetimes)} files have timestamps\"\n\n        channel_info = \"No channels\"\n        if hasattr(self, \"channel_names\") and self.channel_names:\n            # if len(self.channel_names) &lt;= 5:\n            channel_info = f\"[{', '.join(self.channel_names)}]\"\n            # else:\n            #     channel_info = f\"[{', '.join(self.channel_names[:3])}, ..., {self.channel_names[-1]}] ({len(self.channel_names)} total)\"\n\n        metadata_info = \"\"\n        if hasattr(self, \"meta\") and self.meta:\n            if hasattr(self.meta, \"precision\") and self.meta.precision:\n                metadata_info = f\", {self.meta.precision} precision\"\n            if hasattr(self.meta, \"V_units\") and self.meta.V_units:\n                metadata_info += f\", {self.meta.V_units} units\"\n\n        return (\n            f\"LongRecording: {n_files} files, {n_channels} channels, \"\n            f\"{sampling_freq} Hz, {total_duration:.1f}s duration, \"\n            f\"channels: {channel_info}{metadata_info}, timestamps: {timestamp_info}\"\n        )\n\n    def _apply_resampling(self, recording: \"si.BaseRecording\") -&gt; \"si.BaseRecording\":\n        \"\"\"Apply unified resampling using SpikeInterface preprocessing.\n\n        This method centralizes all resampling logic across the different data loading pipelines\n        (binary, MNE, SI) to use the fast SpikeInterface resampling implementation consistently.\n\n        Args:\n            recording (si.BaseRecording): The recording to resample\n\n        Returns:\n            si.BaseRecording: The resampled recording\n\n        Raises:\n            ImportError: If SpikeInterface preprocessing is not available\n        \"\"\"\n        if spre is None:\n            raise ImportError(\"SpikeInterface preprocessing is required for resampling\")\n\n        current_rate = recording.get_sampling_frequency()\n        target_rate = constants.GLOBAL_SAMPLING_RATE\n\n        if current_rate == target_rate:\n            logging.info(f\"Recording already at target sampling rate ({target_rate} Hz), no resampling needed\")\n            return recording\n\n        logging.info(f\"Resampling recording from {current_rate} Hz to {target_rate} Hz using SpikeInterface\")\n\n        # Use SpikeInterface resampling with margin to reduce edge effects\n        resampled_recording = spre.resample(\n            recording=recording,\n            resample_rate=target_rate,\n        )\n\n        # Update metadata to reflect new sampling rate\n        if hasattr(self, \"meta\") and self.meta is not None:\n            self.meta.update_sampling_rate(target_rate)\n\n        logging.info(f\"Successfully resampled recording to {target_rate} Hz\")\n        return resampled_recording\n\n    def merge(self, other_lro):\n        \"\"\"Merge another LRO into this one using si.concatenate_recordings.\n\n        This creates a new concatenated recording from this LRO and the other LRO.\n        The other LRO should represent a later time period to maintain temporal order.\n\n        Args:\n            other_lro (LongRecordingOrganizer): The LRO to merge into this one\n\n        Raises:\n            ValueError: If LROs are incompatible (different channels, sampling rates, etc.)\n            ImportError: If SpikeInterface is not available\n        \"\"\"\n        if si is None:\n            raise ImportError(\"SpikeInterface is required for LRO merging\")\n\n        # Validate merge compatibility\n        self._validate_merge_compatibility(other_lro)\n\n        # Concatenate recordings using SpikeInterface\n        logging.info(f\"Merging LRO {other_lro.base_folder_path} into {self.base_folder_path}\")\n        self.LongRecording = si.concatenate_recordings([self.LongRecording, other_lro.LongRecording])\n\n        # Update metadata after merge\n        self._update_metadata_after_merge(other_lro)\n\n        logging.info(\"Successfully merged LRO recordings\")\n\n    def _validate_merge_compatibility(self, other_lro):\n        \"\"\"Validate that two LROs can be safely merged.\n\n        Args:\n            other_lro (LongRecordingOrganizer): The LRO to validate against this one\n\n        Raises:\n            ValueError: If LROs are incompatible\n        \"\"\"\n        # Check channel names\n        if self.channel_names != other_lro.channel_names:\n            raise ValueError(\n                f\"Channel names mismatch: this LRO has {self.channel_names}, other LRO has {other_lro.channel_names}\"\n            )\n\n        # Check sampling rates\n        if hasattr(self.meta, \"f_s\") and hasattr(other_lro.meta, \"f_s\"):\n            if self.meta.f_s != other_lro.meta.f_s:\n                raise ValueError(\n                    f\"Sampling rate mismatch: this LRO has {self.meta.f_s} Hz, other LRO has {other_lro.meta.f_s} Hz\"\n                )\n\n        # Check channel counts\n        if hasattr(self.meta, \"n_channels\") and hasattr(other_lro.meta, \"n_channels\"):\n            if self.meta.n_channels != other_lro.meta.n_channels:\n                raise ValueError(\n                    f\"Channel count mismatch: \"\n                    f\"this LRO has {self.meta.n_channels} channels, \"\n                    f\"other LRO has {other_lro.meta.n_channels} channels\"\n                )\n\n        # Check that both have valid recordings\n        if not hasattr(self, \"LongRecording\") or self.LongRecording is None:\n            raise ValueError(\"This LRO does not have a valid LongRecording\")\n        if not hasattr(other_lro, \"LongRecording\") or other_lro.LongRecording is None:\n            raise ValueError(\"Other LRO does not have a valid LongRecording\")\n\n    def _update_metadata_after_merge(self, other_lro):\n        \"\"\"Update this LRO's metadata after merging with another LRO.\n\n        Args:\n            other_lro (LongRecordingOrganizer): The LRO that was merged into this one\n        \"\"\"\n        # Update end time to reflect the merged recording duration\n        if hasattr(other_lro.meta, \"dt_end\") and hasattr(self.meta, \"dt_end\"):\n            self.meta.dt_end = other_lro.meta.dt_end\n\n        # Note: Channel names, sampling rate, etc. should already be validated as identical\n\n    def __repr__(self):\n        \"\"\"Return a detailed string representation for debugging.\"\"\"\n        return self.__str__()\n</code></pre>"},{"location":"reference/core/core/#neurodent.core.core.LongRecordingOrganizer.__fragidx_to_startendind","title":"<code>__fragidx_to_startendind(fragment_len_s, fragment_idx)</code>","text":"<p>Convert fragment index to start and end sample indices.</p> <p>Parameters:</p> Name Type Description Default <code>fragment_len_s</code> <code>float</code> <p>Length of each fragment in seconds</p> required <code>fragment_idx</code> <code>int</code> <p>Index of the fragment to get indices for</p> required <p>Returns:</p> Type Description <p>tuple[int, int]: Start and end sample indices for the fragment. The end index is capped at the recording length.</p> Source code in <code>src/neurodent/core/core.py</code> <pre><code>def __fragidx_to_startendind(self, fragment_len_s, fragment_idx):\n    \"\"\"Convert fragment index to start and end sample indices.\n\n    Args:\n        fragment_len_s (float): Length of each fragment in seconds\n        fragment_idx (int): Index of the fragment to get indices for\n\n    Returns:\n        tuple[int, int]: Start and end sample indices for the fragment. The end index is capped at the recording length.\n    \"\"\"\n    frag_len_idx = self.__time_to_idx(fragment_len_s)\n    startidx = frag_len_idx * fragment_idx\n    endidx = min(frag_len_idx * (fragment_idx + 1), self.LongRecording.get_num_frames())\n    return startidx, endidx\n</code></pre>"},{"location":"reference/core/core/#neurodent.core.core.LongRecordingOrganizer.__init__","title":"<code>__init__(base_folder_path, mode='bin', truncate=False, cache_policy='auto', multiprocess_mode='serial', extract_func=None, input_type='folder', file_pattern=None, manual_datetimes=None, datetimes_are_start=True, n_jobs=1, **kwargs)</code>","text":"<p>Construct a long recording from binary files or EDF files.</p> <p>Parameters:</p> Name Type Description Default <code>base_folder_path</code> <code>str</code> <p>Path to the base folder containing the data files.</p> required <code>mode</code> <code>Literal['bin', 'si', 'mne', None]</code> <p>Mode to load data in. Defaults to 'bin'.</p> <code>'bin'</code> <code>truncate</code> <code>Union[bool, int]</code> <p>If True, truncate data to first 10 files. If an integer, truncate data to the first n files. Defaults to False.</p> <code>False</code> <code>overwrite_rowbins</code> <code>bool</code> <p>If True, overwrite existing row-major binary files. Defaults to False.</p> required <code>multiprocess_mode</code> <code>Literal['dask', 'serial']</code> <p>Processing mode for parallel operations. Defaults to 'serial'.</p> <code>'serial'</code> <code>extract_func</code> <code>Callable</code> <p>Function to extract data when using 'si' or 'mne' mode. Required for those modes.</p> <code>None</code> <code>input_type</code> <code>Literal['folder', 'file', 'files']</code> <p>Type of input to load. Defaults to 'folder'.</p> <code>'folder'</code> <code>file_pattern</code> <code>str</code> <p>Pattern to match files when using 'file' or 'files' input type. Defaults to '*'.</p> <code>None</code> <code>manual_datetimes</code> <code>datetime | list[datetime]</code> <p>Manual timestamps for the recording. For 'bin' mode: if datetime, used as global start/end time; if list, one timestamp per file. For 'si'/'mne' modes: if datetime, used as start/end of entire recording; if list, one per input file.</p> <code>None</code> <code>datetimes_are_start</code> <code>bool</code> <p>If True, manual_datetimes are treated as start times. If False, treated as end times. Defaults to True.</p> <code>True</code> <code>n_jobs</code> <code>int</code> <p>Number of jobs for MNE resampling operations. Defaults to 1 for safety. Set to -1 for automatic parallel detection, or &gt;1 for specific job count.</p> <code>1</code> <code>**kwargs</code> <p>Additional arguments passed to the data loading functions.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no data files are found, if the folder contains mixed file types, or if manual time parameters are invalid.</p> Source code in <code>src/neurodent/core/core.py</code> <pre><code>def __init__(\n    self,\n    base_folder_path,\n    mode: Literal[\"bin\", \"si\", \"mne\", None] = \"bin\",\n    truncate: Union[bool, int] = False,\n    cache_policy: Literal[\"auto\", \"always\", \"force_regenerate\"] = \"auto\",\n    multiprocess_mode: Literal[\"dask\", \"serial\"] = \"serial\",\n    extract_func: Union[Callable[..., \"si.BaseRecording\"], Callable[..., mne.io.Raw]] = None,\n    input_type: Literal[\"folder\", \"file\", \"files\"] = \"folder\",\n    file_pattern: str = None,\n    manual_datetimes: datetime | list[datetime] = None,\n    datetimes_are_start: bool = True,\n    n_jobs: int = 1,\n    **kwargs,\n):\n    \"\"\"Construct a long recording from binary files or EDF files.\n\n    Args:\n        base_folder_path (str): Path to the base folder containing the data files.\n        mode (Literal['bin', 'si', 'mne', None]): Mode to load data in. Defaults to 'bin'.\n        truncate (Union[bool, int], optional): If True, truncate data to first 10 files.\n            If an integer, truncate data to the first n files. Defaults to False.\n        overwrite_rowbins (bool, optional): If True, overwrite existing row-major binary files. Defaults to False.\n        multiprocess_mode (Literal['dask', 'serial'], optional): Processing mode for parallel operations. Defaults to 'serial'.\n        extract_func (Callable, optional): Function to extract data when using 'si' or 'mne' mode. Required for those modes.\n        input_type (Literal['folder', 'file', 'files'], optional): Type of input to load. Defaults to 'folder'.\n        file_pattern (str, optional): Pattern to match files when using 'file' or 'files' input type. Defaults to '*'.\n        manual_datetimes (datetime | list[datetime], optional): Manual timestamps for the recording.\n            For 'bin' mode: if datetime, used as global start/end time; if list, one timestamp per file.\n            For 'si'/'mne' modes: if datetime, used as start/end of entire recording; if list, one per input file.\n        datetimes_are_start (bool, optional): If True, manual_datetimes are treated as start times.\n            If False, treated as end times. Defaults to True.\n        n_jobs (int, optional): Number of jobs for MNE resampling operations. Defaults to 1 for safety.\n            Set to -1 for automatic parallel detection, or &gt;1 for specific job count.\n        **kwargs: Additional arguments passed to the data loading functions.\n\n    Raises:\n        ValueError: If no data files are found, if the folder contains mixed file types,\n            or if manual time parameters are invalid.\n    \"\"\"\n\n    self.base_folder_path = Path(base_folder_path)\n\n    self.n_truncate = parse_truncate(truncate)\n    self.truncate = True if self.n_truncate &gt; 0 else False\n    if self.truncate:\n        warnings.warn(f\"LongRecording will be truncated to the first {self.n_truncate} files\")\n\n    # Store manual time parameters for validation\n    self.manual_datetimes = manual_datetimes\n    self.datetimes_are_start = datetimes_are_start\n\n    # Store n_jobs parameter for MNE operations\n    self.n_jobs = n_jobs\n\n    # Validate manual time parameters\n    self._validate_manual_time_params()\n\n    # Initialize core attributes\n    self.meta = None\n    self.channel_names = None\n    self.LongRecording = None\n    self.temppaths = []\n    self.file_durations = []\n    self.cumulative_file_durations = []\n    self.bad_channel_names = []\n\n    # Load data if mode is specified\n    if mode is not None:\n        self.detect_and_load_data(\n            mode=mode,\n            cache_policy=cache_policy,\n            multiprocess_mode=multiprocess_mode,\n            extract_func=extract_func,\n            input_type=input_type,\n            file_pattern=file_pattern,\n            **kwargs,\n        )\n</code></pre>"},{"location":"reference/core/core/#neurodent.core.core.LongRecordingOrganizer.__repr__","title":"<code>__repr__()</code>","text":"<p>Return a detailed string representation for debugging.</p> Source code in <code>src/neurodent/core/core.py</code> <pre><code>def __repr__(self):\n    \"\"\"Return a detailed string representation for debugging.\"\"\"\n    return self.__str__()\n</code></pre>"},{"location":"reference/core/core/#neurodent.core.core.LongRecordingOrganizer.__str__","title":"<code>__str__()</code>","text":"<p>Return a string representation of critical long recording features.</p> Source code in <code>src/neurodent/core/core.py</code> <pre><code>def __str__(self):\n    \"\"\"Return a string representation of critical long recording features.\"\"\"\n    if not hasattr(self, \"LongRecording\") or self.LongRecording is None:\n        return \"LongRecordingOrganizer: No recording loaded yet\"\n\n    n_channels = self.LongRecording.get_num_channels()\n    sampling_freq = self.LongRecording.get_sampling_frequency()\n    total_duration = self.LongRecording.get_duration()\n\n    n_files = len(self.file_durations) if hasattr(self, \"file_durations\") and self.file_durations else 1\n\n    timestamp_info = \"No timestamps\"\n    if hasattr(self, \"file_end_datetimes\") and self.file_end_datetimes:\n        timestamp_coverage = len([x for x in self.file_end_datetimes if x is not None])\n        timestamp_info = f\"{timestamp_coverage}/{len(self.file_end_datetimes)} files have timestamps\"\n\n    channel_info = \"No channels\"\n    if hasattr(self, \"channel_names\") and self.channel_names:\n        # if len(self.channel_names) &lt;= 5:\n        channel_info = f\"[{', '.join(self.channel_names)}]\"\n        # else:\n        #     channel_info = f\"[{', '.join(self.channel_names[:3])}, ..., {self.channel_names[-1]}] ({len(self.channel_names)} total)\"\n\n    metadata_info = \"\"\n    if hasattr(self, \"meta\") and self.meta:\n        if hasattr(self.meta, \"precision\") and self.meta.precision:\n            metadata_info = f\", {self.meta.precision} precision\"\n        if hasattr(self.meta, \"V_units\") and self.meta.V_units:\n            metadata_info += f\", {self.meta.V_units} units\"\n\n    return (\n        f\"LongRecording: {n_files} files, {n_channels} channels, \"\n        f\"{sampling_freq} Hz, {total_duration:.1f}s duration, \"\n        f\"channels: {channel_info}{metadata_info}, timestamps: {timestamp_info}\"\n    )\n</code></pre>"},{"location":"reference/core/core/#neurodent.core.core.LongRecordingOrganizer.apply_lof_threshold","title":"<code>apply_lof_threshold(lof_threshold)</code>","text":"<p>Apply threshold to existing LOF scores to determine bad channels.</p> <p>Parameters:</p> Name Type Description Default <code>lof_threshold</code> <code>float</code> <p>Threshold for determining bad channels.</p> required Source code in <code>src/neurodent/core/core.py</code> <pre><code>def apply_lof_threshold(self, lof_threshold: float):\n    \"\"\"Apply threshold to existing LOF scores to determine bad channels.\n\n    Args:\n        lof_threshold (float): Threshold for determining bad channels.\n    \"\"\"\n    if not hasattr(self, \"lof_scores\") or self.lof_scores is None:\n        raise ValueError(\"LOF scores not available. Run compute_bad_channels() first.\")\n\n    is_inlier = self.lof_scores &lt; lof_threshold\n    self.bad_channel_names = [self.channel_names[i] for i in np.where(~is_inlier)[0]]\n    logging.info(f\"Applied threshold {lof_threshold}: bad_channel_names = {self.bad_channel_names}\")\n</code></pre>"},{"location":"reference/core/core/#neurodent.core.core.LongRecordingOrganizer.compute_bad_channels","title":"<code>compute_bad_channels(lof_threshold=None, limit_memory=True, force_recompute=False)</code>","text":"<p>Compute bad channels using LOF analysis with unified score storage.</p> <p>Parameters:</p> Name Type Description Default <code>lof_threshold</code> <code>float</code> <p>Threshold for determining bad channels from LOF scores.                            If None, only computes/loads scores without setting bad_channel_names.</p> <code>None</code> <code>limit_memory</code> <code>bool</code> <p>Whether to reduce memory usage by decimation and float16.</p> <code>True</code> <code>force_recompute</code> <code>bool</code> <p>Whether to recompute LOF scores even if they exist.</p> <code>False</code> Source code in <code>src/neurodent/core/core.py</code> <pre><code>def compute_bad_channels(\n    self, lof_threshold: float = None, limit_memory: bool = True, force_recompute: bool = False\n):\n    \"\"\"Compute bad channels using LOF analysis with unified score storage.\n\n    Args:\n        lof_threshold (float, optional): Threshold for determining bad channels from LOF scores.\n                                       If None, only computes/loads scores without setting bad_channel_names.\n        limit_memory (bool): Whether to reduce memory usage by decimation and float16.\n        force_recompute (bool): Whether to recompute LOF scores even if they exist.\n    \"\"\"\n    # Check if LOF scores already exist and are current\n    if not force_recompute and hasattr(self, \"lof_scores\") and self.lof_scores is not None:\n        logging.info(\"Using existing LOF scores\")\n    else:\n        # Compute new LOF scores\n        try:\n            scores = self._compute_lof_scores(limit_memory=limit_memory)\n            self.lof_scores = scores\n            logging.info(f\"Computed LOF scores for {len(scores)} channels\")\n        except Exception as e:\n            logging.error(f\"Failed to compute LOF scores for recording: {e}\")\n            raise\n\n    # Apply threshold if provided\n    if lof_threshold is not None:\n        self.apply_lof_threshold(lof_threshold)\n</code></pre>"},{"location":"reference/core/core/#neurodent.core.core.LongRecordingOrganizer.convert_colbins_to_rowbins","title":"<code>convert_colbins_to_rowbins(overwrite=False, multiprocess_mode='serial')</code>","text":"<p>Convert column-major binary files to row-major binary files, and save them in the rowbin_folder_path.</p> <p>Parameters:</p> Name Type Description Default <code>overwrite</code> <code>bool</code> <p>If True, overwrite existing row-major binary files. Defaults to True.</p> <code>False</code> <code>multiprocess_mode</code> <code>Literal['dask', 'serial']</code> <p>If 'dask', use dask to convert the files in parallel. If 'serial', convert the files in serial. Defaults to 'serial'.</p> <code>'serial'</code> Source code in <code>src/neurodent/core/core.py</code> <pre><code>def convert_colbins_to_rowbins(self, overwrite=False, multiprocess_mode: Literal[\"dask\", \"serial\"] = \"serial\"):\n    \"\"\"\n    Convert column-major binary files to row-major binary files, and save them in the rowbin_folder_path.\n\n    Args:\n        overwrite (bool, optional): If True, overwrite existing row-major binary files. Defaults to True.\n        multiprocess_mode (Literal['dask', 'serial'], optional): If 'dask', use dask to convert the files in parallel.\n            If 'serial', convert the files in serial. Defaults to 'serial'.\n    \"\"\"\n\n    # if overwrite, regenerate regardless of existence\n    # else, read them (they exist) or make them (they don't exist)\n    # there is no error condition, and rowbins will be recreated regardless of choice\n\n    logging.info(f\"Converting {len(self.colbins)} column-major binary files to row-major format\")\n    if overwrite:\n        logging.info(\"Overwrite flag set - regenerating all row-major files\")\n    else:\n        logging.info(\"Overwrite flag not set - only generating missing row-major files\")\n\n    delayed = []\n    for i, e in enumerate(self.colbins):\n        if convert_colpath_to_rowpath(self.rowbin_folder_path, e, aspath=False) not in self.rowbins or overwrite:\n            logging.info(f\"Converting {e}\")\n            match multiprocess_mode:\n                case \"dask\":\n                    delayed.append(\n                        dask.delayed(convert_ddfcolbin_to_ddfrowbin)(\n                            self.rowbin_folder_path, e, self.meta, save_gzip=True\n                        )\n                    )\n                case \"serial\":\n                    convert_ddfcolbin_to_ddfrowbin(self.rowbin_folder_path, e, self.meta, save_gzip=True)\n                case _:\n                    raise ValueError(f\"Invalid multiprocess_mode: {multiprocess_mode}\")\n\n    if multiprocess_mode == \"dask\":\n        # Run all conversions in parallel\n        dask.compute(*delayed)\n\n    self.__update_colbins_rowbins_metas()\n</code></pre>"},{"location":"reference/core/core/#neurodent.core.core.LongRecordingOrganizer.convert_file_with_mne_to_recording","title":"<code>convert_file_with_mne_to_recording(extract_func, input_type='folder', file_pattern='*', intermediate='edf', intermediate_name=None, cache_policy='auto', multiprocess_mode='serial', n_jobs=None, **kwargs)</code>","text":"<p>Convert MNE-compatible files to SpikeInterface recording format with metadata caching.</p> <p>Parameters:</p> Name Type Description Default <code>extract_func</code> <code>Callable</code> <p>Function that takes a file path and returns mne.io.Raw object</p> required <code>input_type</code> <code>Literal</code> <p>Type of input - \"folder\", \"file\", or \"files\"</p> <code>'folder'</code> <code>file_pattern</code> <code>str</code> <p>Glob pattern for file matching (default: \"*\")</p> <code>'*'</code> <code>intermediate</code> <code>Literal</code> <p>Intermediate format - \"edf\" or \"bin\" (default: \"edf\")</p> <code>'edf'</code> <code>intermediate_name</code> <code>str</code> <p>Custom name for intermediate file</p> <code>None</code> <code>cache_policy</code> <code>Literal</code> <p>Caching policy for intermediate and metadata files (default: \"auto\") - \"auto\": Use cached files if both data and metadata exist and cache is newer than sources, regenerate with logging if missing/invalid - \"always\": Use cached files if both data and metadata exist, raise error if missing/invalid - \"force_regenerate\": Always regenerate files, overwrite existing cache</p> <code>'auto'</code> <code>multiprocess_mode</code> <code>Literal</code> <p>Processing mode - \"dask\" or \"serial\" (default: \"serial\")</p> <code>'serial'</code> <code>n_jobs</code> <code>int</code> <p>Number of jobs for MNE resampling. If None (default),                 uses the instance n_jobs value. Set to -1 for automatic parallel                 detection, or &gt;1 for specific job count.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments passed to extract_func</p> <code>{}</code> Note <p>Creates two cache files: data file (e.g., file.edf) and metadata sidecar (e.g., file.edf.meta.json). Both files must exist for cache to be used. Metadata preserves channel names, original sampling rates, and other DDFBinaryMetadata fields across cache hits.</p> Source code in <code>src/neurodent/core/core.py</code> <pre><code>def convert_file_with_mne_to_recording(\n    self,\n    extract_func: Callable[..., mne.io.Raw],\n    input_type: Literal[\"folder\", \"file\", \"files\"] = \"folder\",\n    file_pattern: str = \"*\",\n    intermediate: Literal[\"edf\", \"bin\"] = \"edf\",\n    intermediate_name=None,\n    cache_policy: Literal[\"auto\", \"always\", \"force_regenerate\"] = \"auto\",\n    multiprocess_mode: Literal[\"dask\", \"serial\"] = \"serial\",\n    n_jobs: int = None,\n    **kwargs,\n):\n    \"\"\"\n    Convert MNE-compatible files to SpikeInterface recording format with metadata caching.\n\n    Args:\n        extract_func (Callable): Function that takes a file path and returns mne.io.Raw object\n        input_type (Literal): Type of input - \"folder\", \"file\", or \"files\"\n        file_pattern (str): Glob pattern for file matching (default: \"*\")\n        intermediate (Literal): Intermediate format - \"edf\" or \"bin\" (default: \"edf\")\n        intermediate_name (str, optional): Custom name for intermediate file\n        cache_policy (Literal): Caching policy for intermediate and metadata files (default: \"auto\")\n            - \"auto\": Use cached files if both data and metadata exist and cache is newer than sources, regenerate with logging if missing/invalid\n            - \"always\": Use cached files if both data and metadata exist, raise error if missing/invalid\n            - \"force_regenerate\": Always regenerate files, overwrite existing cache\n        multiprocess_mode (Literal): Processing mode - \"dask\" or \"serial\" (default: \"serial\")\n        n_jobs (int, optional): Number of jobs for MNE resampling. If None (default),\n                            uses the instance n_jobs value. Set to -1 for automatic parallel\n                            detection, or &gt;1 for specific job count.\n        **kwargs: Additional arguments passed to extract_func\n\n    Note:\n        Creates two cache files: data file (e.g., file.edf) and metadata sidecar (e.g., file.edf.meta.json).\n        Both files must exist for cache to be used. Metadata preserves channel names, original\n        sampling rates, and other DDFBinaryMetadata fields across cache hits.\n    \"\"\"\n    if se is None:\n        raise ImportError(\"SpikeInterface is required for convert_file_with_mne_to_recording\")\n    # Early validation and file discovery\n    if input_type == \"folder\":\n        self._validate_timestamps_for_mode(\"mne\", 1)\n        datafolder = self.base_folder_path\n        datafile = None\n        datafiles = None\n        source_paths = [self.base_folder_path]\n        n_processed_files = 1\n\n    elif input_type == \"file\":\n        self._validate_timestamps_for_mode(\"mne\", 1)\n        datafiles = list(self.base_folder_path.glob(file_pattern))\n        if len(datafiles) == 0:\n            raise ValueError(f\"No files found matching pattern: {file_pattern}\")\n        elif len(datafiles) &gt; 1:\n            warnings.warn(f\"Multiple files found matching pattern: {file_pattern}. Using first file.\")\n        datafile = datafiles[0]\n        datafolder = None\n        source_paths = [datafile]\n        n_processed_files = 1\n\n    elif input_type == \"files\":\n        datafiles = list(self.base_folder_path.glob(file_pattern))\n        if len(datafiles) == 0:\n            raise ValueError(f\"No files found matching pattern: {file_pattern}\")\n        datafiles = self._truncate_file_list(datafiles)\n        self._validate_timestamps_for_mode(\"mne\", len(datafiles))\n        datafiles.sort()\n        datafolder = None\n        datafile = None\n        source_paths = datafiles\n        n_processed_files = len(datafiles)\n\n    else:\n        raise ValueError(f\"Invalid input_type: {input_type}\")\n\n    # Store number of processed files for timestamp handling\n    self._n_processed_files = n_processed_files\n\n    # Determine intermediate file path\n    intermediate_name = (\n        f\"{self.base_folder_path.name}_mne-to-rec\" if intermediate_name is None else intermediate_name\n    )\n    fname = self.base_folder_path / f\"{intermediate_name}.{intermediate}\"\n\n    # Get or create the intermediate file (this handles caching logic)\n    rec, _, metadata = self._get_or_create_intermediate_file(\n        fname=fname,\n        source_paths=source_paths,\n        cache_policy=cache_policy,\n        intermediate=intermediate,\n        extract_func=extract_func,\n        input_type=input_type,\n        datafolder=datafolder,\n        datafile=datafile,\n        datafiles=datafiles,\n        n_jobs=n_jobs,\n        **kwargs,\n    )\n\n    # Set metadata first so resampling can update it\n    self.meta = metadata\n\n    # Apply unified resampling to the loaded recording (this will update metadata sampling rate)\n    self.LongRecording = self._apply_resampling(rec)\n\n    # Update dt_end for manual timestamps (will be properly set by finalize_file_timestamps)\n    if self.manual_datetimes is not None:\n        self.meta.dt_end = None  # Will be set by finalize_file_timestamps\n    self.channel_names = self.meta.channel_names\n\n    # For mne mode, handle multiple files or single file\n    if not hasattr(self, \"file_durations\") or not self.file_durations:\n        if hasattr(self, \"_n_processed_files\") and self._n_processed_files &gt; 1:\n            # Multiple files concatenated - estimate equal durations\n            total_duration = self.LongRecording.get_duration()\n            avg_duration = total_duration / self._n_processed_files\n            self.file_durations = [avg_duration] * self._n_processed_files\n        else:\n            # Single file or folder\n            self.file_durations = [self.LongRecording.get_duration()]\n        self.file_end_datetimes = []\n\n    # Apply manual timestamps if provided\n    self.finalize_file_timestamps()\n\n    # Debug logging for critical recording features\n    logging.debug(f\"LongRecording created via MNE: {self}\")\n</code></pre>"},{"location":"reference/core/core/#neurodent.core.core.LongRecordingOrganizer.convert_file_with_si_to_recording","title":"<code>convert_file_with_si_to_recording(extract_func, input_type='folder', file_pattern='*', cache_policy='auto', **kwargs)</code>","text":"<p>Create a SpikeInterface Recording from a folder, a single file, or multiple files.</p> <p>This is a thin wrapper around <code>extract_func</code> that discovers inputs under <code>self.base_folder_path</code> and builds a <code>si.BaseRecording</code> accordingly.</p> <p>Modes: - <code>folder</code>: Passes <code>self.base_folder_path</code> directly to <code>extract_func</code>. - <code>file</code>: Uses <code>glob</code> with <code>file_pattern</code> relative to <code>self.base_folder_path</code>.   If multiple matches are found, the first match is used and a warning is issued. - <code>files</code>: Uses <code>Path.glob</code> with <code>file_pattern</code> under <code>self.base_folder_path</code>,   optionally truncates via <code>self._truncate_file_list(...)</code>, sorts the files, applies   <code>extract_func</code> to each file, and concatenates the resulting recordings via   <code>si.concatenate_recordings</code>.</p> <p>Parameters:</p> Name Type Description Default <code>extract_func</code> <code>Callable[..., BaseRecording]</code> <p>Function that consumes a path (folder or file path) and returns a <code>si.BaseRecording</code>.</p> required <code>input_type</code> <code>Literal['folder', 'file', 'files']</code> <p>How to discover inputs. Defaults to <code>'folder'</code>.</p> <code>'folder'</code> <code>file_pattern</code> <code>str</code> <p>Glob pattern used when <code>input_type</code> is <code>'file'</code> or <code>'files'</code>. Defaults to <code>'*'</code>.</p> <code>'*'</code> <code>**kwargs</code> <p>Additional keyword arguments forwarded to <code>extract_func</code>.</p> <code>{}</code> Side Effects <p>Sets <code>self.LongRecording</code> to the resulting recording and initializes <code>self.meta</code> based on that recording's properties.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no files are found for the given <code>file_pattern</code> or <code>input_type</code> is invalid.</p> Source code in <code>src/neurodent/core/core.py</code> <pre><code>def convert_file_with_si_to_recording(\n    self,\n    extract_func: Callable[..., \"si.BaseRecording\"],\n    input_type: Literal[\"folder\", \"file\", \"files\"] = \"folder\",\n    file_pattern: str = \"*\",\n    cache_policy: Literal[\"auto\", \"always\", \"force_regenerate\"] = \"auto\",\n    **kwargs,\n):\n    \"\"\"Create a SpikeInterface Recording from a folder, a single file, or multiple files.\n\n    This is a thin wrapper around ``extract_func`` that discovers inputs under\n    ``self.base_folder_path`` and builds a ``si.BaseRecording`` accordingly.\n\n    Modes:\n    - ``folder``: Passes ``self.base_folder_path`` directly to ``extract_func``.\n    - ``file``: Uses ``glob`` with ``file_pattern`` relative to ``self.base_folder_path``.\n      If multiple matches are found, the first match is used and a warning is issued.\n    - ``files``: Uses ``Path.glob`` with ``file_pattern`` under ``self.base_folder_path``,\n      optionally truncates via ``self._truncate_file_list(...)``, sorts the files, applies\n      ``extract_func`` to each file, and concatenates the resulting recordings via\n      ``si.concatenate_recordings``.\n\n    Args:\n        extract_func (Callable[..., \"si.BaseRecording\"]): Function that consumes a path\n            (folder or file path) and returns a ``si.BaseRecording``.\n        input_type (Literal['folder', 'file', 'files'], optional): How to discover inputs.\n            Defaults to ``'folder'``.\n        file_pattern (str, optional): Glob pattern used when ``input_type`` is ``'file'`` or\n            ``'files'``. Defaults to ``'*'``.\n        **kwargs: Additional keyword arguments forwarded to ``extract_func``.\n\n    Side Effects:\n        Sets ``self.LongRecording`` to the resulting recording and initializes ``self.meta``\n        based on that recording's properties.\n\n    Raises:\n        ValueError: If no files are found for the given ``file_pattern`` or ``input_type`` is invalid.\n    \"\"\"\n    if si is None:\n        raise ImportError(\"SpikeInterface is required for convert_file_with_si_to_recording\")\n    # Early validation and file discovery\n    if input_type == \"folder\":\n        # For single folder, validate that timestamps are provided\n        self._validate_timestamps_for_mode(\"si\", 1)\n        datafolder = self.base_folder_path\n        rec: \"si.BaseRecording\" = extract_func(datafolder, **kwargs)\n        n_processed_files = 1\n    elif input_type == \"file\":\n        # For single file, validate that timestamps are provided\n        self._validate_timestamps_for_mode(\"si\", 1)\n        datafiles = glob.glob(str(self.base_folder_path / file_pattern))\n        if len(datafiles) == 0:\n            raise ValueError(f\"No files found matching pattern: {file_pattern}\")\n        elif len(datafiles) &gt; 1:\n            warnings.warn(f\"Multiple files found matching pattern: {file_pattern}. Using first file.\")\n        datafile = datafiles[0]\n        rec: \"si.BaseRecording\" = extract_func(datafile, **kwargs)\n        n_processed_files = 1\n    elif input_type == \"files\":\n        datafiles = [str(x) for x in self.base_folder_path.glob(file_pattern)]\n        if len(datafiles) == 0:\n            raise ValueError(f\"No files found matching pattern: {file_pattern}\")\n        datafiles = self._truncate_file_list(datafiles)\n        # Validate timestamps early before slow processing\n        self._validate_timestamps_for_mode(\"si\", len(datafiles))\n        datafiles.sort()  # FIXME sort by index, or some other logic. Files may be out of order otherwise, messing up isday calculation\n        recs: list[\"si.BaseRecording\"] = [extract_func(x, **kwargs) for x in datafiles]\n        rec = si.concatenate_recordings(recs)\n        n_processed_files = len(datafiles)\n    else:\n        raise ValueError(f\"Invalid input_type: {input_type}\")\n\n    # Store number of processed files for timestamp handling\n    self._n_processed_files = n_processed_files\n\n    # Apply unified resampling to the recording\n    self.LongRecording = self._apply_resampling(rec)\n\n    # For SI mode, don't use confusing DEFAULT_DAY if we have manual timestamps\n    dt_end = None if self.manual_datetimes is not None else None  # Will be set by finalize_file_timestamps\n\n    self.meta = DDFBinaryMetadata(\n        None,\n        n_channels=self.LongRecording.get_num_channels(),\n        f_s=self.LongRecording.get_sampling_frequency(),\n        dt_end=dt_end,  # Will be properly set by finalize_file_timestamps\n        channel_names=self.LongRecording.get_channel_ids().tolist(),  # NOTE may potentially be a list of integers, which is undesirable. The ability to set names is available in the extractor function itself\n        # In the case this is integers, raise a warning and/or error, convert to string, and make a note that you may need to adjust parameters in si extractor\n    )\n    self.channel_names = self.meta.channel_names\n\n    # For si mode, handle multiple files or single file\n    if not hasattr(self, \"file_durations\") or not self.file_durations:\n        if hasattr(self, \"_n_processed_files\") and self._n_processed_files &gt; 1:\n            # Multiple files concatenated - estimate equal durations\n            total_duration = self.LongRecording.get_duration()\n            avg_duration = total_duration / self._n_processed_files\n            self.file_durations = [avg_duration] * self._n_processed_files\n        else:\n            # Single file or folder\n            self.file_durations = [self.LongRecording.get_duration()]\n        self.file_end_datetimes = []\n\n    # Apply manual timestamps if provided\n    self.finalize_file_timestamps()\n\n    # Debug logging for critical recording features\n    logging.debug(f\"LongRecording created via SI: {self}\")\n</code></pre>"},{"location":"reference/core/core/#neurodent.core.core.LongRecordingOrganizer.convert_rowbins_to_rec","title":"<code>convert_rowbins_to_rec(multiprocess_mode='serial', cache_policy='auto')</code>","text":"<p>Convert row-major binary files to SpikeInterface Recording structure.</p> <p>Parameters:</p> Name Type Description Default <code>multiprocess_mode</code> <code>Literal['dask', 'serial']</code> <p>If 'dask', use dask to convert the files in parallel. If 'serial', convert the files in serial. Defaults to 'serial'.</p> <code>'serial'</code> <code>cache_policy</code> <code>Literal</code> <p>Caching policy for intermediate files (default: \"auto\") - \"auto\": Use cached files if exist and newer than sources, regenerate with logging if missing/invalid - \"always\": Use cached files if exist, raise error if missing/invalid - \"force_regenerate\": Always regenerate files, overwrite existing cache</p> <code>'auto'</code> Source code in <code>src/neurodent/core/core.py</code> <pre><code>def convert_rowbins_to_rec(\n    self,\n    multiprocess_mode: Literal[\"dask\", \"serial\"] = \"serial\",\n    cache_policy: Literal[\"auto\", \"always\", \"force_regenerate\"] = \"auto\",\n):\n    \"\"\"\n    Convert row-major binary files to SpikeInterface Recording structure.\n\n    Args:\n        multiprocess_mode (Literal['dask', 'serial'], optional): If 'dask', use dask to convert the files in parallel.\n            If 'serial', convert the files in serial. Defaults to 'serial'.\n        cache_policy (Literal): Caching policy for intermediate files (default: \"auto\")\n            - \"auto\": Use cached files if exist and newer than sources, regenerate with logging if missing/invalid\n            - \"always\": Use cached files if exist, raise error if missing/invalid\n            - \"force_regenerate\": Always regenerate files, overwrite existing cache\n    \"\"\"\n    if si is None:\n        raise ImportError(\"SpikeInterface is required for convert_rowbins_to_rec\")\n    if len(self.rowbins) &lt; len(self.colbins):\n        warnings.warn(\n            f\"{len(self.colbins)} column-major files found, but only {len(self.rowbins)} row-major files found. Some column-major files may be missing.\"\n        )\n    elif len(self.rowbins) &gt; len(self.colbins):\n        warnings.warn(\n            f\"{len(self.rowbins)} row-major files found, but only {len(self.colbins)} column-major files found. Some row-major files will be ignored.\"\n        )\n\n    recs = []\n    t_cumulative = 0\n    self.temppaths = []\n\n    match multiprocess_mode:\n        case \"dask\":\n            # Compute all conversions in parallel\n            delayed_results = []\n            for i, e in enumerate(self.rowbins):\n                delayed_results.append((i, dask.delayed(_convert_ddfrowbin_to_si_no_resample)(e, self.meta)))\n            computed_results = dask.compute(*delayed_results)\n\n            # Reconstruct results in the correct order\n            results = [None] * len(self.rowbins)\n            for i, result in computed_results:\n                results[i] = result\n            logging.info(f\"self.rowbins: {[Path(x).name for x in self.rowbins]}\")\n\n        case \"serial\":\n            results = [_convert_ddfrowbin_to_si_no_resample(e, self.meta) for e in self.rowbins]\n        case _:\n            raise ValueError(f\"Invalid multiprocess_mode: {multiprocess_mode}\")\n\n    # Process results\n    for i, (rec, temppath) in enumerate(results):\n        recs.append(rec)\n        self.temppaths.append(temppath)\n\n        duration = rec.get_duration()\n        self.file_durations.append(duration)\n\n        t_cumulative += duration  # NOTE  use numpy cumsum later\n        self.cumulative_file_durations.append(t_cumulative)\n\n    if not recs:\n        raise ValueError(\"No recordings generated. Check that all row-major files are present and readable.\")\n    elif len(recs) &lt; len(self.rowbins):\n        logging.warning(f\"Only {len(recs)} recordings generated. Some row-major files may be missing.\")\n\n    # Concatenate recordings first\n    concatenated_recording = si.concatenate_recordings(recs).rename_channels(self.channel_names)\n\n    # Apply unified resampling to the concatenated recording\n    self.LongRecording: \"si.BaseRecording\" = self._apply_resampling(concatenated_recording)\n\n    # Debug logging for critical recording features\n    logging.info(f\"LongRecording created: {self}\")\n</code></pre>"},{"location":"reference/core/core/#neurodent.core.core.LongRecordingOrganizer.convert_to_mne","title":"<code>convert_to_mne()</code>","text":"<p>Convert this LongRecording object to an MNE RawArray.</p> <p>Returns:</p> Type Description <code>RawArray</code> <p>mne.io.RawArray: The converted MNE RawArray</p> Source code in <code>src/neurodent/core/core.py</code> <pre><code>def convert_to_mne(self) -&gt; mne.io.RawArray:\n    \"\"\"Convert this LongRecording object to an MNE RawArray.\n\n    Returns:\n        mne.io.RawArray: The converted MNE RawArray\n    \"\"\"\n    data = self.LongRecording.get_traces(return_scaled=True)  # This gets data in (n_samples, n_channels) format\n    data = data.T  # Convert to (n_channels, n_samples) format for MNE\n\n    info = mne.create_info(\n        ch_names=self.channel_names, sfreq=self.LongRecording.get_sampling_frequency(), ch_types=\"eeg\"\n    )\n\n    return mne.io.RawArray(data=data, info=info)\n</code></pre>"},{"location":"reference/core/core/#neurodent.core.core.LongRecordingOrganizer.detect_and_load_data","title":"<code>detect_and_load_data(mode='bin', cache_policy='auto', multiprocess_mode='serial', extract_func=None, input_type='folder', file_pattern=None, **kwargs)</code>","text":"<p>Load in recording based on mode.</p> Source code in <code>src/neurodent/core/core.py</code> <pre><code>def detect_and_load_data(\n    self,\n    mode: Literal[\"bin\", \"si\", \"mne\", None] = \"bin\",\n    cache_policy: Literal[\"auto\", \"always\", \"force_regenerate\"] = \"auto\",\n    multiprocess_mode: Literal[\"dask\", \"serial\"] = \"serial\",\n    extract_func: Union[Callable[..., \"si.BaseRecording\"], Callable[..., mne.io.Raw]] = None,\n    input_type: Literal[\"folder\", \"file\", \"files\"] = \"folder\",\n    file_pattern: str = None,\n    **kwargs,\n):\n    \"\"\"Load in recording based on mode.\"\"\"\n\n    if mode == \"bin\":\n        # Binary file pipeline\n        self.convert_colbins_rowbins_to_rec(\n            cache_policy=cache_policy,\n            multiprocess_mode=multiprocess_mode,\n        )\n    elif mode == \"si\":\n        # EDF file pipeline\n        self.convert_file_with_si_to_recording(\n            extract_func=extract_func,\n            input_type=input_type,\n            file_pattern=file_pattern,\n            cache_policy=cache_policy,\n            **kwargs,\n        )\n    elif mode == \"mne\":\n        # MNE file pipeline\n        self.convert_file_with_mne_to_recording(\n            extract_func=extract_func,\n            input_type=input_type,\n            file_pattern=file_pattern,\n            cache_policy=cache_policy,\n            n_jobs=self.n_jobs,\n            **kwargs,\n        )\n    elif mode is None:\n        pass\n    else:\n        raise ValueError(f\"Invalid mode: {mode}\")\n</code></pre>"},{"location":"reference/core/core/#neurodent.core.core.LongRecordingOrganizer.finalize_file_timestamps","title":"<code>finalize_file_timestamps()</code>","text":"<p>Finalize file timestamps using manual times if provided, otherwise validate CSV times.</p> Source code in <code>src/neurodent/core/core.py</code> <pre><code>def finalize_file_timestamps(self):\n    \"\"\"Finalize file timestamps using manual times if provided, otherwise validate CSV times.\"\"\"\n    logging.info(\"Finalizing file timestamps\")\n    if not hasattr(self, \"file_durations\") or not self.file_durations:\n        return  # No file durations available yet\n\n    manual_file_datetimes = self._compute_manual_file_datetimes(len(self.file_durations), self.file_durations)\n\n    if manual_file_datetimes is not None:\n        self.file_end_datetimes = manual_file_datetimes\n        logging.info(f\"Using manual timestamps: {len(manual_file_datetimes)} file end times specified\")\n    else:\n        # Check if CSV times are sufficient (only for bin mode)\n        if hasattr(self, \"file_end_datetimes\") and self.file_end_datetimes:\n            if all(x is None for x in self.file_end_datetimes):\n                raise ValueError(\"No dates found in any metadata object and no manual times specified!\")\n            logging.info(\"Using CSV metadata timestamps\")\n        else:\n            # For si/mne modes, manual timestamps are required\n            raise ValueError(\"manual_datetimes must be provided when no CSV metadata is available!\")\n</code></pre>"},{"location":"reference/core/core/#neurodent.core.core.LongRecordingOrganizer.get_datetime_fragment","title":"<code>get_datetime_fragment(fragment_len_s, fragment_idx)</code>","text":"<p>Get the datetime for a specific fragment using the timestamp mapper.</p> <p>Parameters:</p> Name Type Description Default <code>fragment_len_s</code> <code>float</code> <p>Length of each fragment in seconds</p> required <code>fragment_idx</code> <code>int</code> <p>Index of the fragment to get datetime for</p> required <p>Returns:</p> Name Type Description <code>datetime</code> <p>The datetime corresponding to the start of the fragment</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If timestamp mapper is not initialized (only available in 'bin' mode)</p> Source code in <code>src/neurodent/core/core.py</code> <pre><code>def get_datetime_fragment(self, fragment_len_s, fragment_idx):\n    \"\"\"\n    Get the datetime for a specific fragment using the timestamp mapper.\n\n    Args:\n        fragment_len_s (float): Length of each fragment in seconds\n        fragment_idx (int): Index of the fragment to get datetime for\n\n    Returns:\n        datetime: The datetime corresponding to the start of the fragment\n\n    Raises:\n        ValueError: If timestamp mapper is not initialized (only available in 'bin' mode)\n    \"\"\"\n    return TimestampMapper(self.file_end_datetimes, self.file_durations).get_fragment_timestamp(\n        fragment_idx, fragment_len_s\n    )\n</code></pre>"},{"location":"reference/core/core/#neurodent.core.core.LongRecordingOrganizer.get_lof_scores","title":"<code>get_lof_scores()</code>","text":"<p>Get LOF scores with channel names.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary mapping channel names to LOF scores.</p> Source code in <code>src/neurodent/core/core.py</code> <pre><code>def get_lof_scores(self) -&gt; dict:\n    \"\"\"Get LOF scores with channel names.\n\n    Returns:\n        dict: Dictionary mapping channel names to LOF scores.\n    \"\"\"\n    if not hasattr(self, \"lof_scores\") or self.lof_scores is None:\n        raise ValueError(\"LOF scores not available. Run compute_bad_channels() first.\")\n\n    return dict(zip(self.channel_names, self.lof_scores))\n</code></pre>"},{"location":"reference/core/core/#neurodent.core.core.LongRecordingOrganizer.merge","title":"<code>merge(other_lro)</code>","text":"<p>Merge another LRO into this one using si.concatenate_recordings.</p> <p>This creates a new concatenated recording from this LRO and the other LRO. The other LRO should represent a later time period to maintain temporal order.</p> <p>Parameters:</p> Name Type Description Default <code>other_lro</code> <code>LongRecordingOrganizer</code> <p>The LRO to merge into this one</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If LROs are incompatible (different channels, sampling rates, etc.)</p> <code>ImportError</code> <p>If SpikeInterface is not available</p> Source code in <code>src/neurodent/core/core.py</code> <pre><code>def merge(self, other_lro):\n    \"\"\"Merge another LRO into this one using si.concatenate_recordings.\n\n    This creates a new concatenated recording from this LRO and the other LRO.\n    The other LRO should represent a later time period to maintain temporal order.\n\n    Args:\n        other_lro (LongRecordingOrganizer): The LRO to merge into this one\n\n    Raises:\n        ValueError: If LROs are incompatible (different channels, sampling rates, etc.)\n        ImportError: If SpikeInterface is not available\n    \"\"\"\n    if si is None:\n        raise ImportError(\"SpikeInterface is required for LRO merging\")\n\n    # Validate merge compatibility\n    self._validate_merge_compatibility(other_lro)\n\n    # Concatenate recordings using SpikeInterface\n    logging.info(f\"Merging LRO {other_lro.base_folder_path} into {self.base_folder_path}\")\n    self.LongRecording = si.concatenate_recordings([self.LongRecording, other_lro.LongRecording])\n\n    # Update metadata after merge\n    self._update_metadata_after_merge(other_lro)\n\n    logging.info(\"Successfully merged LRO recordings\")\n</code></pre>"},{"location":"reference/core/core/#neurodent.core.core.convert_ddfrowbin_to_si","title":"<code>convert_ddfrowbin_to_si(bin_rowmajor_path, metadata)</code>","text":"<p>Convert a row-major binary file to a SpikeInterface recording object.</p> <p>Parameters:</p> Name Type Description Default <code>bin_rowmajor_path</code> <code>str</code> <p>Path to the row-major binary file</p> required <code>metadata</code> <code>DDFBinaryMetadata</code> <p>Metadata object containing information about the recording</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing: - se.BaseRecording: The SpikeInterface Recording object. - str or None: Path to temporary file if created, None otherwise.</p> Source code in <code>src/neurodent/core/core.py</code> <pre><code>def convert_ddfrowbin_to_si(bin_rowmajor_path, metadata):\n    \"\"\"Convert a row-major binary file to a SpikeInterface recording object.\n\n    Args:\n        bin_rowmajor_path (str): Path to the row-major binary file\n        metadata (DDFBinaryMetadata): Metadata object containing information about the recording\n\n    Returns:\n        tuple: A tuple containing:\n            - se.BaseRecording: The SpikeInterface Recording object.\n            - str or None: Path to temporary file if created, None otherwise.\n    \"\"\"\n    if se is None:\n        raise ImportError(\"SpikeInterface is required for convert_ddfrowbin_to_si\")\n    assert isinstance(metadata, DDFBinaryMetadata), \"Metadata needs to be of type DDFBinaryMetadata\"\n\n    bin_rowmajor_path = Path(bin_rowmajor_path)\n    params = {\n        \"sampling_frequency\": metadata.f_s,\n        \"dtype\": metadata.precision,\n        \"num_channels\": metadata.n_channels,\n        \"gain_to_uV\": metadata.mult_to_uV,\n        \"offset_to_uV\": 0,\n        \"time_axis\": 0,\n        \"is_filtered\": False,\n    }\n\n    # Read either .npy.gz files or .bin files into the recording object\n    if \".npy.gz\" in str(bin_rowmajor_path):\n        temppath = os.path.join(get_temp_directory(), os.urandom(24).hex())\n        try:\n            with open(temppath, \"wb\") as tmp:\n                try:\n                    fcomp = gzip.GzipFile(bin_rowmajor_path, \"r\")\n                    bin_rowmajor_decomp = np.load(fcomp)\n                    bin_rowmajor_decomp.tofile(tmp)\n                except (EOFError, OSError) as e:\n                    logging.error(\n                        f\"Failed to read .npy.gz file: {bin_rowmajor_path}. Try regenerating row-major files.\"\n                    )\n                    raise\n\n            rec = se.read_binary(tmp.name, **params)\n        except Exception as e:\n            # Clean up temp file if it exists\n            if os.path.exists(temppath):\n                os.remove(temppath)\n            raise\n    else:\n        rec = se.read_binary(bin_rowmajor_path, **params)\n        temppath = None\n\n    if rec.sampling_frequency != constants.GLOBAL_SAMPLING_RATE:\n        warnings.warn(f\"Sampling rate {rec.sampling_frequency} Hz != {constants.GLOBAL_SAMPLING_RATE} Hz. Resampling\")\n        rec = spre.resample(rec, constants.GLOBAL_SAMPLING_RATE)\n        # Update metadata to reflect the new sampling rate\n        metadata.update_sampling_rate(constants.GLOBAL_SAMPLING_RATE)\n\n    rec = spre.astype(rec, dtype=constants.GLOBAL_DTYPE)\n\n    return rec, temppath\n</code></pre>"},{"location":"reference/core/sorting/","title":"Spike Sorting","text":""},{"location":"reference/core/sorting/#neurodent.core.analyze_sort.MountainSortAnalyzer","title":"<code>MountainSortAnalyzer</code>","text":"Source code in <code>src/neurodent/core/analyze_sort.py</code> <pre><code>class MountainSortAnalyzer:\n    @staticmethod\n    def sort_recording(\n        recording: \"si.BaseRecording\", plot_probe=False, multiprocess_mode: Literal[\"dask\", \"serial\"] = \"serial\"\n    ) -&gt; tuple[list[\"si.BaseSorting\"], list[\"si.BaseRecording\"]]:\n        \"\"\"Sort a recording using MountainSort.\n\n        Args:\n            recording (si.BaseRecording): The recording to sort.\n            plot_probe (bool, optional): Whether to plot the probe. Defaults to False.\n            multiprocess_mode (Literal[\"dask\", \"serial\"], optional): Whether to use dask or serial for multiprocessing. Defaults to \"serial\".\n\n        Returns:\n            list[si.SortingAnalyzer]: A list of independent sorting analyzers, one for each channel.\n        \"\"\"\n        if not MOUNTAINSORT_AVAILABLE:\n            raise ImportError(\n                \"MountainSort5 is not available. Spike sorting functionality requires mountainsort5. \"\n                \"Install it with: pip install mountainsort5\"\n            )\n\n        logging.debug(f\"Sorting recording info: {recording}\")\n        logging.debug(f\"Sorting recording channel names: {recording.get_channel_ids()}\")\n\n        if si is None or spre is None:\n            raise ImportError(\"spikeinterface is required for sorting\")\n        rec = recording.clone()\n        probe = MountainSortAnalyzer._get_dummy_probe(rec)\n        rec = rec.set_probe(probe)\n\n        if plot_probe:\n            _, ax2 = plt.subplots(1, 1)\n            pi_plotting.plot_probe(probe, ax=ax2, with_device_index=True, with_contact_id=True)\n            plt.show()\n\n        # Get recordings for sorting and waveforms\n        sort_rec = MountainSortAnalyzer._get_recording_for_sorting(rec)\n        wave_rec = MountainSortAnalyzer._get_recording_for_waveforms(rec)\n\n        # Split recording into separate channels\n        sort_recs = MountainSortAnalyzer._split_recording(sort_rec)\n        wave_recs = MountainSortAnalyzer._split_recording(wave_rec)\n\n        # Run sorting\n        match multiprocess_mode:\n            case \"dask\":\n                if dask is None:\n                    raise ImportError(\"dask is required for multiprocess_mode='dask'\")\n                cached_recs = [dask.delayed(MountainSortAnalyzer._cache_recording)(sort_rec) for sort_rec in sort_recs]\n                sortings = [dask.delayed(MountainSortAnalyzer._run_sorting)(cached_rec) for cached_rec in cached_recs]\n            case \"serial\":\n                cached_recs = [MountainSortAnalyzer._cache_recording(sort_rec) for sort_rec in sort_recs]\n                sortings = [MountainSortAnalyzer._run_sorting(cached_rec) for cached_rec in cached_recs]\n\n        return sortings, wave_recs\n\n    @staticmethod\n    def _get_dummy_probe(recording: si.BaseRecording) -&gt; pi.Probe:\n        linprobe = pi.generate_linear_probe(recording.get_num_channels(), ypitch=40)\n        linprobe.set_device_channel_indices(np.arange(recording.get_num_channels()))\n        linprobe.set_contact_ids(recording.get_channel_ids())\n        return linprobe\n\n    @staticmethod\n    def _get_recording_for_sorting(recording: si.BaseRecording) -&gt; si.BaseRecording:\n        return MountainSortAnalyzer._apply_preprocessing(recording, constants.SORTING_PARAMS)\n\n    @staticmethod\n    def _get_recording_for_waveforms(recording: si.BaseRecording) -&gt; si.BaseRecording:\n        return MountainSortAnalyzer._apply_preprocessing(recording, constants.WAVEFORM_PARAMS)\n\n    @staticmethod\n    def _apply_preprocessing(recording: si.BaseRecording, params: dict) -&gt; si.BaseRecording:\n        rec = recording.clone()\n\n        if params[\"notch_freq\"]:\n            rec = spre.notch_filter(rec, freq=params[\"notch_freq\"], q=100)\n        if params[\"common_ref\"]:\n            rec = spre.common_reference(rec)\n        if params[\"scale\"]:\n            rec = spre.scale(rec, gain=params[\"scale\"])  # Scaling for whitening to work properly\n        if params[\"whiten\"]:\n            rec = spre.whiten(rec)\n\n        if params[\"freq_min\"] and not params[\"freq_max\"]:\n            rec = spre.highpass_filter(rec, freq_min=params[\"freq_min\"], ftype=\"bessel\")\n        elif params[\"freq_min\"] and params[\"freq_max\"]:\n            rec = spre.bandpass_filter(rec, freq_min=params[\"freq_min\"], freq_max=params[\"freq_max\"], ftype=\"bessel\")\n        elif not params[\"freq_min\"] and params[\"freq_max\"]:\n            rec = spre.bandpass_filter(\n                rec, freq_min=0.1, freq_max=params[\"freq_max\"], ftype=\"bessel\"\n            )  # Spike Interface doesn't have a lowpass filter\n\n        return rec\n\n    @staticmethod\n    def _split_recording(recording: si.BaseRecording) -&gt; list[si.BaseRecording]:\n        rec_preps = []\n        for channel_id in recording.get_channel_ids():\n            rec_preps.append(recording.clone().select_channels([channel_id]))\n        return rec_preps\n\n    @staticmethod\n    def _cache_recording(recording: si.BaseRecording) -&gt; si.BaseRecording:\n        temp_dir = get_temp_directory() / os.urandom(24).hex()\n        # dask.distributed.print(f\"Caching recording to {temp_dir}\")\n        os.makedirs(temp_dir)\n        cached_rec = create_cached_recording(recording.clone(), folder=temp_dir, chunk_duration=\"60s\")\n        cached_rec = spre.astype(cached_rec, dtype=constants.GLOBAL_DTYPE)\n        return cached_rec\n\n    @staticmethod\n    def _run_sorting(recording: si.BaseRecording) -&gt; si.BaseSorting:\n        # Confusingly, the snippet_T1 and snippet_T2 parameters in MS are in samples, not seconds\n        snippet_T1 = constants.SCHEME2_SORTING_PARAMS[\"snippet_T1\"]\n        snippet_T2 = constants.SCHEME2_SORTING_PARAMS[\"snippet_T2\"]\n        snippet_T1_samples = round(recording.get_sampling_frequency() * snippet_T1)\n        snippet_T2_samples = round(recording.get_sampling_frequency() * snippet_T2)\n\n        sort_params = Scheme2SortingParameters(\n            phase1_detect_channel_radius=constants.SCHEME2_SORTING_PARAMS[\"phase1_detect_channel_radius\"],\n            detect_channel_radius=constants.SCHEME2_SORTING_PARAMS[\"detect_channel_radius\"],\n            snippet_T1=snippet_T1_samples,\n            snippet_T2=snippet_T2_samples,\n        )\n\n        # dask.distributed.print(f\"recording.dtype: {recording.dtype}\")\n        with _HiddenPrints():\n            sorting = sorting_scheme2(recording=recording, sorting_parameters=sort_params)\n\n        return sorting\n</code></pre>"},{"location":"reference/core/sorting/#neurodent.core.analyze_sort.MountainSortAnalyzer.sort_recording","title":"<code>sort_recording(recording, plot_probe=False, multiprocess_mode='serial')</code>  <code>staticmethod</code>","text":"<p>Sort a recording using MountainSort.</p> <p>Parameters:</p> Name Type Description Default <code>recording</code> <code>BaseRecording</code> <p>The recording to sort.</p> required <code>plot_probe</code> <code>bool</code> <p>Whether to plot the probe. Defaults to False.</p> <code>False</code> <code>multiprocess_mode</code> <code>Literal['dask', 'serial']</code> <p>Whether to use dask or serial for multiprocessing. Defaults to \"serial\".</p> <code>'serial'</code> <p>Returns:</p> Type Description <code>tuple[list[BaseSorting], list[BaseRecording]]</code> <p>list[si.SortingAnalyzer]: A list of independent sorting analyzers, one for each channel.</p> Source code in <code>src/neurodent/core/analyze_sort.py</code> <pre><code>@staticmethod\ndef sort_recording(\n    recording: \"si.BaseRecording\", plot_probe=False, multiprocess_mode: Literal[\"dask\", \"serial\"] = \"serial\"\n) -&gt; tuple[list[\"si.BaseSorting\"], list[\"si.BaseRecording\"]]:\n    \"\"\"Sort a recording using MountainSort.\n\n    Args:\n        recording (si.BaseRecording): The recording to sort.\n        plot_probe (bool, optional): Whether to plot the probe. Defaults to False.\n        multiprocess_mode (Literal[\"dask\", \"serial\"], optional): Whether to use dask or serial for multiprocessing. Defaults to \"serial\".\n\n    Returns:\n        list[si.SortingAnalyzer]: A list of independent sorting analyzers, one for each channel.\n    \"\"\"\n    if not MOUNTAINSORT_AVAILABLE:\n        raise ImportError(\n            \"MountainSort5 is not available. Spike sorting functionality requires mountainsort5. \"\n            \"Install it with: pip install mountainsort5\"\n        )\n\n    logging.debug(f\"Sorting recording info: {recording}\")\n    logging.debug(f\"Sorting recording channel names: {recording.get_channel_ids()}\")\n\n    if si is None or spre is None:\n        raise ImportError(\"spikeinterface is required for sorting\")\n    rec = recording.clone()\n    probe = MountainSortAnalyzer._get_dummy_probe(rec)\n    rec = rec.set_probe(probe)\n\n    if plot_probe:\n        _, ax2 = plt.subplots(1, 1)\n        pi_plotting.plot_probe(probe, ax=ax2, with_device_index=True, with_contact_id=True)\n        plt.show()\n\n    # Get recordings for sorting and waveforms\n    sort_rec = MountainSortAnalyzer._get_recording_for_sorting(rec)\n    wave_rec = MountainSortAnalyzer._get_recording_for_waveforms(rec)\n\n    # Split recording into separate channels\n    sort_recs = MountainSortAnalyzer._split_recording(sort_rec)\n    wave_recs = MountainSortAnalyzer._split_recording(wave_rec)\n\n    # Run sorting\n    match multiprocess_mode:\n        case \"dask\":\n            if dask is None:\n                raise ImportError(\"dask is required for multiprocess_mode='dask'\")\n            cached_recs = [dask.delayed(MountainSortAnalyzer._cache_recording)(sort_rec) for sort_rec in sort_recs]\n            sortings = [dask.delayed(MountainSortAnalyzer._run_sorting)(cached_rec) for cached_rec in cached_recs]\n        case \"serial\":\n            cached_recs = [MountainSortAnalyzer._cache_recording(sort_rec) for sort_rec in sort_recs]\n            sortings = [MountainSortAnalyzer._run_sorting(cached_rec) for cached_rec in cached_recs]\n\n    return sortings, wave_recs\n</code></pre>"},{"location":"reference/core/windowed/","title":"Windowed Analysis","text":""},{"location":"reference/core/windowed/#neurodent.core.analyze_frag.FragmentAnalyzer","title":"<code>FragmentAnalyzer</code>","text":"<p>Static class for analyzing fragments of EEG data. All functions receive a (N x M) numpy array, where N is the number of samples, and M is the number of channels.</p> Source code in <code>src/neurodent/core/analyze_frag.py</code> <pre><code>class FragmentAnalyzer:\n    \"\"\"Static class for analyzing fragments of EEG data.\n    All functions receive a (N x M) numpy array, where N is the number of samples, and M is the number of channels.\n    \"\"\"\n\n    # Unified feature dependency mapping - all dependencies as lists for consistency\n    FEATURE_DEPENDENCIES = {\n        # Log transforms\n        \"logrms\": [\"rms\"],\n        \"logampvar\": [\"ampvar\"],\n        \"lognspike\": [\"nspike\"],\n        \"logpsdband\": [\"psdband\"],\n        \"logpsdtotal\": [\"psdtotal\"],\n        \"logpsdfrac\": [\"psdfrac\"],\n        # Z-transforms\n        \"zpcorr\": [\"pcorr\"],\n        \"zcohere\": [\"cohere\"],\n        \"zimcoh\": [\"imcoh\"],\n        # PSD-dependent features\n        \"psdband\": [\"psd\"],\n        \"psdtotal\": [\"psd\"],\n        \"psdslope\": [\"psd\"],\n        \"psdfrac\": [\"psdband\"],\n        # Coherency-dependent features\n        \"cohere\": [\"coherency\"],\n        \"imcoh\": [\"coherency\"],\n    }\n\n    @staticmethod\n    def _process_fragment_features_dask(rec: np.ndarray, f_s: int, features: list[str], kwargs: dict):\n        \"\"\"\n        Legacy fragment processing method without dependency optimization.\n\n        Note: Consider using process_fragment_with_dependencies() instead for better performance\n        when computing interdependent features like PSD-based features.\n        \"\"\"\n        row = {}\n        for feat in features:\n            func = getattr(FragmentAnalyzer, f\"compute_{feat}\")\n            if callable(func):\n                row[feat] = func(rec=rec, f_s=f_s, **kwargs)\n            else:\n                raise AttributeError(f\"Invalid function {func}\")\n        return row\n\n    @staticmethod\n    def _check_rec_np(rec: np.ndarray, **kwargs):\n        \"\"\"Check if the recording is a numpy array and has the correct shape.\"\"\"\n        if not isinstance(rec, np.ndarray):\n            raise ValueError(\"rec must be a numpy array\")\n        if rec.ndim != 2:\n            raise ValueError(\"rec must be a 2D numpy array\")\n\n    @staticmethod\n    def _check_rec_mne(rec: np.ndarray, **kwargs):\n        \"\"\"Check if the recording is a MNE-ready numpy array.\"\"\"\n        if not isinstance(rec, np.ndarray):\n            raise ValueError(\"rec must be a numpy array\")\n        if rec.ndim != 3:\n            raise ValueError(\"rec must be a 3D numpy array\")\n        if rec.shape[0] != 1:\n            raise ValueError(\"rec must be a 1 x M x N array\")\n\n    @staticmethod\n    def _reshape_np_for_mne(rec: np.ndarray, **kwargs) -&gt; np.ndarray:\n        \"\"\"Reshape numpy array of (N x M) to (1 x M x N) array for MNE. 1 epoch, M = number of channels, N = number of samples.\"\"\"\n        FragmentAnalyzer._check_rec_np(rec)\n        rec = rec[..., np.newaxis]\n        return np.transpose(rec, (2, 1, 0))\n\n    @staticmethod\n    def compute_rms(rec: np.ndarray, **kwargs) -&gt; np.ndarray:\n        \"\"\"Compute the root mean square of the signal.\"\"\"\n        FragmentAnalyzer._check_rec_np(rec)\n        out = np.sqrt((rec**2).sum(axis=0) / rec.shape[0])\n        # del rec\n        return out\n\n    @staticmethod\n    def compute_logrms(rec: np.ndarray, precomputed_rms: np.ndarray = None, **kwargs) -&gt; np.ndarray:\n        \"\"\"Compute the log of the root mean square of the signal.\"\"\"\n        FragmentAnalyzer._check_rec_np(rec)\n        # Local import to avoid importing heavy dependencies from utils at module import time\n\n        if precomputed_rms is not None:\n            return log_transform(precomputed_rms)\n        else:\n            return log_transform(FragmentAnalyzer.compute_rms(rec, **kwargs))\n\n    @staticmethod\n    def compute_ampvar(rec: np.ndarray, **kwargs) -&gt; np.ndarray:\n        \"\"\"Compute the amplitude variance of the signal.\"\"\"\n        FragmentAnalyzer._check_rec_np(rec)\n        return np.std(rec, axis=0) ** 2\n\n    @staticmethod\n    def compute_logampvar(rec: np.ndarray, precomputed_ampvar: np.ndarray = None, **kwargs) -&gt; np.ndarray:\n        \"\"\"Compute the log of the amplitude variance of the signal.\"\"\"\n        FragmentAnalyzer._check_rec_np(rec)\n        # Local import to avoid importing heavy dependencies from utils at module import time\n\n        if precomputed_ampvar is not None:\n            return log_transform(precomputed_ampvar)\n        else:\n            return log_transform(FragmentAnalyzer.compute_ampvar(rec, **kwargs))\n\n    @staticmethod\n    def compute_psd(\n        rec: np.ndarray,\n        f_s: float,\n        welch_bin_t: float = 1,\n        notch_filter: bool = True,\n        multitaper: bool = False,\n        **kwargs,\n    ) -&gt; np.ndarray:\n        \"\"\"Compute the power spectral density of the signal.\"\"\"\n        FragmentAnalyzer._check_rec_np(rec)\n\n        if notch_filter:\n            b, a = iirnotch(constants.LINE_FREQ, 30, fs=f_s)\n            rec = filtfilt(b, a, rec, axis=0)\n\n        if not multitaper:\n            f, psd = welch(rec, fs=f_s, nperseg=round(welch_bin_t * f_s), axis=0)\n        else:\n            if psd_array_multitaper is None:\n                raise ImportError(\"mne is required for multitaper PSD; install mne or set multitaper=False\")\n            # REVIEW psd calulation will give different bins if using multitaper\n            psd, f = psd_array_multitaper(\n                rec.transpose(),\n                f_s,\n                fmax=constants.FREQ_BAND_TOTAL[1],\n                adaptive=True,\n                normalization=\"full\",\n                low_bias=False,\n                verbose=0,\n            )\n            psd = psd.transpose()\n        return f, psd\n\n    @staticmethod\n    def compute_psdband(\n        rec: np.ndarray,\n        f_s: float,\n        welch_bin_t: float = 1,\n        notch_filter: bool = True,\n        bands: list[tuple[float, float]] = constants.FREQ_BANDS,\n        multitaper: bool = False,\n        precomputed_psd: tuple = None,\n        **kwargs,\n    ) -&gt; dict[str, np.ndarray]:\n        \"\"\"Compute the power spectral density of the signal for each frequency band.\"\"\"\n        FragmentAnalyzer._check_rec_np(rec)\n\n        if precomputed_psd is not None:\n            f, psd = precomputed_psd\n        else:\n            f, psd = FragmentAnalyzer.compute_psd(rec, f_s, welch_bin_t, notch_filter, multitaper, **kwargs)\n        deltaf = np.median(np.diff(f))\n\n        # Integrate each band separately using trapezoidal integration\n        result = {}\n\n        for band_name, (f_low, f_high) in bands.items():\n            # Always use inclusive boundaries for both ends\n            freq_mask = np.logical_and(f &gt;= f_low, f &lt;= f_high)\n            result[band_name] = trapezoid(psd[freq_mask, :], dx=deltaf, axis=0)\n\n        return result\n\n    @staticmethod\n    def compute_logpsdband(\n        rec: np.ndarray,\n        f_s: float,\n        welch_bin_t: float = 1,\n        notch_filter: bool = True,\n        bands: list[tuple[float, float]] = constants.FREQ_BANDS,\n        multitaper: bool = False,\n        precomputed_psd: tuple = None,\n        precomputed_psdband: dict = None,\n        **kwargs,\n    ) -&gt; dict[str, np.ndarray]:\n        \"\"\"Compute the log of the power spectral density of the signal for each frequency band.\"\"\"\n        FragmentAnalyzer._check_rec_np(rec)\n\n        # Local import to avoid importing heavy dependencies from utils at module import time\n\n        if precomputed_psdband is not None:\n            psd = precomputed_psdband\n        else:\n            psd = FragmentAnalyzer.compute_psdband(\n                rec, f_s, welch_bin_t, notch_filter, bands, multitaper, precomputed_psd=precomputed_psd, **kwargs\n            )\n        return {k: log_transform(v) for k, v in psd.items()}\n\n    @staticmethod\n    def compute_psdtotal(\n        rec: np.ndarray,\n        f_s: float,\n        welch_bin_t: float = 1,\n        notch_filter: bool = True,\n        band: tuple[float, float] = constants.FREQ_BAND_TOTAL,\n        multitaper: bool = False,\n        precomputed_psd: tuple = None,\n        **kwargs,\n    ) -&gt; np.ndarray:\n        \"\"\"Compute the total power spectral density of the signal.\"\"\"\n        FragmentAnalyzer._check_rec_np(rec)\n\n        if precomputed_psd is not None:\n            f, psd = precomputed_psd\n        else:\n            f, psd = FragmentAnalyzer.compute_psd(rec, f_s, welch_bin_t, notch_filter, multitaper, **kwargs)\n        deltaf = np.median(np.diff(f))\n\n        # Use inclusive bounds for total power calculation\n        freq_mask = np.logical_and(f &gt;= band[0], f &lt;= band[1])\n        return trapezoid(psd[freq_mask, :], dx=deltaf, axis=0)\n\n    @staticmethod\n    def compute_logpsdtotal(\n        rec: np.ndarray,\n        f_s: float,\n        welch_bin_t: float = 1,\n        notch_filter: bool = True,\n        band: tuple[float, float] = constants.FREQ_BAND_TOTAL,\n        multitaper: bool = False,\n        precomputed_psd: tuple = None,\n        precomputed_psdtotal: np.ndarray = None,\n        **kwargs,\n    ) -&gt; np.ndarray:\n        \"\"\"Compute the log of the total power spectral density of the signal.\"\"\"\n        FragmentAnalyzer._check_rec_np(rec)\n\n        # Local import to avoid importing heavy dependencies from utils at module import time\n\n        if precomputed_psdtotal is not None:\n            return log_transform(precomputed_psdtotal)\n        else:\n            return log_transform(\n                FragmentAnalyzer.compute_psdtotal(\n                    rec, f_s, welch_bin_t, notch_filter, band, multitaper, precomputed_psd=precomputed_psd, **kwargs\n                )\n            )\n\n    @staticmethod\n    def compute_psdfrac(\n        rec: np.ndarray,\n        f_s: float,\n        welch_bin_t: float = 1,\n        notch_filter: bool = True,\n        bands: list[tuple[float, float]] = constants.FREQ_BANDS,\n        total_band: tuple[float, float] = constants.FREQ_BAND_TOTAL,\n        multitaper: bool = False,\n        precomputed_psdband: dict = None,\n        **kwargs,\n    ) -&gt; dict[str, np.ndarray]:\n        \"\"\"Compute the power spectral density of bands as a fraction of the total power.\"\"\"\n        FragmentAnalyzer._check_rec_np(rec)\n\n        if precomputed_psdband is not None:\n            psdband = precomputed_psdband\n        else:\n            psdband = FragmentAnalyzer.compute_psdband(rec, f_s, welch_bin_t, notch_filter, bands, multitaper, **kwargs)\n        psdtotal = sum(psdband.values())\n\n        return {k: v / psdtotal for k, v in psdband.items()}\n\n    @staticmethod\n    def compute_logpsdfrac(\n        rec: np.ndarray,\n        f_s: float,\n        welch_bin_t: float = 1,\n        notch_filter: bool = True,\n        bands: list[tuple[float, float]] = constants.FREQ_BANDS,\n        total_band: tuple[float, float] = constants.FREQ_BAND_TOTAL,\n        multitaper: bool = False,\n        precomputed_psdfrac: dict = None,\n        **kwargs,\n    ) -&gt; dict[str, np.ndarray]:\n        \"\"\"Compute the log of the power spectral density of bands as a fraction of the log total power.\"\"\"\n        FragmentAnalyzer._check_rec_np(rec)\n\n        if precomputed_psdfrac is not None:\n            psdfrac = precomputed_psdfrac\n        else:\n            psdfrac = FragmentAnalyzer.compute_psdfrac(\n                rec, f_s, welch_bin_t, notch_filter, bands, total_band, multitaper, **kwargs\n            )\n\n        return {k: log_transform(v) for k, v in psdfrac.items()}\n\n    @staticmethod\n    def compute_psdslope(\n        rec: np.ndarray,\n        f_s: float,\n        welch_bin_t: float = 1,\n        notch_filter: bool = True,\n        band: tuple[float, float] = constants.FREQ_BAND_TOTAL,\n        multitaper: bool = False,\n        precomputed_psd: tuple = None,\n        **kwargs,\n    ) -&gt; np.ndarray:\n        \"\"\"Compute the slope of the power spectral density of the signal on a log-log scale.\"\"\"\n        FragmentAnalyzer._check_rec_np(rec)\n\n        if precomputed_psd is not None:\n            f, psd = precomputed_psd\n        else:\n            f, psd = FragmentAnalyzer.compute_psd(rec, f_s, welch_bin_t, notch_filter, multitaper, **kwargs)\n\n        freqs = f[np.logical_and(f &gt;= band[0], f &lt;= band[1])]\n        psd_band = psd[np.logical_and(f &gt;= band[0], f &lt;= band[1]), :]\n        logpsd = np.log10(psd_band)\n        logf = np.log10(freqs)\n\n        # Fit a line to the log-transformed data\n        out = []\n        for i in range(psd_band.shape[1]):\n            result = linregress(logf, logpsd[:, i])\n            out.append([result.slope, result.intercept])\n        return np.array(out)\n\n    @staticmethod\n    def _get_freqs_cycles(\n        rec: np.ndarray,\n        f_s: float,\n        freq_res: float,\n        geomspace: bool,\n        mode: Literal[\"cwt_morlet\", \"multitaper\"],\n        cwt_n_cycles_max: float,\n        epsilon: float,\n    ) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"Get the frequencies and number of cycles for the signal.\n        rec is a (1 x M x N) numpy array for MNE. N = number of samples, M = number of channels.\n        \"\"\"\n        FragmentAnalyzer._check_rec_mne(rec)\n\n        if geomspace:  # REVIEW by default geomspace is True, but a linear scale is simpler for DOF calculation -&gt; zcohere correction\n            freqs = np.geomspace(\n                constants.FREQ_BAND_TOTAL[0],\n                constants.FREQ_BAND_TOTAL[1],\n                round((np.diff(constants.FREQ_BAND_TOTAL) / freq_res).item()),\n            )\n        else:\n            freqs = np.arange(constants.FREQ_BAND_TOTAL[0], constants.FREQ_BAND_TOTAL[1], freq_res)\n\n        frag_len_s = rec.shape[2] / f_s\n        if mode == \"cwt_morlet\":\n            maximum_cyc = (frag_len_s * f_s + 1) * np.pi / 5 * freqs / f_s\n            maximum_cyc = maximum_cyc - epsilon  # Shave off a bit to avoid indexing errors\n            n_cycles = np.minimum(np.full(maximum_cyc.shape, cwt_n_cycles_max), maximum_cyc)\n        elif mode == \"multitaper\":\n            maximum_cyc = frag_len_s * freqs  # Maximize number of cycles for maximum frequency resolution\n            maximum_cyc = maximum_cyc - epsilon\n            n_cycles = maximum_cyc\n\n        return freqs, n_cycles\n\n    @staticmethod\n    def compute_coherency(\n        rec: np.ndarray,\n        f_s: float,\n        freq_res: float = 1,\n        mode: Literal[\"cwt_morlet\", \"multitaper\"] = \"multitaper\",\n        geomspace: bool = False,\n        cwt_n_cycles_max: float = 7.0,\n        mt_bandwidth: float = 4.0,\n        downsamp_q: int = 4,\n        epsilon: float = 1e-2,\n        **kwargs,\n    ) -&gt; np.ndarray:\n        \"\"\"Compute the complex coherency of the signal.\"\"\"\n        FragmentAnalyzer._check_rec_np(rec)\n\n        rec_mne = FragmentAnalyzer._reshape_np_for_mne(rec)\n        rec_mne = decimate(rec_mne, q=downsamp_q, axis=2)  # Along the time axis\n        f_s = int(f_s / downsamp_q)\n\n        f, n_cycles = FragmentAnalyzer._get_freqs_cycles(\n            rec=rec_mne,\n            f_s=f_s,\n            freq_res=freq_res,\n            geomspace=geomspace,\n            mode=mode,\n            cwt_n_cycles_max=cwt_n_cycles_max,\n            epsilon=epsilon,\n        )\n\n        if spectral_connectivity_epochs is None:\n            raise ImportError(\"mne_connectivity is required for connectivity computations\")\n        try:\n            con = spectral_connectivity_epochs(\n                rec_mne,\n                # freqs=f,\n                method=\"cohy\",\n                # average=True,\n                faverage=True,\n                mode=mode,\n                fmin=constants.FREQ_MINS,\n                fmax=constants.FREQ_MAXS,\n                sfreq=f_s,\n                cwt_freqs=f,\n                cwt_n_cycles=n_cycles,\n                mt_bandwidth=mt_bandwidth,\n                verbose=False,\n            )\n        except MemoryError as e:\n            raise MemoryError(\n                \"Out of memory. Use a larger freq_res parameter, a smaller n_cycles_max parameter, or a larger downsamp_q parameter\"\n            ) from e\n\n        data = con.get_data()\n        n_channels = rec.shape[1]\n\n        out = {}\n        # Make data symmetric\n        for i, band_name in enumerate(constants.BAND_NAMES):\n            if i &gt;= data.shape[1]:  # Skip if we don't have data for this band\n                warnings.warn(f\"No coherence data for band {band_name}\")\n                continue\n\n            band_data = data[:, i]\n\n            full_matrix = band_data.reshape((n_channels, n_channels))\n\n            symmetric_matrix = full_matrix.copy()\n            symmetric_matrix = np.triu(symmetric_matrix.T, k=1) + np.tril(symmetric_matrix, k=-1)\n\n            np.fill_diagonal(symmetric_matrix, 1.0)\n\n            out[band_name] = symmetric_matrix\n        return out\n\n    @staticmethod\n    def compute_cohere(\n        rec: np.ndarray,\n        f_s: float,\n        precomputed_coherency: dict = None,\n        **kwargs,\n    ) -&gt; np.ndarray:\n        \"\"\"Compute the coherence of the signal.\"\"\"\n        FragmentAnalyzer._check_rec_np(rec)\n        if precomputed_coherency is not None:\n            cohere = precomputed_coherency\n        else:\n            cohere = FragmentAnalyzer.compute_coherency(rec, f_s, **kwargs)\n        return {k: np.abs(v) for k, v in cohere.items()}\n\n    @staticmethod\n    def compute_zcohere(\n        rec: np.ndarray, f_s: float, z_epsilon: float = 1e-6, precomputed_cohere=None, **kwargs\n    ) -&gt; dict[str, np.ndarray]:\n        \"\"\"Compute the Fisher z-transformed coherence of the signal.\n\n        Args:\n            rec: Input signal array\n            f_s: Sampling frequency\n            z_epsilon: Small value to prevent arctanh(1) = inf. Values are clipped to [-1+z_epsilon, 1-z_epsilon]\n            **kwargs: Additional arguments passed to compute_cohere\n        \"\"\"\n        FragmentAnalyzer._check_rec_np(rec)\n\n        if precomputed_cohere is not None:\n            cohere = precomputed_cohere.copy()\n        else:\n            cohere = FragmentAnalyzer.compute_cohere(rec, f_s, **kwargs)\n        clip_max = 1.0 - z_epsilon\n        clip_min = -1.0 + z_epsilon\n        return {k: np.arctanh(np.clip(v, clip_min, clip_max)) for k, v in cohere.items()}\n\n    @staticmethod\n    def compute_imcoh(\n        rec: np.ndarray, f_s: float, precomputed_coherency: dict = None, **kwargs\n    ) -&gt; dict[str, np.ndarray]:\n        \"\"\"Compute the imaginary coherence of the signal.\"\"\"\n        FragmentAnalyzer._check_rec_np(rec)\n        if precomputed_coherency is not None:\n            cohere = precomputed_coherency\n        else:\n            cohere = FragmentAnalyzer.compute_coherency(rec, f_s, **kwargs)\n        return {k: np.imag(v) for k, v in cohere.items()}\n\n    @staticmethod\n    def compute_zimcoh(\n        rec: np.ndarray, f_s: float, z_epsilon: float = 1e-6, precomputed_imcoh: dict = None, **kwargs\n    ) -&gt; dict[str, np.ndarray]:\n        \"\"\"Compute the Fisher z-transformed imaginary coherence of the signal.\"\"\"\n        FragmentAnalyzer._check_rec_np(rec)\n        if precomputed_imcoh is not None:\n            imcoh = precomputed_imcoh\n        else:\n            imcoh = FragmentAnalyzer.compute_imcoh(rec, f_s, **kwargs)\n        clip_max = 1.0 - z_epsilon\n        clip_min = -1.0 + z_epsilon\n        return {k: np.arctanh(np.clip(v, clip_min, clip_max)) for k, v in imcoh.items()}\n\n    @staticmethod\n    def compute_pcorr(rec: np.ndarray, f_s: float, lower_triag: bool = False, **kwargs) -&gt; np.ndarray:\n        \"\"\"Compute the Pearson correlation coefficient of the signal.\"\"\"\n        FragmentAnalyzer._check_rec_np(rec)\n\n        sos = butter(2, constants.FREQ_BAND_TOTAL, btype=\"bandpass\", output=\"sos\", fs=f_s)\n        rec = sosfiltfilt(sos, rec, axis=0)\n\n        rec = rec.transpose()\n        result = pearsonr(rec[:, np.newaxis, :], rec, axis=-1)\n        if lower_triag:\n            return np.tril(result.correlation, k=-1)\n        else:\n            return result.correlation\n\n    @staticmethod\n    def compute_zpcorr(\n        rec: np.ndarray, f_s: float, z_epsilon: float = 1e-6, precomputed_pcorr: np.ndarray = None, **kwargs\n    ) -&gt; np.ndarray:\n        \"\"\"Compute the Fisher z-transformed Pearson correlation coefficient of the signal.\n\n        Args:\n            rec: Input signal array\n            f_s: Sampling frequency\n            z_epsilon: Small value to prevent arctanh(1) = inf. Values are clipped to [-1+z_epsilon, 1-z_epsilon]\n            **kwargs: Additional arguments passed to compute_pcorr\n        \"\"\"\n        FragmentAnalyzer._check_rec_np(rec)\n\n        if precomputed_pcorr is not None:\n            pcorr = precomputed_pcorr.copy()\n        else:\n            # Get full correlation matrix for z-transform\n            pcorr = FragmentAnalyzer.compute_pcorr(rec, f_s, lower_triag=False, **kwargs)\n        clip_max = 1.0 - z_epsilon\n        clip_min = -1.0 + z_epsilon\n        return np.arctanh(np.clip(pcorr, clip_min, clip_max))\n\n    @staticmethod\n    def compute_nspike(rec: np.ndarray, **kwargs):\n        \"\"\"Returns NaN array as placeholder. Compute and load in spikes with SpikeAnalysisResult\"\"\"\n        return np.full(rec.shape[1], np.nan)\n\n    @staticmethod\n    def compute_lognspike(rec: np.ndarray, precomputed_nspike: np.ndarray = None, **kwargs):\n        \"\"\"Returns log-transformed NaN array as placeholder. Compute and load in spikes with SpikeAnalysisResult\"\"\"\n        # Local import to avoid importing heavy dependencies from utils at module import time\n\n        if precomputed_nspike is not None:\n            return log_transform(precomputed_nspike)\n        else:\n            n_spike = FragmentAnalyzer.compute_nspike(rec, **kwargs)\n            return log_transform(n_spike)\n\n    # def compute_csd(self, index, magnitude=True, n_jobs=None, **kwargs) -&gt; np.ndarray:\n    #     rec = self.get_fragment_mne(index)\n    #     csd = csd_array_fourier(rec, self.f_s,\n    #                             fmin=constants.FREQ_BAND_TOTAL[0],\n    #                             fmax=constants.FREQ_BAND_TOTAL[1],\n    #                             ch_names=self.channel_names,\n    #                             n_jobs=n_jobs,\n    #                             verbose=False)\n    #     out = {}\n    #     for k,v in constants.FREQ_BANDS.items():\n    #         try:\n    #             csd_band = csd.mean(fmin=v[0], fmax=v[1]) # Breaks if slice is too short\n    #         except (IndexError, UnboundLocalError):\n    #             timebound = self.convert_idx_to_timebound(index)\n    #             warnings.warn(f\"compute_csd failed for window {index}, {round(timebound[1]-timebound[0], 5)} s. Likely too short\")\n    #             data = self.compute_csd(index - 1, magnitude)[k]\n    #         else:\n    #             data = csd_band.get_data()\n    #         finally:\n    #             if magnitude:\n    #                 out[k] = np.abs(data)\n    #             else:\n    #                 out[k] = data\n    #     return out\n\n    # def compute_envcorr(self, index, **kwargs) -&gt; np.ndarray:\n    #     rec = spre.bandpass_filter(self.get_fragment_rec(index),\n    #                                 freq_min=constants.FREQ_BAND_TOTAL[0],\n    #                                 freq_max=constants.FREQ_BAND_TOTAL[1])\n    #     rec = self.get_fragment_mne(index, rec)\n    #     envcor = envelope_correlation(rec, self.channel_names)\n    #     return envcor.get_data().reshape((self.n_channels, self.n_channels))\n\n    # def compute_pac(self, index):\n    #     ... # NOTE implement CFC measures\n\n    # def compute_cacoh(self, index, freq_res=1, n_cycles_max=7.0, geomspace=True, mode:str='cwt_morlet', downsamp_q=4, epsilon=1e-2, mag_phase=True, indices=None, **kwargs):\n    #     rec = self.get_fragment_mne(index)\n    #     rec = decimate(rec, q=downsamp_q, axis=-1)\n    #     freqs, n_cycles = self.__get_freqs_cycles(index=index, freq_res=freq_res, n_cycles_max=n_cycles_max, geomspace=geomspace, mode=mode, epsilon=epsilon)\n    #     try:\n    #         con = spectral_connectivity_time(rec,\n    #                                         freqs=freqs,\n    #                                         method='cacoh',\n    #                                         average=True,\n    #                                         mode=mode,\n    #                                         fmin=constants.FREQ_BAND_TOTAL[0],\n    #                                         fmax=constants.FREQ_BAND_TOTAL[1],\n    #                                         sfreq=self.f_s / downsamp_q,\n    #                                         n_cycles=n_cycles,\n    #                                         indices=indices, # NOTE implement L/R hemisphere coherence metrics\n    #                                         verbose=False)\n    #     except MemoryError as e:\n    #         raise MemoryError(\"Out of memory, use a larger freq_res parameter\") from e\n\n    #     data:np.ndarray = con.get_data().squeeze()\n    #     if mag_phase:\n    #         return np.abs(data), np.angle(data, deg=True), con.freqs\n    #     else:\n    #         return data, con.freqs\n\n    @staticmethod\n    def process_fragment_with_dependencies(\n        fragment_data: np.ndarray, f_s: int, features: List[str], kwargs: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Process a single fragment with efficient dependency resolution.\n\n        This is the enhanced replacement for _process_fragment_features_dask that\n        automatically resolves feature dependencies and reuses intermediate calculations\n        to avoid redundant computations (e.g., computing PSD once for multiple dependent features).\n\n        Args:\n            fragment_data: Single fragment data with shape (n_samples, n_channels)\n            f_s: Sampling frequency\n            features: List of features to compute\n            kwargs: Additional parameters for feature computation\n\n        Returns:\n            Dictionary of computed features for this fragment\n        \"\"\"\n        computed_cache = {}\n        results = {}\n\n        # Compute all requested features using dependency resolution\n        for feature in features:\n            if feature not in results:\n                results[feature] = FragmentAnalyzer._resolve_feature_dependencies(\n                    feature, fragment_data, f_s, kwargs, computed_cache\n                )\n\n        return results\n\n    @staticmethod\n    def _resolve_feature_dependencies(\n        feature: str, fragment_data: np.ndarray, f_s: int, kwargs: Dict[str, Any], computed_cache: Dict[str, Any]\n    ) -&gt; Any:\n        \"\"\"\n        Resolve feature dependencies recursively, caching intermediate results.\n\n        This handles the dependency tree resolution automatically. For example:\n        - logpsdfrac -&gt; psdfrac -&gt; [psdband, psdtotal] -&gt; psd\n        - All intermediate results are cached and reused within the fragment\n\n        Args:\n            feature: Name of feature to compute\n            fragment_data: EEG fragment data (n_samples, n_channels)\n            f_s: Sampling frequency\n            kwargs: Computation parameters\n            computed_cache: Cache to store intermediate results for this fragment\n\n        Returns:\n            Computed feature value\n        \"\"\"\n        # Return cached result if already computed\n        if feature in computed_cache:\n            return computed_cache[feature]\n\n        # Check if feature has dependencies\n        if feature in FragmentAnalyzer.FEATURE_DEPENDENCIES:\n            dependencies = FragmentAnalyzer.FEATURE_DEPENDENCIES[feature]\n\n            # Recursively compute all dependencies first\n            precomputed_kwargs = kwargs.copy()\n            for dependency in dependencies:\n                if dependency not in computed_cache:\n                    computed_cache[dependency] = FragmentAnalyzer._resolve_feature_dependencies(\n                        dependency, fragment_data, f_s, kwargs, computed_cache\n                    )\n                precomputed_key = f\"precomputed_{dependency}\"\n                precomputed_kwargs[precomputed_key] = computed_cache[dependency]\n\n            # Compute the feature using precomputed dependencies\n            func = getattr(FragmentAnalyzer, f\"compute_{feature}\")\n            result = func(rec=fragment_data, f_s=f_s, **precomputed_kwargs)\n        else:\n            # Base feature with no dependencies - compute directly\n            func = getattr(FragmentAnalyzer, f\"compute_{feature}\")\n            result = func(rec=fragment_data, f_s=f_s, **kwargs)\n\n        # Cache and return result\n        computed_cache[feature] = result\n        return result\n</code></pre>"},{"location":"reference/core/windowed/#neurodent.core.analyze_frag.FragmentAnalyzer.compute_ampvar","title":"<code>compute_ampvar(rec, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Compute the amplitude variance of the signal.</p> Source code in <code>src/neurodent/core/analyze_frag.py</code> <pre><code>@staticmethod\ndef compute_ampvar(rec: np.ndarray, **kwargs) -&gt; np.ndarray:\n    \"\"\"Compute the amplitude variance of the signal.\"\"\"\n    FragmentAnalyzer._check_rec_np(rec)\n    return np.std(rec, axis=0) ** 2\n</code></pre>"},{"location":"reference/core/windowed/#neurodent.core.analyze_frag.FragmentAnalyzer.compute_cohere","title":"<code>compute_cohere(rec, f_s, precomputed_coherency=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Compute the coherence of the signal.</p> Source code in <code>src/neurodent/core/analyze_frag.py</code> <pre><code>@staticmethod\ndef compute_cohere(\n    rec: np.ndarray,\n    f_s: float,\n    precomputed_coherency: dict = None,\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"Compute the coherence of the signal.\"\"\"\n    FragmentAnalyzer._check_rec_np(rec)\n    if precomputed_coherency is not None:\n        cohere = precomputed_coherency\n    else:\n        cohere = FragmentAnalyzer.compute_coherency(rec, f_s, **kwargs)\n    return {k: np.abs(v) for k, v in cohere.items()}\n</code></pre>"},{"location":"reference/core/windowed/#neurodent.core.analyze_frag.FragmentAnalyzer.compute_coherency","title":"<code>compute_coherency(rec, f_s, freq_res=1, mode='multitaper', geomspace=False, cwt_n_cycles_max=7.0, mt_bandwidth=4.0, downsamp_q=4, epsilon=0.01, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Compute the complex coherency of the signal.</p> Source code in <code>src/neurodent/core/analyze_frag.py</code> <pre><code>@staticmethod\ndef compute_coherency(\n    rec: np.ndarray,\n    f_s: float,\n    freq_res: float = 1,\n    mode: Literal[\"cwt_morlet\", \"multitaper\"] = \"multitaper\",\n    geomspace: bool = False,\n    cwt_n_cycles_max: float = 7.0,\n    mt_bandwidth: float = 4.0,\n    downsamp_q: int = 4,\n    epsilon: float = 1e-2,\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"Compute the complex coherency of the signal.\"\"\"\n    FragmentAnalyzer._check_rec_np(rec)\n\n    rec_mne = FragmentAnalyzer._reshape_np_for_mne(rec)\n    rec_mne = decimate(rec_mne, q=downsamp_q, axis=2)  # Along the time axis\n    f_s = int(f_s / downsamp_q)\n\n    f, n_cycles = FragmentAnalyzer._get_freqs_cycles(\n        rec=rec_mne,\n        f_s=f_s,\n        freq_res=freq_res,\n        geomspace=geomspace,\n        mode=mode,\n        cwt_n_cycles_max=cwt_n_cycles_max,\n        epsilon=epsilon,\n    )\n\n    if spectral_connectivity_epochs is None:\n        raise ImportError(\"mne_connectivity is required for connectivity computations\")\n    try:\n        con = spectral_connectivity_epochs(\n            rec_mne,\n            # freqs=f,\n            method=\"cohy\",\n            # average=True,\n            faverage=True,\n            mode=mode,\n            fmin=constants.FREQ_MINS,\n            fmax=constants.FREQ_MAXS,\n            sfreq=f_s,\n            cwt_freqs=f,\n            cwt_n_cycles=n_cycles,\n            mt_bandwidth=mt_bandwidth,\n            verbose=False,\n        )\n    except MemoryError as e:\n        raise MemoryError(\n            \"Out of memory. Use a larger freq_res parameter, a smaller n_cycles_max parameter, or a larger downsamp_q parameter\"\n        ) from e\n\n    data = con.get_data()\n    n_channels = rec.shape[1]\n\n    out = {}\n    # Make data symmetric\n    for i, band_name in enumerate(constants.BAND_NAMES):\n        if i &gt;= data.shape[1]:  # Skip if we don't have data for this band\n            warnings.warn(f\"No coherence data for band {band_name}\")\n            continue\n\n        band_data = data[:, i]\n\n        full_matrix = band_data.reshape((n_channels, n_channels))\n\n        symmetric_matrix = full_matrix.copy()\n        symmetric_matrix = np.triu(symmetric_matrix.T, k=1) + np.tril(symmetric_matrix, k=-1)\n\n        np.fill_diagonal(symmetric_matrix, 1.0)\n\n        out[band_name] = symmetric_matrix\n    return out\n</code></pre>"},{"location":"reference/core/windowed/#neurodent.core.analyze_frag.FragmentAnalyzer.compute_imcoh","title":"<code>compute_imcoh(rec, f_s, precomputed_coherency=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Compute the imaginary coherence of the signal.</p> Source code in <code>src/neurodent/core/analyze_frag.py</code> <pre><code>@staticmethod\ndef compute_imcoh(\n    rec: np.ndarray, f_s: float, precomputed_coherency: dict = None, **kwargs\n) -&gt; dict[str, np.ndarray]:\n    \"\"\"Compute the imaginary coherence of the signal.\"\"\"\n    FragmentAnalyzer._check_rec_np(rec)\n    if precomputed_coherency is not None:\n        cohere = precomputed_coherency\n    else:\n        cohere = FragmentAnalyzer.compute_coherency(rec, f_s, **kwargs)\n    return {k: np.imag(v) for k, v in cohere.items()}\n</code></pre>"},{"location":"reference/core/windowed/#neurodent.core.analyze_frag.FragmentAnalyzer.compute_logampvar","title":"<code>compute_logampvar(rec, precomputed_ampvar=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Compute the log of the amplitude variance of the signal.</p> Source code in <code>src/neurodent/core/analyze_frag.py</code> <pre><code>@staticmethod\ndef compute_logampvar(rec: np.ndarray, precomputed_ampvar: np.ndarray = None, **kwargs) -&gt; np.ndarray:\n    \"\"\"Compute the log of the amplitude variance of the signal.\"\"\"\n    FragmentAnalyzer._check_rec_np(rec)\n    # Local import to avoid importing heavy dependencies from utils at module import time\n\n    if precomputed_ampvar is not None:\n        return log_transform(precomputed_ampvar)\n    else:\n        return log_transform(FragmentAnalyzer.compute_ampvar(rec, **kwargs))\n</code></pre>"},{"location":"reference/core/windowed/#neurodent.core.analyze_frag.FragmentAnalyzer.compute_lognspike","title":"<code>compute_lognspike(rec, precomputed_nspike=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Returns log-transformed NaN array as placeholder. Compute and load in spikes with SpikeAnalysisResult</p> Source code in <code>src/neurodent/core/analyze_frag.py</code> <pre><code>@staticmethod\ndef compute_lognspike(rec: np.ndarray, precomputed_nspike: np.ndarray = None, **kwargs):\n    \"\"\"Returns log-transformed NaN array as placeholder. Compute and load in spikes with SpikeAnalysisResult\"\"\"\n    # Local import to avoid importing heavy dependencies from utils at module import time\n\n    if precomputed_nspike is not None:\n        return log_transform(precomputed_nspike)\n    else:\n        n_spike = FragmentAnalyzer.compute_nspike(rec, **kwargs)\n        return log_transform(n_spike)\n</code></pre>"},{"location":"reference/core/windowed/#neurodent.core.analyze_frag.FragmentAnalyzer.compute_logpsdband","title":"<code>compute_logpsdband(rec, f_s, welch_bin_t=1, notch_filter=True, bands=constants.FREQ_BANDS, multitaper=False, precomputed_psd=None, precomputed_psdband=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Compute the log of the power spectral density of the signal for each frequency band.</p> Source code in <code>src/neurodent/core/analyze_frag.py</code> <pre><code>@staticmethod\ndef compute_logpsdband(\n    rec: np.ndarray,\n    f_s: float,\n    welch_bin_t: float = 1,\n    notch_filter: bool = True,\n    bands: list[tuple[float, float]] = constants.FREQ_BANDS,\n    multitaper: bool = False,\n    precomputed_psd: tuple = None,\n    precomputed_psdband: dict = None,\n    **kwargs,\n) -&gt; dict[str, np.ndarray]:\n    \"\"\"Compute the log of the power spectral density of the signal for each frequency band.\"\"\"\n    FragmentAnalyzer._check_rec_np(rec)\n\n    # Local import to avoid importing heavy dependencies from utils at module import time\n\n    if precomputed_psdband is not None:\n        psd = precomputed_psdband\n    else:\n        psd = FragmentAnalyzer.compute_psdband(\n            rec, f_s, welch_bin_t, notch_filter, bands, multitaper, precomputed_psd=precomputed_psd, **kwargs\n        )\n    return {k: log_transform(v) for k, v in psd.items()}\n</code></pre>"},{"location":"reference/core/windowed/#neurodent.core.analyze_frag.FragmentAnalyzer.compute_logpsdfrac","title":"<code>compute_logpsdfrac(rec, f_s, welch_bin_t=1, notch_filter=True, bands=constants.FREQ_BANDS, total_band=constants.FREQ_BAND_TOTAL, multitaper=False, precomputed_psdfrac=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Compute the log of the power spectral density of bands as a fraction of the log total power.</p> Source code in <code>src/neurodent/core/analyze_frag.py</code> <pre><code>@staticmethod\ndef compute_logpsdfrac(\n    rec: np.ndarray,\n    f_s: float,\n    welch_bin_t: float = 1,\n    notch_filter: bool = True,\n    bands: list[tuple[float, float]] = constants.FREQ_BANDS,\n    total_band: tuple[float, float] = constants.FREQ_BAND_TOTAL,\n    multitaper: bool = False,\n    precomputed_psdfrac: dict = None,\n    **kwargs,\n) -&gt; dict[str, np.ndarray]:\n    \"\"\"Compute the log of the power spectral density of bands as a fraction of the log total power.\"\"\"\n    FragmentAnalyzer._check_rec_np(rec)\n\n    if precomputed_psdfrac is not None:\n        psdfrac = precomputed_psdfrac\n    else:\n        psdfrac = FragmentAnalyzer.compute_psdfrac(\n            rec, f_s, welch_bin_t, notch_filter, bands, total_band, multitaper, **kwargs\n        )\n\n    return {k: log_transform(v) for k, v in psdfrac.items()}\n</code></pre>"},{"location":"reference/core/windowed/#neurodent.core.analyze_frag.FragmentAnalyzer.compute_logpsdtotal","title":"<code>compute_logpsdtotal(rec, f_s, welch_bin_t=1, notch_filter=True, band=constants.FREQ_BAND_TOTAL, multitaper=False, precomputed_psd=None, precomputed_psdtotal=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Compute the log of the total power spectral density of the signal.</p> Source code in <code>src/neurodent/core/analyze_frag.py</code> <pre><code>@staticmethod\ndef compute_logpsdtotal(\n    rec: np.ndarray,\n    f_s: float,\n    welch_bin_t: float = 1,\n    notch_filter: bool = True,\n    band: tuple[float, float] = constants.FREQ_BAND_TOTAL,\n    multitaper: bool = False,\n    precomputed_psd: tuple = None,\n    precomputed_psdtotal: np.ndarray = None,\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"Compute the log of the total power spectral density of the signal.\"\"\"\n    FragmentAnalyzer._check_rec_np(rec)\n\n    # Local import to avoid importing heavy dependencies from utils at module import time\n\n    if precomputed_psdtotal is not None:\n        return log_transform(precomputed_psdtotal)\n    else:\n        return log_transform(\n            FragmentAnalyzer.compute_psdtotal(\n                rec, f_s, welch_bin_t, notch_filter, band, multitaper, precomputed_psd=precomputed_psd, **kwargs\n            )\n        )\n</code></pre>"},{"location":"reference/core/windowed/#neurodent.core.analyze_frag.FragmentAnalyzer.compute_logrms","title":"<code>compute_logrms(rec, precomputed_rms=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Compute the log of the root mean square of the signal.</p> Source code in <code>src/neurodent/core/analyze_frag.py</code> <pre><code>@staticmethod\ndef compute_logrms(rec: np.ndarray, precomputed_rms: np.ndarray = None, **kwargs) -&gt; np.ndarray:\n    \"\"\"Compute the log of the root mean square of the signal.\"\"\"\n    FragmentAnalyzer._check_rec_np(rec)\n    # Local import to avoid importing heavy dependencies from utils at module import time\n\n    if precomputed_rms is not None:\n        return log_transform(precomputed_rms)\n    else:\n        return log_transform(FragmentAnalyzer.compute_rms(rec, **kwargs))\n</code></pre>"},{"location":"reference/core/windowed/#neurodent.core.analyze_frag.FragmentAnalyzer.compute_nspike","title":"<code>compute_nspike(rec, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Returns NaN array as placeholder. Compute and load in spikes with SpikeAnalysisResult</p> Source code in <code>src/neurodent/core/analyze_frag.py</code> <pre><code>@staticmethod\ndef compute_nspike(rec: np.ndarray, **kwargs):\n    \"\"\"Returns NaN array as placeholder. Compute and load in spikes with SpikeAnalysisResult\"\"\"\n    return np.full(rec.shape[1], np.nan)\n</code></pre>"},{"location":"reference/core/windowed/#neurodent.core.analyze_frag.FragmentAnalyzer.compute_pcorr","title":"<code>compute_pcorr(rec, f_s, lower_triag=False, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Compute the Pearson correlation coefficient of the signal.</p> Source code in <code>src/neurodent/core/analyze_frag.py</code> <pre><code>@staticmethod\ndef compute_pcorr(rec: np.ndarray, f_s: float, lower_triag: bool = False, **kwargs) -&gt; np.ndarray:\n    \"\"\"Compute the Pearson correlation coefficient of the signal.\"\"\"\n    FragmentAnalyzer._check_rec_np(rec)\n\n    sos = butter(2, constants.FREQ_BAND_TOTAL, btype=\"bandpass\", output=\"sos\", fs=f_s)\n    rec = sosfiltfilt(sos, rec, axis=0)\n\n    rec = rec.transpose()\n    result = pearsonr(rec[:, np.newaxis, :], rec, axis=-1)\n    if lower_triag:\n        return np.tril(result.correlation, k=-1)\n    else:\n        return result.correlation\n</code></pre>"},{"location":"reference/core/windowed/#neurodent.core.analyze_frag.FragmentAnalyzer.compute_psd","title":"<code>compute_psd(rec, f_s, welch_bin_t=1, notch_filter=True, multitaper=False, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Compute the power spectral density of the signal.</p> Source code in <code>src/neurodent/core/analyze_frag.py</code> <pre><code>@staticmethod\ndef compute_psd(\n    rec: np.ndarray,\n    f_s: float,\n    welch_bin_t: float = 1,\n    notch_filter: bool = True,\n    multitaper: bool = False,\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"Compute the power spectral density of the signal.\"\"\"\n    FragmentAnalyzer._check_rec_np(rec)\n\n    if notch_filter:\n        b, a = iirnotch(constants.LINE_FREQ, 30, fs=f_s)\n        rec = filtfilt(b, a, rec, axis=0)\n\n    if not multitaper:\n        f, psd = welch(rec, fs=f_s, nperseg=round(welch_bin_t * f_s), axis=0)\n    else:\n        if psd_array_multitaper is None:\n            raise ImportError(\"mne is required for multitaper PSD; install mne or set multitaper=False\")\n        # REVIEW psd calulation will give different bins if using multitaper\n        psd, f = psd_array_multitaper(\n            rec.transpose(),\n            f_s,\n            fmax=constants.FREQ_BAND_TOTAL[1],\n            adaptive=True,\n            normalization=\"full\",\n            low_bias=False,\n            verbose=0,\n        )\n        psd = psd.transpose()\n    return f, psd\n</code></pre>"},{"location":"reference/core/windowed/#neurodent.core.analyze_frag.FragmentAnalyzer.compute_psdband","title":"<code>compute_psdband(rec, f_s, welch_bin_t=1, notch_filter=True, bands=constants.FREQ_BANDS, multitaper=False, precomputed_psd=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Compute the power spectral density of the signal for each frequency band.</p> Source code in <code>src/neurodent/core/analyze_frag.py</code> <pre><code>@staticmethod\ndef compute_psdband(\n    rec: np.ndarray,\n    f_s: float,\n    welch_bin_t: float = 1,\n    notch_filter: bool = True,\n    bands: list[tuple[float, float]] = constants.FREQ_BANDS,\n    multitaper: bool = False,\n    precomputed_psd: tuple = None,\n    **kwargs,\n) -&gt; dict[str, np.ndarray]:\n    \"\"\"Compute the power spectral density of the signal for each frequency band.\"\"\"\n    FragmentAnalyzer._check_rec_np(rec)\n\n    if precomputed_psd is not None:\n        f, psd = precomputed_psd\n    else:\n        f, psd = FragmentAnalyzer.compute_psd(rec, f_s, welch_bin_t, notch_filter, multitaper, **kwargs)\n    deltaf = np.median(np.diff(f))\n\n    # Integrate each band separately using trapezoidal integration\n    result = {}\n\n    for band_name, (f_low, f_high) in bands.items():\n        # Always use inclusive boundaries for both ends\n        freq_mask = np.logical_and(f &gt;= f_low, f &lt;= f_high)\n        result[band_name] = trapezoid(psd[freq_mask, :], dx=deltaf, axis=0)\n\n    return result\n</code></pre>"},{"location":"reference/core/windowed/#neurodent.core.analyze_frag.FragmentAnalyzer.compute_psdfrac","title":"<code>compute_psdfrac(rec, f_s, welch_bin_t=1, notch_filter=True, bands=constants.FREQ_BANDS, total_band=constants.FREQ_BAND_TOTAL, multitaper=False, precomputed_psdband=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Compute the power spectral density of bands as a fraction of the total power.</p> Source code in <code>src/neurodent/core/analyze_frag.py</code> <pre><code>@staticmethod\ndef compute_psdfrac(\n    rec: np.ndarray,\n    f_s: float,\n    welch_bin_t: float = 1,\n    notch_filter: bool = True,\n    bands: list[tuple[float, float]] = constants.FREQ_BANDS,\n    total_band: tuple[float, float] = constants.FREQ_BAND_TOTAL,\n    multitaper: bool = False,\n    precomputed_psdband: dict = None,\n    **kwargs,\n) -&gt; dict[str, np.ndarray]:\n    \"\"\"Compute the power spectral density of bands as a fraction of the total power.\"\"\"\n    FragmentAnalyzer._check_rec_np(rec)\n\n    if precomputed_psdband is not None:\n        psdband = precomputed_psdband\n    else:\n        psdband = FragmentAnalyzer.compute_psdband(rec, f_s, welch_bin_t, notch_filter, bands, multitaper, **kwargs)\n    psdtotal = sum(psdband.values())\n\n    return {k: v / psdtotal for k, v in psdband.items()}\n</code></pre>"},{"location":"reference/core/windowed/#neurodent.core.analyze_frag.FragmentAnalyzer.compute_psdslope","title":"<code>compute_psdslope(rec, f_s, welch_bin_t=1, notch_filter=True, band=constants.FREQ_BAND_TOTAL, multitaper=False, precomputed_psd=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Compute the slope of the power spectral density of the signal on a log-log scale.</p> Source code in <code>src/neurodent/core/analyze_frag.py</code> <pre><code>@staticmethod\ndef compute_psdslope(\n    rec: np.ndarray,\n    f_s: float,\n    welch_bin_t: float = 1,\n    notch_filter: bool = True,\n    band: tuple[float, float] = constants.FREQ_BAND_TOTAL,\n    multitaper: bool = False,\n    precomputed_psd: tuple = None,\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"Compute the slope of the power spectral density of the signal on a log-log scale.\"\"\"\n    FragmentAnalyzer._check_rec_np(rec)\n\n    if precomputed_psd is not None:\n        f, psd = precomputed_psd\n    else:\n        f, psd = FragmentAnalyzer.compute_psd(rec, f_s, welch_bin_t, notch_filter, multitaper, **kwargs)\n\n    freqs = f[np.logical_and(f &gt;= band[0], f &lt;= band[1])]\n    psd_band = psd[np.logical_and(f &gt;= band[0], f &lt;= band[1]), :]\n    logpsd = np.log10(psd_band)\n    logf = np.log10(freqs)\n\n    # Fit a line to the log-transformed data\n    out = []\n    for i in range(psd_band.shape[1]):\n        result = linregress(logf, logpsd[:, i])\n        out.append([result.slope, result.intercept])\n    return np.array(out)\n</code></pre>"},{"location":"reference/core/windowed/#neurodent.core.analyze_frag.FragmentAnalyzer.compute_psdtotal","title":"<code>compute_psdtotal(rec, f_s, welch_bin_t=1, notch_filter=True, band=constants.FREQ_BAND_TOTAL, multitaper=False, precomputed_psd=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Compute the total power spectral density of the signal.</p> Source code in <code>src/neurodent/core/analyze_frag.py</code> <pre><code>@staticmethod\ndef compute_psdtotal(\n    rec: np.ndarray,\n    f_s: float,\n    welch_bin_t: float = 1,\n    notch_filter: bool = True,\n    band: tuple[float, float] = constants.FREQ_BAND_TOTAL,\n    multitaper: bool = False,\n    precomputed_psd: tuple = None,\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"Compute the total power spectral density of the signal.\"\"\"\n    FragmentAnalyzer._check_rec_np(rec)\n\n    if precomputed_psd is not None:\n        f, psd = precomputed_psd\n    else:\n        f, psd = FragmentAnalyzer.compute_psd(rec, f_s, welch_bin_t, notch_filter, multitaper, **kwargs)\n    deltaf = np.median(np.diff(f))\n\n    # Use inclusive bounds for total power calculation\n    freq_mask = np.logical_and(f &gt;= band[0], f &lt;= band[1])\n    return trapezoid(psd[freq_mask, :], dx=deltaf, axis=0)\n</code></pre>"},{"location":"reference/core/windowed/#neurodent.core.analyze_frag.FragmentAnalyzer.compute_rms","title":"<code>compute_rms(rec, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Compute the root mean square of the signal.</p> Source code in <code>src/neurodent/core/analyze_frag.py</code> <pre><code>@staticmethod\ndef compute_rms(rec: np.ndarray, **kwargs) -&gt; np.ndarray:\n    \"\"\"Compute the root mean square of the signal.\"\"\"\n    FragmentAnalyzer._check_rec_np(rec)\n    out = np.sqrt((rec**2).sum(axis=0) / rec.shape[0])\n    # del rec\n    return out\n</code></pre>"},{"location":"reference/core/windowed/#neurodent.core.analyze_frag.FragmentAnalyzer.compute_zcohere","title":"<code>compute_zcohere(rec, f_s, z_epsilon=1e-06, precomputed_cohere=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Compute the Fisher z-transformed coherence of the signal.</p> <p>Parameters:</p> Name Type Description Default <code>rec</code> <code>ndarray</code> <p>Input signal array</p> required <code>f_s</code> <code>float</code> <p>Sampling frequency</p> required <code>z_epsilon</code> <code>float</code> <p>Small value to prevent arctanh(1) = inf. Values are clipped to [-1+z_epsilon, 1-z_epsilon]</p> <code>1e-06</code> <code>**kwargs</code> <p>Additional arguments passed to compute_cohere</p> <code>{}</code> Source code in <code>src/neurodent/core/analyze_frag.py</code> <pre><code>@staticmethod\ndef compute_zcohere(\n    rec: np.ndarray, f_s: float, z_epsilon: float = 1e-6, precomputed_cohere=None, **kwargs\n) -&gt; dict[str, np.ndarray]:\n    \"\"\"Compute the Fisher z-transformed coherence of the signal.\n\n    Args:\n        rec: Input signal array\n        f_s: Sampling frequency\n        z_epsilon: Small value to prevent arctanh(1) = inf. Values are clipped to [-1+z_epsilon, 1-z_epsilon]\n        **kwargs: Additional arguments passed to compute_cohere\n    \"\"\"\n    FragmentAnalyzer._check_rec_np(rec)\n\n    if precomputed_cohere is not None:\n        cohere = precomputed_cohere.copy()\n    else:\n        cohere = FragmentAnalyzer.compute_cohere(rec, f_s, **kwargs)\n    clip_max = 1.0 - z_epsilon\n    clip_min = -1.0 + z_epsilon\n    return {k: np.arctanh(np.clip(v, clip_min, clip_max)) for k, v in cohere.items()}\n</code></pre>"},{"location":"reference/core/windowed/#neurodent.core.analyze_frag.FragmentAnalyzer.compute_zimcoh","title":"<code>compute_zimcoh(rec, f_s, z_epsilon=1e-06, precomputed_imcoh=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Compute the Fisher z-transformed imaginary coherence of the signal.</p> Source code in <code>src/neurodent/core/analyze_frag.py</code> <pre><code>@staticmethod\ndef compute_zimcoh(\n    rec: np.ndarray, f_s: float, z_epsilon: float = 1e-6, precomputed_imcoh: dict = None, **kwargs\n) -&gt; dict[str, np.ndarray]:\n    \"\"\"Compute the Fisher z-transformed imaginary coherence of the signal.\"\"\"\n    FragmentAnalyzer._check_rec_np(rec)\n    if precomputed_imcoh is not None:\n        imcoh = precomputed_imcoh\n    else:\n        imcoh = FragmentAnalyzer.compute_imcoh(rec, f_s, **kwargs)\n    clip_max = 1.0 - z_epsilon\n    clip_min = -1.0 + z_epsilon\n    return {k: np.arctanh(np.clip(v, clip_min, clip_max)) for k, v in imcoh.items()}\n</code></pre>"},{"location":"reference/core/windowed/#neurodent.core.analyze_frag.FragmentAnalyzer.compute_zpcorr","title":"<code>compute_zpcorr(rec, f_s, z_epsilon=1e-06, precomputed_pcorr=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Compute the Fisher z-transformed Pearson correlation coefficient of the signal.</p> <p>Parameters:</p> Name Type Description Default <code>rec</code> <code>ndarray</code> <p>Input signal array</p> required <code>f_s</code> <code>float</code> <p>Sampling frequency</p> required <code>z_epsilon</code> <code>float</code> <p>Small value to prevent arctanh(1) = inf. Values are clipped to [-1+z_epsilon, 1-z_epsilon]</p> <code>1e-06</code> <code>**kwargs</code> <p>Additional arguments passed to compute_pcorr</p> <code>{}</code> Source code in <code>src/neurodent/core/analyze_frag.py</code> <pre><code>@staticmethod\ndef compute_zpcorr(\n    rec: np.ndarray, f_s: float, z_epsilon: float = 1e-6, precomputed_pcorr: np.ndarray = None, **kwargs\n) -&gt; np.ndarray:\n    \"\"\"Compute the Fisher z-transformed Pearson correlation coefficient of the signal.\n\n    Args:\n        rec: Input signal array\n        f_s: Sampling frequency\n        z_epsilon: Small value to prevent arctanh(1) = inf. Values are clipped to [-1+z_epsilon, 1-z_epsilon]\n        **kwargs: Additional arguments passed to compute_pcorr\n    \"\"\"\n    FragmentAnalyzer._check_rec_np(rec)\n\n    if precomputed_pcorr is not None:\n        pcorr = precomputed_pcorr.copy()\n    else:\n        # Get full correlation matrix for z-transform\n        pcorr = FragmentAnalyzer.compute_pcorr(rec, f_s, lower_triag=False, **kwargs)\n    clip_max = 1.0 - z_epsilon\n    clip_min = -1.0 + z_epsilon\n    return np.arctanh(np.clip(pcorr, clip_min, clip_max))\n</code></pre>"},{"location":"reference/core/windowed/#neurodent.core.analyze_frag.FragmentAnalyzer.process_fragment_with_dependencies","title":"<code>process_fragment_with_dependencies(fragment_data, f_s, features, kwargs)</code>  <code>staticmethod</code>","text":"<p>Process a single fragment with efficient dependency resolution.</p> <p>This is the enhanced replacement for _process_fragment_features_dask that automatically resolves feature dependencies and reuses intermediate calculations to avoid redundant computations (e.g., computing PSD once for multiple dependent features).</p> <p>Parameters:</p> Name Type Description Default <code>fragment_data</code> <code>ndarray</code> <p>Single fragment data with shape (n_samples, n_channels)</p> required <code>f_s</code> <code>int</code> <p>Sampling frequency</p> required <code>features</code> <code>List[str]</code> <p>List of features to compute</p> required <code>kwargs</code> <code>Dict[str, Any]</code> <p>Additional parameters for feature computation</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary of computed features for this fragment</p> Source code in <code>src/neurodent/core/analyze_frag.py</code> <pre><code>@staticmethod\ndef process_fragment_with_dependencies(\n    fragment_data: np.ndarray, f_s: int, features: List[str], kwargs: Dict[str, Any]\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Process a single fragment with efficient dependency resolution.\n\n    This is the enhanced replacement for _process_fragment_features_dask that\n    automatically resolves feature dependencies and reuses intermediate calculations\n    to avoid redundant computations (e.g., computing PSD once for multiple dependent features).\n\n    Args:\n        fragment_data: Single fragment data with shape (n_samples, n_channels)\n        f_s: Sampling frequency\n        features: List of features to compute\n        kwargs: Additional parameters for feature computation\n\n    Returns:\n        Dictionary of computed features for this fragment\n    \"\"\"\n    computed_cache = {}\n    results = {}\n\n    # Compute all requested features using dependency resolution\n    for feature in features:\n        if feature not in results:\n            results[feature] = FragmentAnalyzer._resolve_feature_dependencies(\n                feature, fragment_data, f_s, kwargs, computed_cache\n            )\n\n    return results\n</code></pre>"},{"location":"reference/visualization/animal/","title":"Animal Level","text":""},{"location":"reference/visualization/animal/#neurodent.visualization.plotting.animal.AnimalPlotter","title":"<code>AnimalPlotter</code>","text":"<p>               Bases: <code>AnimalFeatureParser</code></p> Source code in <code>src/neurodent/visualization/plotting/animal.py</code> <pre><code>class AnimalPlotter(viz.AnimalFeatureParser):\n    def __init__(self, war: viz.WindowAnalysisResult, save_fig: bool = False, save_path: Path = None) -&gt; None:\n        self.window_result = war\n        self.genotype = war.genotype\n        self.channel_names = war.channel_names\n        self.n_channels = len(self.channel_names)\n        self.__assume_from_number = war.assume_from_number\n        self.channel_abbrevs = war.channel_abbrevs\n        self.save_fig = save_fig\n        self.save_path: Path = save_path\n\n    def _abbreviate_channel(self, ch_name: str):\n        for k, v in self.CHNAME_TO_ABBREV:\n            if k in ch_name:\n                return v\n        return ch_name\n\n    def plot_coherecorr_matrix(self, groupby=\"animalday\", bands=None, figsize=None, cmap=\"viridis\", **kwargs):\n        avg_result = self.__get_groupavg_coherecorr(groupby, **kwargs)\n\n        if bands is None:\n            bands = constants.BAND_NAMES + [\"pcorr\"]\n        elif isinstance(bands, str):\n            bands = [bands]\n        n_row = avg_result.index.size\n        # rowcount = 0\n        fig, ax = plt.subplots(n_row, len(bands), squeeze=False, figsize=figsize, **kwargs)\n\n        normlist = [\n            matplotlib.colors.Normalize(vmin=0, vmax=np.max(np.concatenate(avg_result[band].values))) for band in bands\n        ]\n        for i, (_, row) in enumerate(avg_result.iterrows()):\n            self._plot_coherecorr_matrixgroup(\n                row, bands, ax[i, :], show_bandname=i == 0, norm_list=normlist, cmap=cmap, **kwargs\n            )\n            # rowcount += 1\n        self._handle_figure(fig, title=\"coherecorr_matrix\")\n\n    def plot_coherecorr_diff(self, groupby=\"isday\", bands=None, figsize=None, cmap=\"bwr\", **kwargs):\n        avg_result = self.__get_groupavg_coherecorr(groupby, **kwargs)\n        avg_result = avg_result.drop(\"cohere\", axis=1, errors=\"ignore\")\n        if len(avg_result.index) != 2:\n            raise ValueError(\n                f\"Difference can only be calculated between 2 rows. {groupby} resulted in {len(avg_result.index)} rows\"\n            )\n\n        if bands is None:\n            bands = constants.BAND_NAMES + [\"pcorr\"]\n        elif isinstance(bands, str):\n            bands = [bands]\n\n        diff_result = avg_result.iloc[1] - avg_result.iloc[0]\n        diff_result.name = f\"{avg_result.iloc[1].name} - {avg_result.iloc[0].name}\"\n\n        fig, ax = plt.subplots(1, len(bands), squeeze=False, figsize=figsize, **kwargs)\n\n        self._plot_coherecorr_matrixgroup(\n            diff_result, bands, ax[0, :], show_bandname=True, center_cmap=True, cmap=cmap, **kwargs\n        )\n        self._handle_figure(fig, title=\"coherecorr_diff\")\n\n    def _plot_coherecorr_matrixgroup(\n        self,\n        group: pd.Series,\n        bands: list[str],\n        ax: list[matplotlib.axes.Axes],\n        show_bandname,\n        center_cmap=False,\n        norm_list=None,\n        show_channelname=True,\n        **kwargs,\n    ):\n        rowname = group.name\n        for i, band in enumerate(bands):\n            if norm_list is None:\n                if center_cmap:\n                    divnorm = matplotlib.colors.CenteredNorm()\n                else:\n                    divnorm = None\n                ax[i].imshow(group[band], norm=divnorm, **kwargs)\n            else:\n                ax[i].imshow(group[band], norm=norm_list[i], **kwargs)\n\n            if show_bandname:\n                ax[i].set_xlabel(band, fontsize=\"x-large\")\n                ax[i].xaxis.set_label_position(\"top\")\n\n            if show_channelname:\n                ax[i].set_xticks(range(self.n_channels), self.channel_abbrevs, rotation=\"vertical\")\n                ax[i].set_yticks(range(self.n_channels), self.channel_abbrevs)\n            else:\n                ax[i].set_xticks(range(self.n_channels), \" \")\n                ax[i].set_yticks(range(self.n_channels), \" \")\n\n        ax[0].set_ylabel(rowname, rotation=\"horizontal\", ha=\"right\")\n\n    def __get_groupavg_coherecorr(self, groupby=\"animalday\", **kwargs):\n        avg_result = self.window_result.get_groupavg_result(constants.MATRIX_FEATURES.copy(), groupby=groupby)\n        avg_coheresplit = pd.json_normalize(avg_result[\"cohere\"]).set_index(\n            avg_result.index\n        )  # Split apart the cohere dictionaries\n        return avg_coheresplit.join(avg_result)\n\n    def plot_linear_temporal(\n        self,\n        multiindex=[\"animalday\", \"animal\", \"genotype\"],\n        features: list[str] = None,\n        channels: list[int] = None,\n        figsize=None,\n        score_type=\"z\",\n        show_endfile=False,\n        **kwargs,\n    ):\n        # REVIEW this breaks for plotting psdslope, which contains both slope and intercept values.\n        # Perhaps split apart psdslope more cleanly into psdslope + psdintercept when computing WAR\n        if features is None:\n            features = constants.LINEAR_FEATURES.copy() + constants.BAND_FEATURES.copy()\n            features = [x for x in features if x and not x.startswith(\"log\")]\n        if channels is None:\n            channels = np.arange(self.n_channels)\n\n        # df_featgroups = self.window_result.get_grouped(features, groupby=groupby)\n        df_rowgroup = self.window_result.get_grouprows_result(features, multiindex=multiindex)\n        for i, df_row in df_rowgroup.groupby(level=0):\n            fig, ax = plt.subplots(\n                len(features),\n                1,\n                figsize=figsize,\n                sharex=True,\n                gridspec_kw={\"height_ratios\": [constants.FEATURE_PLOT_HEIGHT_RATIOS[x] for x in features]},\n                squeeze=False,\n            )\n            plt.subplots_adjust(hspace=0)\n\n            for j, feat in enumerate(features):\n                self._plot_linear_temporalgroup(\n                    group=df_row,\n                    feature=feat,\n                    ax=ax[j, 0],\n                    score_type=score_type,\n                    channels=channels,\n                    show_endfile=show_endfile,\n                    **kwargs,\n                )\n            ax[-1, 0].set_xlabel(\"Time (s)\")\n            fig.suptitle(i)\n            self._handle_figure(fig, title=f\"linear_temporal_{i}\")\n\n    def _plot_linear_temporalgroup(\n        self,\n        group: pd.DataFrame,\n        feature: str,\n        ax: matplotlib.axes.Axes,\n        channels: list[int] = None,\n        score_type: str = \"z\",\n        duration_name=\"duration\",\n        channel_y_offset=10,\n        feature_y_offset=10,\n        endfile_name=\"endfile\",\n        show_endfile=False,\n        show_channelname=True,\n        **kwargs,\n    ):\n        data_Z = self.__get_linear_feature(group=group, feature=feature, score_type=score_type)\n\n        data_t = group[duration_name]\n        data_T = np.cumsum(data_t)\n\n        # Handle both 2D and 3D feature arrays\n        if data_Z.ndim == 2:\n            # 2D array (time, channels) - expand to 3D for consistent handling\n            data_Z = np.expand_dims(data_Z, axis=-1)\n        elif data_Z.ndim != 3:\n            raise ValueError(f\"Expected 2D or 3D feature array, got {data_Z.ndim}D for feature '{feature}'\")\n\n        if channels is None:\n            channels = np.arange(data_Z.shape[1])\n        data_Z = data_Z[:, channels, :]\n\n        n_chan = data_Z.shape[1]\n        n_feat = data_Z.shape[2]\n        chan_offset = np.linspace(0, channel_y_offset * n_chan, n_chan, endpoint=False).reshape((1, -1, 1))\n        feat_offset = np.linspace(0, feature_y_offset * n_chan * n_feat, n_feat, endpoint=False).reshape((1, 1, -1))\n        data_Z += chan_offset\n        data_Z += feat_offset\n        ytick_offset = feat_offset.squeeze() + np.mean(chan_offset.flatten())\n\n        for i in range(n_feat):\n            ax.plot(data_T, data_Z[:, :, i], c=f\"C{i}\", **kwargs)\n        match feature:  # NOTE refactor this to use constants\n            case \"rms\" | \"ampvar\" | \"psdtotal\" | \"nspike\" | \"logrms\" | \"logampvar\" | \"logpsdtotal\" | \"lognspike\":\n                ax.set_yticks([ytick_offset], [feature])\n            case \"psdslope\":\n                ax.set_yticks(ytick_offset, [\"psdslope\", \"psdintercept\"])\n            case \"psdband\" | \"psdfrac\" | \"logpsdband\" | \"logpsdfrac\":\n                ax.set_yticks(ytick_offset, constants.BAND_NAMES)\n            case _:\n                raise ValueError(f\"Invalid feature {feature}\")\n\n        if show_endfile:\n            self._plot_filediv_lines(group=group, ax=ax, duration_name=duration_name, endfile_name=endfile_name)\n\n    def __get_linear_feature(self, group: pd.DataFrame, feature: str, score_type=\"z\", triag=True):\n        match feature:  # NOTE refactor this to use constants\n            case \"rms\" | \"ampvar\" | \"psdtotal\" | \"nspike\" | \"logrms\" | \"logampvar\" | \"logpsdtotal\" | \"lognspike\":\n                data_X = np.array(group[feature].to_list())\n                data_X = np.expand_dims(data_X, axis=-1)\n            case \"psdband\" | \"psdfrac\" | \"logpsdband\" | \"logpsdfrac\":\n                data_X = np.array([list(d.values()) for d in group[feature]])\n                data_X = np.stack(data_X, axis=-1)\n                data_X = np.transpose(data_X)\n            case \"psdslope\":\n                data_X = np.array(group[feature].to_list())\n                data_X = data_X[:, :, 0]  # Take first component (slope)\n                # data_X = np.expand_dims(data_X, axis=-1)  # Keep 3D format for consistency\n            case \"cohere\" | \"zcohere\" | \"imcoh\" | \"zimcoh\":\n                data_X = np.array([list(d.values()) for d in group[feature]])\n                data_X = np.stack(data_X, axis=-1)\n                if triag:\n                    tril = np.tril_indices(data_X.shape[1], k=-1)\n                    data_X = data_X[:, tril[0], tril[1], :]\n                data_X = data_X.reshape(data_X.shape[0], -1, data_X.shape[-1])\n                data_X = np.transpose(data_X)\n            case \"pcorr\" | \"zpcorr\":\n                data_X = np.stack(group[feature], axis=-1)\n                if triag:\n                    tril = np.tril_indices(data_X.shape[1], k=-1)\n                    data_X = data_X[tril[0], tril[1], :]\n                data_X = data_X.reshape(-1, data_X.shape[-1])\n                data_X = data_X.transpose()\n                data_X = np.expand_dims(data_X, axis=-1)\n            case _:\n                raise ValueError(f\"Invalid feature {feature}\")\n\n        return self._calculate_standard_data(data_X, mode=score_type, axis=0)\n\n    def _plot_filediv_lines(self, group: pd.DataFrame, ax: matplotlib.axes.Axes, duration_name, endfile_name):\n        filedivs = self.__get_filediv_times(group, duration_name, endfile_name)\n        for xpos in filedivs:\n            ax.axvline(xpos, ls=\"--\", c=\"black\", lw=1)\n\n    def __get_filediv_times(self, group, duration_name, endfile_name):\n        cumulative = group[duration_name].cumsum().shift(fill_value=0)\n        # display( group[[endfile_name]].dropna().head())\n        # display(cumulative.head())\n        filedivs = group[endfile_name].dropna() + cumulative[group[endfile_name].notna()]\n        return filedivs.tolist()\n\n    def _calculate_standard_data(self, X, mode=\"z\", axis=0):\n        match mode:\n            case \"z\":\n                data_Z = zscore(X, axis=axis, nan_policy=\"omit\")\n            case \"zall\":\n                data_Z = zscore(X, axis=None, nan_policy=\"omit\")\n            case \"gz\":\n                data_Z = gzscore(X, axis=axis, nan_policy=\"omit\")\n            case \"modz\":\n                data_Z = self.__calculate_modified_zscore(X, axis=axis)\n            case \"none\" | None:\n                data_Z = X\n            case \"center\":\n                data_Z = X - np.nanmean(X, axis=axis, keepdims=True)\n            case _:\n                raise ValueError(f\"Invalid mode {mode}\")\n        return data_Z\n\n    def __calculate_modified_zscore(self, X, axis=0):\n        X_mid = np.nanmedian(X, axis=axis)\n        X_absdev = np.nanmedian(np.abs(X - X_mid), axis=axis)\n        return 0.6745 * (X - X_mid) / X_absdev\n\n    def plot_coherecorr_spectral(\n        self,\n        multiindex=[\"animalday\", \"animal\", \"genotype\"],\n        features: list[str] = None,\n        figsize=None,\n        score_type=\"z\",\n        cmap=\"bwr\",\n        triag=True,\n        show_endfile=False,\n        duration_name=\"duration\",\n        endfile_name=\"endfile\",\n        **kwargs,\n    ):\n        if features is None:\n            features = [\"zcohere\", \"zpcorr\"]\n        # Use consolidated height ratios from constants (matrix features for spectral heatmaps)\n\n        df_rowgroup = self.window_result.get_grouprows_result(features, multiindex=multiindex)\n        for feature in features:\n            if feature not in df_rowgroup.columns:\n                warnings.warn(f\"Feature {feature} not found in dataframe\")\n                features.remove(feature)\n\n        for i, df_row in df_rowgroup.groupby(level=0):\n            fig, ax = plt.subplots(\n                len(features),\n                1,\n                figsize=figsize,\n                sharex=True,\n                gridspec_kw={\"height_ratios\": [constants.FEATURE_PLOT_HEIGHT_RATIOS[x] for x in features]},\n                squeeze=False,\n            )\n            plt.subplots_adjust(hspace=0)\n            for j, feat in enumerate(features):\n                self._plot_coherecorr_spectralgroup(\n                    group=df_row,\n                    feature=feat,\n                    ax=ax[j, 0],\n                    score_type=score_type,\n                    triag=triag,\n                    show_endfile=show_endfile,\n                    duration_name=duration_name,\n                    endfile_name=endfile_name,\n                    **kwargs,\n                )\n            ax[-1, 0].set_xlabel(\"Time (s)\")\n            fig.suptitle(i)\n            self._handle_figure(fig, title=f\"coherecorr_spectral_{i}\")\n\n    def _plot_coherecorr_spectralgroup(\n        self,\n        group: pd.DataFrame,\n        feature: str,\n        ax: matplotlib.axes.Axes,\n        center_cmap=True,\n        score_type=\"z\",\n        norm_list=None,\n        show_featurename=True,\n        show_endfile=False,\n        duration_name=\"duration\",\n        endfile_name=\"endfile\",\n        cmap=\"bwr\",\n        triag=True,\n        **kwargs,\n    ):\n        data_Z = self.__get_linear_feature(group=group, feature=feature, score_type=score_type)\n        std_dev = np.nanstd(data_Z.flatten())\n\n        # data_flat = data_Z.reshape(data_Z.shape[0], -1).transpose()\n\n        if center_cmap:\n            norm = matplotlib.colors.CenteredNorm(vcenter=0, halfrange=std_dev * 2)\n        else:\n            norm = None\n\n        n_ch = data_Z.shape[1]\n        n_bands = len(constants.BAND_NAMES)\n\n        for i in range(data_Z.shape[-1]):\n            extent = (0, data_Z.shape[0] * group[\"duration\"].median(), i * n_ch, (i + 1) * n_ch)\n            ax.imshow(\n                data_Z[:, :, i].transpose(), interpolation=\"none\", aspect=\"auto\", norm=norm, cmap=cmap, extent=extent\n            )\n\n        if show_featurename:\n            if feature in [\"cohere\", \"zcohere\", \"imcoh\", \"zimcoh\"]:\n                ticks = n_ch * np.linspace(1 / 2, n_bands + 1 / 2, n_bands, endpoint=False)\n                ax.set_yticks(ticks=ticks, labels=constants.BAND_NAMES)\n                for ypos in np.linspace(0, n_bands * n_ch, n_bands, endpoint=False):\n                    ax.axhline(ypos, lw=1, ls=\"--\", color=\"black\")\n            elif feature in [\"pcorr\", \"zpcorr\"]:\n                ax.set_yticks(ticks=[1 / 2 * n_ch], labels=[feature])\n            else:\n                raise ValueError(f\"Unknown feature name {feature}\")\n\n        if show_endfile:\n            self._plot_filediv_lines(group=group, ax=ax, duration_name=duration_name, endfile_name=endfile_name)\n\n    def plot_psd_histogram(\n        self,\n        groupby=\"animalday\",\n        figsize=None,\n        avg_channels=False,\n        plot_type=\"loglog\",\n        plot_slope=True,\n        xlim=None,\n        **kwargs,\n    ):\n        avg_result = self.window_result.get_groupavg_result([\"psd\"], groupby=groupby)\n\n        n_col = avg_result.index.size\n        fig, ax = plt.subplots(1, n_col, squeeze=False, figsize=figsize, sharex=True, sharey=True, **kwargs)\n        plt.subplots_adjust(wspace=0)\n        for i, (idx, row) in enumerate(avg_result.iterrows()):\n            freqs = row[\"psd\"][0]\n            psd = row[\"psd\"][1]\n            if avg_channels:\n                psd = np.nanmean(psd, axis=-1, keepdims=True)\n                label = \"Average\"\n            else:\n                label = self.channel_abbrevs\n            match plot_type:\n                case \"loglog\":\n                    ax[0, i].loglog(freqs, psd, label=label)\n                case \"semilogy\":\n                    ax[0, i].semilogy(freqs, psd, label=label)\n                case \"semilogx\":\n                    ax[0, i].semilogy(freqs, psd, label=label)\n                case \"linear\":\n                    ax[0, i].plot(freqs, psd, label=label)\n                case _:\n                    raise ValueError(f\"Invalid plot type {plot_type}\")\n\n            frange = np.logical_and(freqs &gt;= constants.FREQ_BAND_TOTAL[0], freqs &lt;= constants.FREQ_BAND_TOTAL[1])\n            logf = np.log10(freqs[frange])\n            logpsd = np.log10(psd[frange, :])\n\n            linfit = np.zeros((psd.shape[1], 2))\n            for k in range(psd.shape[1]):\n                result = linregress(logf, logpsd[:, k], \"less\")\n                linfit[k, :] = [result.slope, result.intercept]\n\n            for j, (m, b) in enumerate(linfit.tolist()):\n                ax[0, i].plot(freqs, 10 ** (b + m * np.log10(freqs)), c=f\"C{j}\", alpha=0.75)\n\n            ax[0, i].set_title(idx)\n            ax[0, i].set_xlabel(\"Frequency (Hz)\")\n        ax[0, 0].set_ylabel(\"PSD (uV^2/Hz)\")\n        ax[0, -1].legend(loc=\"center left\", bbox_to_anchor=(1.05, 0.5))\n        ax[0, -1].set_xlim(xlim)\n        self._handle_figure(fig, title=\"psd_histogram\")\n\n    def plot_psd_spectrogram(\n        self,\n        multiindex=[\"animalday\", \"animal\", \"genotype\"],\n        freq_range=(1, 50),\n        center_stat=\"mean\",\n        mode=\"z\",\n        figsize=None,\n        cmap=\"magma\",\n        **kwargs,\n    ):\n        df_rowgroup = self.window_result.get_grouprows_result([\"psd\"], multiindex=multiindex)\n        for i, df_row in df_rowgroup.groupby(level=0):\n            freqs = df_row.iloc[0][\"psd\"][0]\n            psd = np.array([x[1] for x in df_row[\"psd\"].tolist()])\n            match center_stat:\n                case \"mean\":\n                    psd = np.nanmean(psd, axis=-1).transpose()\n                case \"median\":\n                    psd = np.nanmedian(psd, axis=-1).transpose()\n                case _:\n                    raise ValueError(f\"Invalid statistic {center_stat}. Pick mean or median\")\n            psd = np.log10(psd)\n            psd = self._calculate_standard_data(psd, mode=mode, axis=-1)\n            freq_mask = np.logical_and((freq_range[0] &lt;= freqs), (freqs &lt;= freq_range[1]))\n            freqs = freqs[freq_mask]\n            psd = psd[freq_mask, :]\n\n            extent = (0, psd.shape[1] * df_row[\"duration\"].median(), np.min(freqs), np.max(freqs))\n            # print(psd.nanmin(), psd.nanmax())\n            norm = matplotlib.colors.Normalize()\n            # norm = matplotlib.colors.LogNorm()\n            # norm = matplotlib.colors.CenteredNorm()\n\n            fig, ax = plt.subplots(1, 1, figsize=figsize)\n            # ax.pcolormesh(psd, )\n            axim = ax.imshow(\n                np.flip(psd, axis=0), interpolation=\"none\", aspect=\"auto\", norm=norm, cmap=cmap, extent=extent\n            )\n            cbar = fig.colorbar(axim, ax=ax)\n            cbar.set_label(f\"log(PSD) {mode}\")\n\n            ax.set_xlabel(\"Time (s)\")\n            ax.set_ylabel(\"Frequency (Hz)\")\n            ax.set_title(i)\n            self._handle_figure(fig, title=f\"psd_spectrogram_{i}\")\n\n    def plot_temporal_heatmap(\n        self,\n        features: list[str] | str = None,\n        figsize=None,\n        cmap=\"viridis\",\n        score_type=None,\n        norm=None,\n        **kwargs,\n    ):\n        \"\"\"\n        Create temporal heatmap showing feature patterns over time.\n\n        Creates a heatmap where:\n        - X-axis: Time of day (timestamp mod 24h)\n        - Y-axis: Days\n        - Color: Feature values (flattened across channels)\n\n        Parameters\n        ----------\n        features : list[str], optional\n            List of features to plot. If None, uses non-band linear features.\n        figsize : tuple, optional\n            Figure size (width, height)\n        cmap : str, optional\n            Colormap for the heatmap\n        score_type : str, optional\n            Standardization method for feature values\n        norm : matplotlib.colors.Normalize, optional\n            Normalization object for the colormap. If None, uses default normalization.\n            Common options:\n            - matplotlib.colors.Normalize(vmin=0, vmax=1)  # Fixed range\n            - matplotlib.colors.CenteredNorm(vcenter=0)  # Auto-detect range around 0\n            - matplotlib.colors.LogNorm()  # Logarithmic scale\n        **kwargs\n            Additional arguments passed to matplotlib\n        \"\"\"\n        if features is None:\n            # Use non-band linear features for temporal analysis\n            features = [f for f in constants.LINEAR_FEATURES]\n        if isinstance(features, str):\n            features = [features]\n\n        # Get data grouped by animalday\n        df_rowgroup = self.window_result.get_grouprows_result(\n            features, multiindex=[\"animal\", \"genotype\"], include=[\"duration\", \"endfile\", \"timestamp\", \"animalday\"]\n        )\n\n        for feature in features:\n            if feature not in df_rowgroup.columns:\n                warnings.warn(f\"Feature {feature} not found in dataframe\")\n                features.remove(feature)\n\n        if not features:\n            raise ValueError(\"No valid features found for temporal heatmap\")\n\n        # Process each feature\n        for feature in features:\n            self._plot_temporal_heatmap_feature(\n                df_rowgroup=df_rowgroup,\n                feature=feature,\n                figsize=figsize,\n                cmap=cmap,\n                score_type=score_type,\n                norm=norm,\n                **kwargs,\n            )\n\n    def _plot_temporal_heatmap_feature(\n        self,\n        df_rowgroup: pd.DataFrame,\n        feature: str,\n        n_bins=24 * 60,\n        figsize=None,\n        cmap=\"viridis\",\n        score_type=\"z\",\n        norm=None,\n        **kwargs,\n    ):\n        \"\"\"\n        Create temporal heatmap for a single feature.\n        \"\"\"\n        # Group by animalday to process each recording session\n        for animalday, df_day in df_rowgroup.groupby(level=0):\n            # Extract timestamps and convert to time of day (modulo 24h)\n            timestamps = df_day[\"timestamp\"]\n            time_of_day = timestamps.dt.hour + timestamps.dt.minute / 60.0 + timestamps.dt.second / 3600.0\n\n            # Get feature data and flatten across channels\n            feature_data = self.__get_linear_feature(group=df_day, feature=feature, score_type=score_type)\n\n            # Flatten across channels (take mean across channels)\n            if feature_data.ndim &gt; 2:\n                feature_data = np.nanmean(feature_data, axis=1).squeeze()\n            else:\n                feature_data = feature_data.squeeze()\n\n            # Create time bins for the heatmap (24 hours)\n            time_bins = np.linspace(0, 24, n_bins + 1)  # 25 edges for 24 bins\n            bin_centers = (time_bins[:-1] + time_bins[1:]) / 2\n\n            # Create day bins (unique days)\n            days = timestamps.dt.date.unique()\n            days = sorted(days, reverse=True)\n\n            # Initialize heatmap matrix\n            heatmap_matrix = np.full((len(days), n_bins), np.nan)\n\n            # Fill the heatmap matrix\n            for i, day in enumerate(days):\n                day_mask = timestamps.dt.date == day\n                day_times = time_of_day[day_mask]\n                day_values = feature_data[day_mask]\n\n                # Bin the data by time of day\n                for j, (bin_start, bin_end) in enumerate(zip(time_bins[:-1], time_bins[1:])):\n                    time_mask = (day_times &gt;= bin_start) &amp; (day_times &lt; bin_end)\n                    if np.any(time_mask):\n                        heatmap_matrix[i, j] = np.nanmean(day_values[time_mask])\n\n            # Create the plot\n            fig, ax = plt.subplots(1, 1, figsize=figsize or (10, 3))\n\n            # Create the heatmap\n            im = ax.imshow(\n                heatmap_matrix,\n                aspect=\"auto\",\n                cmap=cmap,\n                norm=norm,\n                extent=[0, 24, 0, len(days)],\n                origin=\"lower\",\n                interpolation=\"none\",\n                **kwargs,\n            )\n\n            # Add red boundary lines between longrecording objects (different animaldays)\n            # Since we're already grouping by animalday, we need to check if there are multiple longrecordings\n            # This would be indicated by breaks in timestamps or endfile markers\n            self._add_longrecording_boundaries(ax, df_day, time_of_day, days)\n\n            # Add colorbar\n            cbar = fig.colorbar(im, ax=ax)\n            cbar.set_label(f\"{feature} ({score_type})\")\n\n            # Set labels and title\n            ax.set_xlabel(\"Time of Day (hours)\")\n            ax.set_ylabel(\"Day\")\n            ax.set_title(f\"Temporal Heatmap - {feature} - {animalday}\")\n\n            # Set x-axis ticks (every 6 hours)\n            ax.set_xticks([0, 6, 12, 18, 24])\n            ax.set_xticklabels([\"00:00\", \"06:00\", \"12:00\", \"18:00\", \"24:00\"])\n\n            # Set y-axis ticks (every day) - centered in each row\n            if len(days) &lt;= 10:\n                ax.set_yticks(np.arange(len(days)) + 0.5)\n                ax.set_yticklabels([day.strftime(\"%Y-%m-%d\") for day in days])\n            else:\n                # Show every nth day if too many days\n                n = max(1, len(days) // 10)\n                ax.set_yticks(np.arange(0, len(days), n) + 0.5)\n                ax.set_yticklabels([days[i].strftime(\"%Y-%m-%d\") for i in range(0, len(days), n)])\n\n            # Add grid\n            ax.grid(True, alpha=0.3)\n\n            # Handle figure saving/display\n            self._handle_figure(fig, title=f\"temporal_heatmap_{feature}_{animalday}\")\n\n    def _add_longrecording_boundaries(self, ax, df_day, time_of_day, days):\n        \"\"\"\n        Add red vertical lines to indicate boundaries between longrecording objects\n        and plot animalday values on top.\n\n        Args:\n            ax: matplotlib axes object\n            df_day: dataframe for the current animalday\n            time_of_day: array of time of day values (0-24 hours)\n            days: sorted list of unique days\n        \"\"\"\n        # Check if we have endfile column to identify longrecording boundaries\n        if \"endfile\" not in df_day.columns:\n            return\n\n        # Find longrecording boundaries based on endfile markers\n        df_day = df_day.reset_index()\n        endfile_indices = df_day.index[df_day[\"endfile\"].notna()].tolist()\n\n        if not endfile_indices:\n            return\n\n        # For each endfile marker, draw a red line at the corresponding timestamp\n        # REVIEW this logic might be faulty because of how timestamps are reported\n        for idx in endfile_indices:\n            if idx in df_day.index:\n                timestamp = df_day.loc[idx, \"timestamp\"]\n                day = timestamp.date()\n\n                # Find which day row this corresponds to\n                if day in days:\n                    day_idx = days.index(day)\n                    time_hour = timestamp.hour + timestamp.minute / 60.0 + timestamp.second / 3600.0\n\n                    # Draw vertical line at this time point for this day\n                    ax.axvline(\n                        x=time_hour,\n                        ymin=(day_idx) / len(days),\n                        ymax=(day_idx + 1) / len(days),\n                        color=\"red\",\n                        linewidth=1,\n                        linestyle=\"--\",\n                        alpha=0.8,\n                    )\n\n        # Add dotted white lines where animalday value changes\n        if \"animalday\" in df_day.columns:\n            df_day_sorted = df_day.sort_values(\"timestamp\")\n            prev_animalday = None\n\n            for idx, row in df_day_sorted.iterrows():\n                timestamp = row[\"timestamp\"]\n                animalday = row[\"animalday\"]\n                day = timestamp.date()\n\n                if day in days and pd.notna(animalday):\n                    # Check if animalday changed from previous row\n                    if prev_animalday is not None and animalday != prev_animalday:\n                        day_idx = days.index(day)\n                        time_hour = timestamp.hour + timestamp.minute / 60.0 + timestamp.second / 3600.0\n\n                        # Draw dotted white vertical line at animalday boundary\n                        ax.axvline(\n                            x=time_hour,\n                            ymin=(day_idx) / len(days),\n                            ymax=(day_idx + 1) / len(days),\n                            color=\"white\",\n                            linewidth=2,\n                            alpha=0.8,\n                        )\n\n                    prev_animalday = animalday\n\n    def _handle_figure(self, fig, title=None):\n        if self.save_fig:\n            if self.save_path is None:\n                raise ValueError(\"save_path must be provided when save_fig is True\")\n            if title:\n                save_name = f\"{self.save_path}_{title}.png\"\n            else:\n                save_name = f\"{self.save_path}.png\"\n            fig.savefig(save_name, bbox_inches=\"tight\", dpi=300)\n            plt.close(fig)\n        else:\n            plt.show()\n</code></pre>"},{"location":"reference/visualization/animal/#neurodent.visualization.plotting.animal.AnimalPlotter.plot_temporal_heatmap","title":"<code>plot_temporal_heatmap(features=None, figsize=None, cmap='viridis', score_type=None, norm=None, **kwargs)</code>","text":"<p>Create temporal heatmap showing feature patterns over time.</p> <p>Creates a heatmap where: - X-axis: Time of day (timestamp mod 24h) - Y-axis: Days - Color: Feature values (flattened across channels)</p>"},{"location":"reference/visualization/animal/#neurodent.visualization.plotting.animal.AnimalPlotter.plot_temporal_heatmap--parameters","title":"Parameters","text":"<p>features : list[str], optional     List of features to plot. If None, uses non-band linear features. figsize : tuple, optional     Figure size (width, height) cmap : str, optional     Colormap for the heatmap score_type : str, optional     Standardization method for feature values norm : matplotlib.colors.Normalize, optional     Normalization object for the colormap. If None, uses default normalization.     Common options:     - matplotlib.colors.Normalize(vmin=0, vmax=1)  # Fixed range     - matplotlib.colors.CenteredNorm(vcenter=0)  # Auto-detect range around 0     - matplotlib.colors.LogNorm()  # Logarithmic scale **kwargs     Additional arguments passed to matplotlib</p> Source code in <code>src/neurodent/visualization/plotting/animal.py</code> <pre><code>def plot_temporal_heatmap(\n    self,\n    features: list[str] | str = None,\n    figsize=None,\n    cmap=\"viridis\",\n    score_type=None,\n    norm=None,\n    **kwargs,\n):\n    \"\"\"\n    Create temporal heatmap showing feature patterns over time.\n\n    Creates a heatmap where:\n    - X-axis: Time of day (timestamp mod 24h)\n    - Y-axis: Days\n    - Color: Feature values (flattened across channels)\n\n    Parameters\n    ----------\n    features : list[str], optional\n        List of features to plot. If None, uses non-band linear features.\n    figsize : tuple, optional\n        Figure size (width, height)\n    cmap : str, optional\n        Colormap for the heatmap\n    score_type : str, optional\n        Standardization method for feature values\n    norm : matplotlib.colors.Normalize, optional\n        Normalization object for the colormap. If None, uses default normalization.\n        Common options:\n        - matplotlib.colors.Normalize(vmin=0, vmax=1)  # Fixed range\n        - matplotlib.colors.CenteredNorm(vcenter=0)  # Auto-detect range around 0\n        - matplotlib.colors.LogNorm()  # Logarithmic scale\n    **kwargs\n        Additional arguments passed to matplotlib\n    \"\"\"\n    if features is None:\n        # Use non-band linear features for temporal analysis\n        features = [f for f in constants.LINEAR_FEATURES]\n    if isinstance(features, str):\n        features = [features]\n\n    # Get data grouped by animalday\n    df_rowgroup = self.window_result.get_grouprows_result(\n        features, multiindex=[\"animal\", \"genotype\"], include=[\"duration\", \"endfile\", \"timestamp\", \"animalday\"]\n    )\n\n    for feature in features:\n        if feature not in df_rowgroup.columns:\n            warnings.warn(f\"Feature {feature} not found in dataframe\")\n            features.remove(feature)\n\n    if not features:\n        raise ValueError(\"No valid features found for temporal heatmap\")\n\n    # Process each feature\n    for feature in features:\n        self._plot_temporal_heatmap_feature(\n            df_rowgroup=df_rowgroup,\n            feature=feature,\n            figsize=figsize,\n            cmap=cmap,\n            score_type=score_type,\n            norm=norm,\n            **kwargs,\n        )\n</code></pre>"},{"location":"reference/visualization/experiment/","title":"Experiment Level","text":""},{"location":"reference/visualization/experiment/#neurodent.visualization.plotting.experiment.ExperimentPlotter","title":"<code>ExperimentPlotter</code>","text":"<p>A class for creating various plots from a list of multiple experimental datasets.</p> <p>This class provides methods for creating different types of plots (boxplot, violin plot, scatter plot, etc.) from experimental data with consistent data processing and styling.</p>"},{"location":"reference/visualization/experiment/#neurodent.visualization.plotting.experiment.ExperimentPlotter--plot-ordering","title":"Plot Ordering","text":"<p>The class automatically sorts data according to predefined plot orders for columns like 'channel', 'genotype', 'sex', 'isday', and 'band'. Users can customize this ordering during initialization:</p> <p>plotter = ExperimentPlotter(wars, plot_order={'channel': ['LMot', 'RMot', ...]})</p> <p>The default plot orders are defined in constants.DF_SORT_ORDER.</p>"},{"location":"reference/visualization/experiment/#neurodent.visualization.plotting.experiment.ExperimentPlotter--validation-and-warnings","title":"Validation and Warnings","text":"<p>The class automatically validates plot order against the processed DataFrame during plotting and raises warnings for any mismatches. Use validate_plot_order() to explicitly validate:</p> <p>plotter.validate_plot_order(df)</p>"},{"location":"reference/visualization/experiment/#neurodent.visualization.plotting.experiment.ExperimentPlotter--examples","title":"Examples","text":""},{"location":"reference/visualization/experiment/#neurodent.visualization.plotting.experiment.ExperimentPlotter--customize-plot-ordering-during-initialization","title":"Customize plot ordering during initialization","text":"<p>custom_order = {     'channel': ['LMot', 'RMot', 'LBar', 'RBar'],  # Only include specific channels     'genotype': ['WT', 'KO'],  # Standard order     'sex': ['Female', 'Male']  # Custom order } plotter = ExperimentPlotter(wars, plot_order=custom_order)</p> Source code in <code>src/neurodent/visualization/plotting/experiment.py</code> <pre><code>class ExperimentPlotter:\n    \"\"\"\n    A class for creating various plots from a list of multiple experimental datasets.\n\n    This class provides methods for creating different types of plots (boxplot, violin plot,\n    scatter plot, etc.) from experimental data with consistent data processing and styling.\n\n    Plot Ordering\n    ------------\n    The class automatically sorts data according to predefined plot orders for columns like\n    'channel', 'genotype', 'sex', 'isday', and 'band'. Users can customize this ordering\n    during initialization:\n\n    plotter = ExperimentPlotter(wars, plot_order={'channel': ['LMot', 'RMot', ...]})\n\n    The default plot orders are defined in constants.DF_SORT_ORDER.\n\n    Validation and Warnings\n    ----------------------\n    The class automatically validates plot order against the processed DataFrame during plotting\n    and raises warnings for any mismatches. Use validate_plot_order() to explicitly validate:\n\n    plotter.validate_plot_order(df)\n\n    Examples\n    --------\n    # Customize plot ordering during initialization\n    custom_order = {\n        'channel': ['LMot', 'RMot', 'LBar', 'RBar'],  # Only include specific channels\n        'genotype': ['WT', 'KO'],  # Standard order\n        'sex': ['Female', 'Male']  # Custom order\n    }\n    plotter = ExperimentPlotter(wars, plot_order=custom_order)\n    \"\"\"\n\n    def __init__(\n        self,\n        wars: viz.WindowAnalysisResult | list[viz.WindowAnalysisResult],\n        features: list[str] = None,\n        exclude: list[str] = None,\n        use_abbreviations: bool = True,\n        plot_order: dict = None,\n    ):\n        \"\"\"\n        Initialize plotter with WindowAnalysisResult object(s).\n\n        Parameters\n        ----------\n        wars : WindowAnalysisResult or list[WindowAnalysisResult]\n            Single WindowAnalysisResult or list of WindowAnalysisResult objects\n        features : list[str], optional\n            List of features to extract. If None, defaults to ['all']\n        exclude : list[str], optional\n            List of features to exclude from extraction\n        use_abbreviations : bool, optional\n            Whether to use abbreviations for channel names\n        plot_order : dict, optional\n            Dictionary mapping column names to the order of values for plotting.\n            If None, uses constants.DF_SORT_ORDER.\n        \"\"\"\n        features = features if features else [\"all\"]\n\n        if not isinstance(wars, list):\n            wars = [wars]\n\n        if not wars:\n            raise ValueError(\"wars cannot be empty\")\n\n        self.results = wars\n        if use_abbreviations:\n            self.channel_names = [war.channel_abbrevs for war in wars]\n        else:\n            self.channel_names = [war.channel_names for war in wars]\n        self.channel_to_idx = [{e: i for i, e in enumerate(chnames)} for chnames in self.channel_names]\n        self.all_channel_names = sorted(list(set([name for chnames in self.channel_names for name in chnames])))\n\n        # Check for inhomogeneous channel numbers/names\n        if len(set(len(channels) for channels in self.channel_names)) &gt; 1:\n            warnings.warn(\n                \"Inhomogeneous channel numbers across WindowAnalysisResult objects, which may cause errors. Run WAR.reorder_and_pad_channels() to fix.\"\n            )\n\n        # Check if channel names are consistent across all results\n        first_channels = set(self.channel_names[0])\n        for i, channels in enumerate(self.channel_names[1:], 1):\n            if set(channels) != first_channels:\n                warnings.warn(\n                    f\"Inhomogeneous channel names between WindowAnalysisResult {wars[i].animal_id} and first result, which may cause errors. Run WAR.reorder_and_pad_channels() to fix.\"\n                )\n\n        logging.info(f\"channel_names: {self.channel_names}\")\n        logging.info(f\"channel_to_idx: {self.channel_to_idx}\")\n        logging.info(f\"all_channel_names: {self.all_channel_names}\")\n\n        animal_ids = [war.animal_id for war in wars]\n        counts = Counter(animal_ids)\n        duplicates = [animal_id for animal_id, count in counts.items() if count &gt; 1]\n        if duplicates:\n            warnings.warn(\n                f\"Duplicate animal IDs found: {duplicates}. Figures will still generate, but there may be data overlap. Change WAR.animal_id to avoid this issue.\"\n            )\n\n        # Process all data into DataFrames\n        df_wars = []\n        for war in wars:\n            try:\n                dftemp = war.get_result(features=features, exclude=exclude, allow_missing=False)\n                df_wars.append(dftemp)\n            except KeyError as e:\n                logging.error(\n                    f\"Features missing in {war}. Exclude the missing features during ExperimentPlotter init, or recompute WARs with missing features.\"\n                )\n                raise e\n\n        self.df_wars: list[pd.DataFrame] = df_wars\n        self.concat_df_wars: pd.DataFrame = pd.concat(\n            df_wars, axis=0, ignore_index=True\n        )  # TODO this raises a warning about df wars having columns that are none I think\n        self.stats = None\n\n        self._plot_order = plot_order if plot_order is not None else constants.DF_SORT_ORDER.copy()\n\n    def validate_plot_order(self, df: pd.DataFrame, raise_errors: bool = False) -&gt; dict:\n        \"\"\"\n        Validate that the current plot_order contains all necessary categories for the data.\n\n        Parameters\n        ----------\n        df : pd.DataFrame\n            DataFrame to validate against (should be the DataFrame that will be sorted).\n        raise_errors : bool, optional\n            Whether to raise errors for validation issues. Default is False.\n\n        Returns\n        -------\n        dict\n            Dictionary with validation results for each column\n        \"\"\"\n        validation_results = {}\n\n        # Only validate columns that exist in the DataFrame\n        columns_to_validate = [col for col in self._plot_order.keys() if col in df.columns]\n\n        for col in columns_to_validate:\n            categories = self._plot_order[col]\n            unique_values = set(df[col].dropna().unique())\n            missing_in_order = unique_values - set(categories)\n\n            validation_results[col] = {\n                \"status\": \"valid\" if not missing_in_order else \"issues\",\n                \"unique_values\": list(unique_values),\n                \"defined_categories\": categories,\n                \"missing_in_order\": list(missing_in_order),\n            }\n\n            if missing_in_order:\n                if raise_errors:\n                    raise ValueError(\n                        f'Plot order for column \"{col}\" is missing values found in data: {missing_in_order}'\n                    )\n                else:\n                    warnings.warn(\n                        f'Plot order for column \"{col}\" is missing values found in data: {missing_in_order}',\n                        UserWarning,\n                        stacklevel=2,\n                    )\n\n        return validation_results\n\n    def pull_timeseries_dataframe(\n        self,\n        feature: str,\n        groupby: str | list[str],\n        channels: str | list[str] = \"all\",\n        collapse_channels: bool = False,\n        average_groupby: bool = False,\n        strict_groupby: bool = False,\n    ):\n        \"\"\"\n        Process feature data for plotting.\n\n        Parameters\n        ----------\n        feature : str\n            The feature to get.\n        groupby : str or list[str]\n            The variable(s) to group by.\n        channels : str or list[str], optional\n            The channels to get. If 'all', all channels are used.\n        collapse_channels : bool, optional\n            Whether to average the channels to one value.\n        average_groupby : bool, optional\n            Whether to average the groupby variable(s).\n        strict_groupby : bool, optional\n            If True, raise an exception when groupby columns contain NaN values.\n            If False (default), only issue a warning.\n\n        Returns\n        -------\n        df : pd.DataFrame\n            A DataFrame with the feature data.\n        \"\"\"\n        if \"band\" in groupby or groupby == \"band\":\n            raise ValueError(\n                # NOTE band not existing is potentially confusing error message if you are just using pull_timesereies_dataframe, there must be a more elegant way to handle this\n                \"'band' is not supported as a groupby variable. Use 'band' as a col/row/hue/x variable instead.\"\n            )\n        logging.info(\n            f\"feature: {feature}, groupby: {groupby}, channels: {channels}, collapse_channels: {collapse_channels}, average_groupby: {average_groupby}\"\n        )\n        if channels == \"all\":\n            channels = self.all_channel_names\n        elif not isinstance(channels, list):\n            channels = [channels]\n        df = self.concat_df_wars.copy()  # Use the combined DataFrame from __init__\n\n        if isinstance(groupby, str):\n            groupby = [groupby]\n\n        # Validate groupby columns exist and check for NaN values\n        missing_cols = [col for col in groupby if col not in df.columns]\n        if missing_cols:\n            raise ValueError(\n                f\"Groupby columns not found in data: {missing_cols}. Available columns: {df.columns.tolist()}\"\n            )\n\n        # Check for NaN values in groupby columns\n        nan_cols = []\n        for col in groupby:\n            if df[col].isna().any():\n                nan_count = df[col].isna().sum()\n                total_count = len(df)\n                nan_cols.append(f\"{col} ({nan_count}/{total_count} NaN values)\")\n\n        if nan_cols:\n            error_msg = (\n                f\"Groupby columns contain NaN values: {', '.join(nan_cols)}. \"\n                \"This may result from previous aggregation operations (e.g., aggregate_time_windows) \"\n                \"where these columns were not included in the groupby. \"\n                \"Consider: 1) Including these columns in your aggregation groupby, \"\n                \"2) Filtering out NaN rows before plotting, or \"\n                \"3) Using different groupby columns.\"\n            )\n            if strict_groupby:\n                raise ValueError(error_msg)\n            else:\n                warnings.warn(error_msg, UserWarning, stacklevel=2)\n\n        groups = list(df.groupby(groupby).groups.keys())\n        logging.debug(f\"groups: {groups}\")\n\n        # Check if grouping resulted in any groups\n        if not groups:  # FIXME this isn't triggering for empty groupby\n            raise ValueError(\n                f\"No valid groups found when grouping by {groupby}. \"\n                \"This may be due to all values being NaN in one or more groupby columns. \"\n                \"Check your data and groupby parameters.\"\n            )\n\n        # first iterate through animals, since that determines channel idx\n        # then pull out feature into matrix, assign channels, and add to dataframe\n        # meanwhile carrying over the groupby feature\n\n        dataframes = []\n        for i, war in enumerate(self.results):\n            df_war = self.df_wars[i]\n            ch_to_idx = self.channel_to_idx[i]\n            ch_names = self.channel_names[i]\n\n            if feature not in df_war.columns:\n                raise ValueError(f\"'{feature}' feature not found in {war}\")\n\n            match feature:\n                case _ if feature in constants.LINEAR_FEATURES + constants.BAND_FEATURES:\n                    if feature in constants.BAND_FEATURES:\n                        df_bands = pd.DataFrame(df_war[feature].tolist())\n                        vals = np.array(df_bands.values.tolist())\n                        vals = vals.transpose((0, 2, 1))\n                    else:\n                        vals = np.array(df_war[feature].tolist())\n\n                    if collapse_channels:\n                        vals = np.nanmean(vals, axis=1)\n                        logging.debug(f\"vals.shape: {vals.shape}\")\n                        vals = {\"average\": vals.tolist()}\n                    else:\n                        logging.debug(f\"vals.shape: {vals.shape}\")\n                        vals = {ch: vals[:, ch_to_idx[ch]].tolist() for ch in channels if ch in ch_names}\n                    vals = df_war[groupby].to_dict(\"list\") | vals\n\n                case \"pcorr\" | \"zpcorr\":\n                    vals = np.array(df_war[feature].tolist())\n                    if collapse_channels:\n                        # Get lower triangular elements (excluding diagonal)\n                        tril_indices = np.tril_indices(vals.shape[1], k=-1)\n                        # Take mean across pairs\n                        vals = np.nanmean(vals[:, tril_indices[0], tril_indices[1]], axis=-1)\n                        logging.debug(f\"vals.shape: {vals.shape}\")\n                        vals = {\"average\": vals.tolist()}\n                    else:\n                        logging.debug(f\"vals.shape: {vals.shape}\")\n                        vals = {\"all\": vals.tolist()}\n                    vals = df_war[groupby].to_dict(\"list\") | vals\n\n                case \"cohere\" | \"zcohere\" | \"imcoh\" | \"zimcoh\":\n                    df_bands = pd.DataFrame(df_war[feature].tolist())\n                    vals = np.array(df_bands.values.tolist())\n                    logging.debug(f\"vals.shape: {vals.shape}\")\n\n                    if collapse_channels:\n                        tril_indices = np.tril_indices(vals.shape[1], k=-1)\n                        vals = np.nanmean(vals[:, :, tril_indices[0], tril_indices[1]], axis=-1)\n                        logging.debug(f\"vals.shape: {vals.shape}\")\n                        vals = {\"average\": vals.tolist()}\n                    else:\n                        logging.debug(f\"vals.shape: {vals.shape}\")\n                        vals = {\"all\": vals.tolist()}\n                    vals = df_war[groupby].to_dict(\"list\") | vals\n\n                # ANCHOR\n                case _ if feature in constants.HIST_FEATURES:\n                    # REVIEW revise this, splitting up frequencys and values and instead making both of them independent features\n                    # this way they can be averaged, processed, etc. without worrying about tuple things\n                    # if freq present, explode it\n                    psd_data = df_war[feature].tolist()\n\n                    freq_vals = np.array(\n                        [item[0] if isinstance(item, tuple) and len(item) == 2 else item for item in psd_data]\n                    )\n                    n_unique_freq_vals = np.unique(freq_vals, axis=0).shape[0]\n                    if n_unique_freq_vals &gt; 1:\n                        raise ValueError(\n                            f\"Multiple frequency bin values found in {feature}: {n_unique_freq_vals} values\"\n                        )\n\n                    psd_vals = np.array(\n                        [item[1] if isinstance(item, tuple) and len(item) == 2 else item for item in psd_data]\n                    )\n                    psd_vals = psd_vals.transpose((0, 2, 1))\n\n                    logging.debug(f\"freq_vals.shape: {freq_vals.shape}, psd_vals.shape: {psd_vals.shape}\")\n\n                    # freq_vals.shape: (8, 501), psd_vals.shape: (8, 10, 501)\n                    if collapse_channels:\n                        psd_vals = np.nanmean(psd_vals, axis=1)\n                        logging.debug(f\"psd_vals.shape: {psd_vals.shape}\")  # (8, 501)\n                        psd_vals = {\"average\": psd_vals.tolist()}\n                    else:\n                        logging.debug(f\"psd_vals.shape: {psd_vals.shape}\")  # (8, 10, 501)\n                        psd_vals = {\n                            ch: psd_vals[:, ch_to_idx[ch], :].tolist() for ch in channels if ch in ch_names\n                        }  # (8, 10, 501)\n                    psd_vals = psd_vals | {\"freq\": freq_vals.tolist()}\n                    vals = df_war[groupby].to_dict(\"list\") | psd_vals\n\n                case _:\n                    raise ValueError(f\"{feature} is not supported in _pull_timeseries_dataframe\")\n\n            df_feature = pd.DataFrame.from_dict(vals, orient=\"columns\")\n            dataframes.append(df_feature)\n\n        df = pd.concat(dataframes, axis=0, ignore_index=True)\n\n        if feature in constants.HIST_FEATURES:\n            melt_groupby = groupby + [\"freq\"]\n        else:\n            melt_groupby = groupby\n        feature_cols = [col for col in df.columns if col not in melt_groupby]\n        df = df.melt(id_vars=melt_groupby, value_vars=feature_cols, var_name=\"channel\", value_name=feature)\n\n        if feature == \"psd\":\n            df = df.explode([\"psd\", \"freq\"])\n\n        if feature == \"psdslope\":\n            if df[feature].isna().any():\n                logging.warning(f\"{feature} contains NaNs\")\n                df = df[df[feature].notna()]\n            df[feature] = df[feature].apply(lambda x: x[0])  # get slope from [slope, intercept]\n        elif feature in constants.BAND_FEATURES + [\"cohere\", \"zcohere\", \"imcoh\", \"zimcoh\"]:\n            df[feature] = df[feature].apply(lambda x: list(zip(x, constants.BAND_NAMES)))\n            df = df.explode(feature)\n            df[[feature, \"band\"]] = pd.DataFrame(df[feature].tolist(), index=df.index)\n\n        df.reset_index(drop=True, inplace=True)\n\n        # REVIEW is this averaging correctly, i.e. animals lumped then lump together.\n        # it appears to average everything at once, but animals should be averaged first?\n        # is this just a user thing?\n        # it is just a user thing, but users should know about this detail\n        # namely, individual animaldays are proccessed so fundamentally its like an underlying \"animalday\" is part of groupby\n        # intuitively you'd think the groupbys are applied to the aggregated full array but that's not so\n        # this shouldn't be the case but maybe merging all the DFs is a memory intensive process\n        # look into this\n        if average_groupby:\n            groupby_cols = df.columns.drop(feature).tolist()\n            logging.debug(f\"groupby_cols: {groupby_cols}\")\n            grouped = df.groupby(groupby_cols, sort=False, dropna=False)\n            df = grouped[feature].apply(core.utils.nanmean_series_of_np).reset_index()\n\n        # baseline_means = (df_base\n        #                   .groupby(remaining_groupby)[feature]\n        #                   .apply(nanmean_series_of_np))\n\n        # Validate plot order against the DataFrame that will be sorted\n        self.validate_plot_order(df)\n\n        df = core.utils.sort_dataframe_by_plot_order(df, self._plot_order)\n\n        return df\n\n    def plot_catplot(\n        self,\n        feature: str,\n        groupby: str | list[str],\n        df: pd.DataFrame = None,\n        x: str = None,\n        col: str = None,\n        hue: str = None,\n        kind: Literal[\"box\", \"boxen\", \"violin\", \"strip\", \"swarm\", \"bar\", \"point\"] = \"box\",\n        catplot_params: dict = None,\n        channels: str | list[str] = \"all\",\n        collapse_channels: bool = False,\n        average_groupby: bool = False,\n        title: str = None,\n        cmap: str = None,\n        stat_pairs: list[tuple[str, str]] | Literal[\"all\", \"x\", \"hue\"] = None,\n        stat_test: str = \"Mann-Whitney\",\n        norm_test: Literal[None, \"D-Agostino\", \"log-D-Agostino\", \"K-S\"] = None,\n    ) -&gt; sns.FacetGrid:\n        \"\"\"\n        Create a boxplot of feature data.\n        \"\"\"\n        if feature in constants.MATRIX_FEATURES and not collapse_channels:\n            raise ValueError(\"To plot matrix features, collapse_channels must be True\")\n        if feature in constants.HIST_FEATURES:\n            raise ValueError(\n                f\"'{feature}' is a histogram feature and is not supported in plot_catplot. Use plot_psd instead.\"\n            )\n\n        if df is None:\n            df = self.pull_timeseries_dataframe(feature, groupby, channels, collapse_channels, average_groupby)\n\n        if isinstance(groupby, str):\n            groupby = [groupby]\n\n        # By default, just map x = groupby0, col = groupby1, hue = channel\n        default_params = {\n            \"data\": df,\n            \"x\": groupby[0],\n            \"y\": feature,\n            \"hue\": \"channel\",\n            \"col\": groupby[1] if len(groupby) &gt; 1 else None,\n            \"kind\": kind,\n            \"palette\": cmap,\n        }\n        # Update default params if x, col, or hue are explicitly provided\n        if x is not None:\n            default_params[\"x\"] = x\n        if col is not None:\n            default_params[\"col\"] = col\n        if hue is not None:\n            default_params[\"hue\"] = hue\n        if catplot_params:\n            default_params.update(catplot_params)\n\n        # Check that x, col, and hue parameters exist in dataframe columns\n        for param_name in [\"x\", \"col\", \"hue\"]:\n            if default_params[param_name] == feature:\n                raise ValueError(f\"'{param_name}' cannot be the same as 'feature'\")\n            if default_params[param_name] is not None and default_params[param_name] not in df.columns:\n                raise ValueError(\n                    f\"Parameter '{param_name}={default_params[param_name]}' not found in dataframe columns: {df.columns.tolist()}\"\n                )\n\n        # # Apply ordering to x, col, and hue if not already provided\n        # for param_name in ['x', 'col', 'hue']:\n        #     param_order_name = 'order' if param_name == 'x' else param_name + '_order'\n        #     if default_params[param_name] in constants.PLOT_ORDER and not (catplot_params is not None and param_order_name in catplot_params):\n        #         default_params[param_order_name] = constants.PLOT_ORDER[default_params[param_name]]\n\n        # Create boxplot using seaborn\n        g = sns.catplot(**default_params)\n\n        g.set_xticklabels(rotation=45, ha=\"right\")\n        g.set_titles(title)\n\n        # Only try to modify legend if it exists\n        if g.legend is not None:\n            g.legend.set_loc(\"center left\")\n            g.legend.set_bbox_to_anchor((1.0, 0.5))\n\n        # Add grid to y-axis for all subplots\n        for ax in g.axes.flat:\n            ax.yaxis.grid(True, linestyle=\"--\", which=\"major\", color=\"grey\", alpha=0.25)\n\n        groupby_test = [default_params[x] for x in [\"x\", \"col\", \"hue\"] if default_params[x] is not None]\n        match norm_test:\n            case None:\n                pass\n            case \"D-Agostino\":\n                normality_test = self._run_normaltest(df, feature, groupby_test)\n                print(f\"D-Agostino normality test: {normality_test}\")\n            case \"log-D-Agostino\":\n                df_log = df.copy()\n                df_log[feature] = np.log(df_log[feature])\n                normality_test = self._run_normaltest(df_log, feature, groupby_test)\n                print(f\"D-Agostino log-transformed normality test: {normality_test}\")\n            case \"K-S\":\n                normality_test = self._run_kstest(df, feature, groupby_test)\n                print(f\"K-S normality test: {normality_test}\")\n            case _:\n                raise ValueError(f\"{norm_test} is not supported\")\n\n        if stat_pairs:\n            annot_params = default_params.copy()\n            for (i, j, k), df in g.facet_data():\n                ax = g.facet_axis(i, j)\n                match stat_pairs:\n                    case \"all\":\n                        items = core.utils._get_groupby_keys(df, [default_params[\"x\"], default_params[\"hue\"]])\n                        pairs = core.utils._get_pairwise_combinations(items)\n                    case \"x\":\n                        items_x = core.utils._get_groupby_keys(df, default_params[\"x\"])\n                        pairs_x = core.utils._get_pairwise_combinations(items_x)\n                        items_hue = core.utils._get_groupby_keys(df, default_params[\"hue\"])\n                        pairs = [\n                            ((pair[0], hue_item), (pair[1], hue_item)) for hue_item in items_hue for pair in pairs_x\n                        ]\n                        logging.debug(f\"pairs: {pairs}\")\n                    case \"hue\":\n                        items_hue = core.utils._get_groupby_keys(df, default_params[\"hue\"])\n                        pairs_hue = core.utils._get_pairwise_combinations(items_hue)\n                        items_x = core.utils._get_groupby_keys(df, default_params[\"x\"])\n                        pairs = [((x_item, pair[0]), (x_item, pair[1])) for x_item in items_x for pair in pairs_hue]\n                        logging.debug(f\"pairs: {pairs}\")\n                    case list():\n                        pairs = stat_pairs\n                    case _:\n                        raise ValueError(f\"{stat_pairs} is not supported\")\n\n                if not pairs:\n                    logging.warning(\"No pairs found for annotation\")\n                    continue\n\n                annot_params[\"data\"] = annot_params[\"data\"].dropna()\n                annotator = Annotator(ax, pairs, verbose=0, **annot_params)\n                annotator.configure(test=stat_test, text_format=\"star\", loc=\"inside\", verbose=1)\n                annotator.apply_test(nan_policy=\"omit\")\n                annotator.annotate()\n\n        plt.tight_layout()\n\n        return g\n\n    def plot_heatmap(\n        self,\n        feature: str,\n        groupby: str | list[str],\n        df: pd.DataFrame = None,\n        col: str = None,\n        row: str = None,\n        channels: str | list[str] = \"all\",  # REVIEW this might not be needed, since all channels should be visualized\n        collapse_channels: bool = False,  # REVIEW Unable to plot a single cell, this parameter is not needed, if needed just use a catplot\n        average_groupby: bool = False,  # REVIEW average groupby might be a redundant parameter, since matrix plotting already averages\n        cmap: str = \"RdBu_r\",\n        norm: colors.Normalize | None = None,\n        height: float = 3,\n        aspect: float = 1,\n    ):\n        \"\"\"\n        Create a 2D feature plot.\n\n        Parameters:\n        -----------\n        cmap : str, default=\"RdBu_r\"\n            Colormap name or matplotlib colormap object\n        norm : matplotlib.colors.Normalize, optional\n            Normalization object. If None, will use CenteredNorm with auto-detected range.\n            Common options:\n            - colors.CenteredNorm(vcenter=0)  # Auto-detect range around 0\n            - colors.Normalize(vmin=-1, vmax=1)  # Fixed range\n            - colors.LogNorm()  # Logarithmic scale\n        \"\"\"\n        if feature not in constants.MATRIX_FEATURES:\n            raise ValueError(f\"{feature} is not supported for 2D feature plots\")\n\n        if isinstance(groupby, str):\n            groupby = [groupby]\n\n        if df is None:\n            df = self.pull_timeseries_dataframe(feature, groupby, channels, collapse_channels, average_groupby)\n\n        # Create FacetGrid\n        facet_vars = {\n            \"col\": groupby[0] if len(groupby) &gt; 0 else None,\n            \"row\": groupby[1] if len(groupby) &gt; 1 else None,\n            \"height\": height,\n            \"aspect\": aspect,\n        }\n        if col is not None:\n            facet_vars[\"col\"] = col\n        if row is not None:\n            facet_vars[\"row\"] = row\n\n        # Check that col and row parameters exist in dataframe columns\n        for param_name in [\"col\", \"row\"]:\n            if facet_vars[param_name] == feature:\n                raise ValueError(f\"'{param_name}' cannot be the same as 'feature'\")\n            if facet_vars[param_name] is not None and facet_vars[param_name] not in df.columns:\n                raise ValueError(\n                    f\"Parameter '{param_name}={facet_vars[param_name]}' not found in dataframe columns: {df.columns.tolist()}\"\n                )\n\n        g = sns.FacetGrid(df, **facet_vars)\n\n        # Map the plotting function\n        g.map_dataframe(self._plot_matrix, feature=feature, color_palette=cmap, norm=norm)\n\n        # Adjust layout\n        plt.tight_layout()\n\n        return g\n\n    def _get_default_pull_timeseries_params(self):\n        \"\"\"Get default parameters for heatmap plotting methods.\"\"\"\n        return {\n            \"channels\": \"all\",\n            \"collapse_channels\": False,\n            \"average_groupby\": False,\n        }\n\n    def plot_heatmap_faceted(\n        self,\n        feature: str,\n        groupby: str | list[str],\n        facet_vars: list[str] | str,\n        df: pd.DataFrame = None,\n        **kwargs,\n    ):\n        if isinstance(groupby, str):\n            groupby = [groupby]\n\n        if df is None:\n            pull_params = self._get_default_pull_timeseries_params()\n            pull_params.update({k: v for k, v in kwargs.items() if k in pull_params.keys()})\n            df = self.pull_timeseries_dataframe(feature=feature, groupby=groupby, **pull_params)\n\n        # Among the variables present, there are a few that need modification\n        # First modify groupby subtracting facetvars\n        if isinstance(facet_vars, str):\n            facet_vars = [facet_vars]\n\n        # FIXME this is a very ad hoc modification, and is tied to fixing pulldataframe accepting band as a feature\n        if feature in [\"cohere\", \"zcohere\", \"imcoh\", \"zimcoh\"]:\n            groupby.append(\"band\")\n\n        subfacet_groupby = groupby.copy()\n        for facet_var in facet_vars:\n            if facet_var not in groupby:\n                raise ValueError(f\"Facet variable {facet_var} must be present in groupby\")\n            subfacet_groupby.remove(facet_var)\n\n        # Then iterate over the dataframe facet_Vars unique groupby keys, passing them to plot_heatmap and building a list of facetgrids\n        grids = []\n        for name, group in df.groupby(\n            facet_vars, sort=False\n        ):  # TODO not related to here, but look at everywhere else that groupby is performed and consider if you should change it to sort=False\n            g = self.plot_heatmap(feature=feature, groupby=subfacet_groupby, df=group, **kwargs)\n\n            # Create title from facet variable values\n            if isinstance(name, tuple):\n                title = \" | \".join(f\"{var}={val}\" for var, val in zip(facet_vars, name))\n            else:\n                title = f\"{facet_vars[0]}={name}\"\n\n            g.figure.suptitle(title, y=1.02)\n            grids.append(g)\n\n        return grids\n\n    def _plot_matrix(self, data, feature, color_palette=\"RdBu_r\", norm=None, **kwargs):\n        matrices = np.array(data[feature].tolist())\n        avg_matrix = np.nanmean(matrices, axis=0)\n\n        if norm is None:\n            norm = colors.CenteredNorm(vcenter=0, halfrange=1)\n\n        # Create heatmap\n        plt.imshow(avg_matrix, cmap=color_palette, norm=norm)\n        plt.colorbar(fraction=0.046, pad=0.04)\n\n        # Set ticks and labels\n        n_channels = avg_matrix.shape[0]\n        ch_names = self.channel_names[0]\n        plt.xticks(range(n_channels), ch_names, rotation=45, ha=\"right\")\n        plt.yticks(range(n_channels), ch_names)\n\n    def plot_diffheatmap(\n        self,\n        feature: str,\n        groupby: str | list[str],\n        baseline_key: str | bool | tuple[str, ...],\n        baseline_groupby: str | list[str] = None,\n        operation: Literal[\"subtract\", \"divide\"] = \"subtract\",\n        remove_baseline: bool = False,\n        df: pd.DataFrame = None,\n        col: str = None,\n        row: str = None,\n        channels: str | list[str] = \"all\",\n        collapse_channels: bool = False,\n        average_groupby: bool = False,\n        cmap: str = \"RdBu_r\",\n        norm: colors.Normalize | None = None,\n        height: float = 3,\n        aspect: float = 1,\n    ):\n        \"\"\"\n        Create a 2D feature plot of differences between groups. Baseline is subtracted from other groups.\n\n        Parameters:\n        -----------\n        cmap : str, default=\"RdBu_r\"\n            Colormap name or matplotlib colormap object\n        norm : matplotlib.colors.Normalize, optional\n            Normalization object. If None, will use CenteredNorm with auto-detected range.\n            Common options:\n            - colors.CenteredNorm(vcenter=0)  # Auto-detect range around 0\n            - colors.Normalize(vmin=-1, vmax=1)  # Fixed range\n            - colors.LogNorm()  # Logarithmic scale\n        \"\"\"\n        if feature not in constants.MATRIX_FEATURES:\n            raise ValueError(f\"{feature} is not supported for 2D feature plots\")\n\n        if isinstance(groupby, str):\n            groupby = [groupby]\n\n        if df is None:\n            df = self.pull_timeseries_dataframe(feature, groupby, channels, collapse_channels, average_groupby)\n\n        facet_vars = {\n            \"col\": groupby[0] if len(groupby) &gt; 0 else None,\n            \"row\": groupby[1] if len(groupby) &gt; 1 else None,\n            \"height\": height,\n            \"aspect\": aspect,\n        }\n        if col is not None:\n            facet_vars[\"col\"] = col\n        if row is not None:\n            facet_vars[\"row\"] = row\n\n        # Check that col and row parameters exist in dataframe columns\n        for param_name in [\"col\", \"row\"]:\n            if facet_vars[param_name] == feature:\n                raise ValueError(f\"'{param_name}' cannot be the same as 'feature'\")\n            if facet_vars[param_name] is not None and facet_vars[param_name] not in df.columns:\n                raise ValueError(\n                    f\"Parameter '{param_name}={facet_vars[param_name]}' not found in dataframe columns: {df.columns.tolist()}\"\n                )\n\n        # Subtract baseline from feature\n        groupby = [x for x in [facet_vars[\"col\"], facet_vars[\"row\"]] if x is not None]\n        df = df_normalize_baseline(\n            df=df,\n            feature=feature,\n            groupby=groupby,\n            baseline_key=baseline_key,\n            baseline_groupby=baseline_groupby,\n            operation=operation,\n            remove_baseline=remove_baseline,\n        )\n\n        if norm is None:\n            norm = colors.CenteredNorm(vcenter=0, halfrange=0.5)\n\n        # Create FacetGrid\n        g = sns.FacetGrid(df, **facet_vars)\n\n        # Map the plotting function\n        g.map_dataframe(self._plot_matrix, feature=feature, color_palette=cmap, norm=norm)\n\n        # Adjust layout\n        plt.tight_layout()\n\n        return g\n\n    def plot_diffheatmap_faceted(\n        self,\n        feature: str,\n        groupby: str | list[str],\n        facet_vars: str | list[str],\n        baseline_key: str\n        | bool\n        | tuple[str, ...],  # NOTE these keys and groupbys only apply to the subfacets not the overall groupby\n        baseline_groupby: str | list[str] = None,\n        operation: Literal[\"subtract\", \"divide\"] = \"subtract\",\n        remove_baseline: bool = False,\n        df: pd.DataFrame = None,\n        cmap: str = \"RdBu_r\",\n        norm: colors.Normalize | None = None,\n        **kwargs,\n    ):\n        if isinstance(groupby, str):\n            groupby = [groupby]\n\n        if df is None:\n            pull_params = self._get_default_pull_timeseries_params()\n            pull_params.update({k: v for k, v in kwargs.items() if k in pull_params.keys()})\n            df = self.pull_timeseries_dataframe(feature=feature, groupby=groupby, **pull_params)\n\n        # Among the variables present, there are a few that need modification\n        # First modify groupby subtracting facetvars\n        if isinstance(facet_vars, str):\n            facet_vars = [facet_vars]\n\n        # FIXME this is a very ad hoc modification, and is tied to fixing pulldataframe accepting band as a feature\n        if feature in [\"cohere\", \"zcohere\", \"imcoh\", \"zimcoh\"]:\n            groupby.append(\"band\")\n\n        subfacet_groupby = groupby.copy()\n        for facet_var in facet_vars:\n            if facet_var not in groupby:\n                raise ValueError(f\"Facet variable {facet_var} must be present in groupby\")\n            subfacet_groupby.remove(facet_var)\n\n        # Then iterate over the dataframe facet_Vars unique groupby keys, passing them to plot_heatmap and building a list of facetgrids\n        grids = []\n        for name, group in df.groupby(\n            facet_vars, sort=False\n        ):  # TODO look at everywhere else that groupby is performed and consider changing to sort=False\n            g = self.plot_diffheatmap(\n                feature=feature,\n                groupby=subfacet_groupby,\n                baseline_key=baseline_key,\n                baseline_groupby=baseline_groupby,\n                operation=operation,\n                remove_baseline=remove_baseline,\n                df=group,\n                cmap=cmap,\n                norm=norm,\n                **kwargs,\n            )\n\n            # Create title from facet variable values\n            if isinstance(name, tuple):\n                title = \" | \".join(f\"{var}={val}\" for var, val in zip(facet_vars, name))\n            else:\n                title = f\"{facet_vars[0]}={name}\"\n\n            g.figure.suptitle(title, y=1.02)\n            grids.append(g)\n\n        return grids\n\n    def plot_qqplot(\n        self,\n        feature: str,\n        groupby: str | list[str],\n        df: pd.DataFrame = None,\n        col: str = None,\n        row: str = None,\n        log: bool = False,\n        channels: str | list[str] = \"all\",\n        collapse_channels: bool = False,\n        height: float = 3,\n        aspect: float = 1,\n        **kwargs,\n    ):\n        \"\"\"\n        Create a QQ plot of the feature data.\n        \"\"\"\n        if feature in constants.MATRIX_FEATURES and not collapse_channels:\n            raise ValueError(\"To plot matrix features, collapse_channels must be True\")\n        if feature in constants.HIST_FEATURES:\n            raise ValueError(f\"'{feature}' is a histogram feature and is not supported in plot_qqplot\")\n\n        if isinstance(groupby, str):\n            groupby = [groupby]\n\n        if df is None:\n            df = self.pull_timeseries_dataframe(feature, groupby, channels, collapse_channels, average_groupby=False)\n\n        # Create FacetGrid\n        facet_vars = {\n            \"col\": groupby[0],\n            \"row\": groupby[1] if len(groupby) &gt; 1 else None,\n            \"height\": height,\n            \"aspect\": aspect,\n        }\n        if col is not None:\n            facet_vars[\"col\"] = col\n        if row is not None:\n            facet_vars[\"row\"] = row\n\n        # Check that col and row parameters exist in dataframe columns\n        for param_name in [\"col\", \"row\"]:\n            if facet_vars[param_name] == feature:\n                raise ValueError(f\"'{param_name}' cannot be the same as 'feature'\")\n            if facet_vars[param_name] is not None and facet_vars[param_name] not in df.columns:\n                raise ValueError(\n                    f\"Parameter '{param_name}={facet_vars[param_name]}' not found in dataframe columns: {df.columns.tolist()}\"\n                )\n\n        g = sns.FacetGrid(df, margin_titles=True, **facet_vars)\n        g.map_dataframe(self._plot_qqplot, feature=feature, log=log, **kwargs)\n\n        g.set_titles(row_template=\"{row_name}\", col_template=\"{col_name}\")\n\n        plt.tight_layout()\n\n        return g\n\n    def _plot_qqplot(self, data: pd.DataFrame, feature: str, log: bool = False, **kwargs):\n        x = data[feature]\n        if log:\n            x = np.log(x)\n        x = x[np.isfinite(x)]\n        ax = plt.gca()\n        pp = sm.ProbPlot(x, fit=True)\n        pp.qqplot(line=\"45\", ax=ax)\n\n    def _run_kstest(self, df: pd.DataFrame, feature: str, groupby: str | list[str]):\n        \"\"\"\n        Run a Kolmogorov-Smirnov test for normality on the feature data.\n        This is not recommended as the test is sensitive to large values.\n        \"\"\"\n        return df.groupby(groupby)[feature].apply(lambda x: stats.kstest(x, cdf=\"norm\", nan_policy=\"omit\"))\n\n    def _run_normaltest(self, df: pd.DataFrame, feature: str, groupby: str | list[str]):\n        \"\"\"\n        Run a D'Agostino-Pearson normality test on the feature data.\n        \"\"\"\n        return df.groupby(groupby)[feature].apply(lambda x: stats.normaltest(x, nan_policy=\"omit\"))\n</code></pre>"},{"location":"reference/visualization/experiment/#neurodent.visualization.plotting.experiment.ExperimentPlotter.__init__","title":"<code>__init__(wars, features=None, exclude=None, use_abbreviations=True, plot_order=None)</code>","text":"<p>Initialize plotter with WindowAnalysisResult object(s).</p>"},{"location":"reference/visualization/experiment/#neurodent.visualization.plotting.experiment.ExperimentPlotter.__init__--parameters","title":"Parameters","text":"<p>wars : WindowAnalysisResult or list[WindowAnalysisResult]     Single WindowAnalysisResult or list of WindowAnalysisResult objects features : list[str], optional     List of features to extract. If None, defaults to ['all'] exclude : list[str], optional     List of features to exclude from extraction use_abbreviations : bool, optional     Whether to use abbreviations for channel names plot_order : dict, optional     Dictionary mapping column names to the order of values for plotting.     If None, uses constants.DF_SORT_ORDER.</p> Source code in <code>src/neurodent/visualization/plotting/experiment.py</code> <pre><code>def __init__(\n    self,\n    wars: viz.WindowAnalysisResult | list[viz.WindowAnalysisResult],\n    features: list[str] = None,\n    exclude: list[str] = None,\n    use_abbreviations: bool = True,\n    plot_order: dict = None,\n):\n    \"\"\"\n    Initialize plotter with WindowAnalysisResult object(s).\n\n    Parameters\n    ----------\n    wars : WindowAnalysisResult or list[WindowAnalysisResult]\n        Single WindowAnalysisResult or list of WindowAnalysisResult objects\n    features : list[str], optional\n        List of features to extract. If None, defaults to ['all']\n    exclude : list[str], optional\n        List of features to exclude from extraction\n    use_abbreviations : bool, optional\n        Whether to use abbreviations for channel names\n    plot_order : dict, optional\n        Dictionary mapping column names to the order of values for plotting.\n        If None, uses constants.DF_SORT_ORDER.\n    \"\"\"\n    features = features if features else [\"all\"]\n\n    if not isinstance(wars, list):\n        wars = [wars]\n\n    if not wars:\n        raise ValueError(\"wars cannot be empty\")\n\n    self.results = wars\n    if use_abbreviations:\n        self.channel_names = [war.channel_abbrevs for war in wars]\n    else:\n        self.channel_names = [war.channel_names for war in wars]\n    self.channel_to_idx = [{e: i for i, e in enumerate(chnames)} for chnames in self.channel_names]\n    self.all_channel_names = sorted(list(set([name for chnames in self.channel_names for name in chnames])))\n\n    # Check for inhomogeneous channel numbers/names\n    if len(set(len(channels) for channels in self.channel_names)) &gt; 1:\n        warnings.warn(\n            \"Inhomogeneous channel numbers across WindowAnalysisResult objects, which may cause errors. Run WAR.reorder_and_pad_channels() to fix.\"\n        )\n\n    # Check if channel names are consistent across all results\n    first_channels = set(self.channel_names[0])\n    for i, channels in enumerate(self.channel_names[1:], 1):\n        if set(channels) != first_channels:\n            warnings.warn(\n                f\"Inhomogeneous channel names between WindowAnalysisResult {wars[i].animal_id} and first result, which may cause errors. Run WAR.reorder_and_pad_channels() to fix.\"\n            )\n\n    logging.info(f\"channel_names: {self.channel_names}\")\n    logging.info(f\"channel_to_idx: {self.channel_to_idx}\")\n    logging.info(f\"all_channel_names: {self.all_channel_names}\")\n\n    animal_ids = [war.animal_id for war in wars]\n    counts = Counter(animal_ids)\n    duplicates = [animal_id for animal_id, count in counts.items() if count &gt; 1]\n    if duplicates:\n        warnings.warn(\n            f\"Duplicate animal IDs found: {duplicates}. Figures will still generate, but there may be data overlap. Change WAR.animal_id to avoid this issue.\"\n        )\n\n    # Process all data into DataFrames\n    df_wars = []\n    for war in wars:\n        try:\n            dftemp = war.get_result(features=features, exclude=exclude, allow_missing=False)\n            df_wars.append(dftemp)\n        except KeyError as e:\n            logging.error(\n                f\"Features missing in {war}. Exclude the missing features during ExperimentPlotter init, or recompute WARs with missing features.\"\n            )\n            raise e\n\n    self.df_wars: list[pd.DataFrame] = df_wars\n    self.concat_df_wars: pd.DataFrame = pd.concat(\n        df_wars, axis=0, ignore_index=True\n    )  # TODO this raises a warning about df wars having columns that are none I think\n    self.stats = None\n\n    self._plot_order = plot_order if plot_order is not None else constants.DF_SORT_ORDER.copy()\n</code></pre>"},{"location":"reference/visualization/experiment/#neurodent.visualization.plotting.experiment.ExperimentPlotter.plot_catplot","title":"<code>plot_catplot(feature, groupby, df=None, x=None, col=None, hue=None, kind='box', catplot_params=None, channels='all', collapse_channels=False, average_groupby=False, title=None, cmap=None, stat_pairs=None, stat_test='Mann-Whitney', norm_test=None)</code>","text":"<p>Create a boxplot of feature data.</p> Source code in <code>src/neurodent/visualization/plotting/experiment.py</code> <pre><code>def plot_catplot(\n    self,\n    feature: str,\n    groupby: str | list[str],\n    df: pd.DataFrame = None,\n    x: str = None,\n    col: str = None,\n    hue: str = None,\n    kind: Literal[\"box\", \"boxen\", \"violin\", \"strip\", \"swarm\", \"bar\", \"point\"] = \"box\",\n    catplot_params: dict = None,\n    channels: str | list[str] = \"all\",\n    collapse_channels: bool = False,\n    average_groupby: bool = False,\n    title: str = None,\n    cmap: str = None,\n    stat_pairs: list[tuple[str, str]] | Literal[\"all\", \"x\", \"hue\"] = None,\n    stat_test: str = \"Mann-Whitney\",\n    norm_test: Literal[None, \"D-Agostino\", \"log-D-Agostino\", \"K-S\"] = None,\n) -&gt; sns.FacetGrid:\n    \"\"\"\n    Create a boxplot of feature data.\n    \"\"\"\n    if feature in constants.MATRIX_FEATURES and not collapse_channels:\n        raise ValueError(\"To plot matrix features, collapse_channels must be True\")\n    if feature in constants.HIST_FEATURES:\n        raise ValueError(\n            f\"'{feature}' is a histogram feature and is not supported in plot_catplot. Use plot_psd instead.\"\n        )\n\n    if df is None:\n        df = self.pull_timeseries_dataframe(feature, groupby, channels, collapse_channels, average_groupby)\n\n    if isinstance(groupby, str):\n        groupby = [groupby]\n\n    # By default, just map x = groupby0, col = groupby1, hue = channel\n    default_params = {\n        \"data\": df,\n        \"x\": groupby[0],\n        \"y\": feature,\n        \"hue\": \"channel\",\n        \"col\": groupby[1] if len(groupby) &gt; 1 else None,\n        \"kind\": kind,\n        \"palette\": cmap,\n    }\n    # Update default params if x, col, or hue are explicitly provided\n    if x is not None:\n        default_params[\"x\"] = x\n    if col is not None:\n        default_params[\"col\"] = col\n    if hue is not None:\n        default_params[\"hue\"] = hue\n    if catplot_params:\n        default_params.update(catplot_params)\n\n    # Check that x, col, and hue parameters exist in dataframe columns\n    for param_name in [\"x\", \"col\", \"hue\"]:\n        if default_params[param_name] == feature:\n            raise ValueError(f\"'{param_name}' cannot be the same as 'feature'\")\n        if default_params[param_name] is not None and default_params[param_name] not in df.columns:\n            raise ValueError(\n                f\"Parameter '{param_name}={default_params[param_name]}' not found in dataframe columns: {df.columns.tolist()}\"\n            )\n\n    # # Apply ordering to x, col, and hue if not already provided\n    # for param_name in ['x', 'col', 'hue']:\n    #     param_order_name = 'order' if param_name == 'x' else param_name + '_order'\n    #     if default_params[param_name] in constants.PLOT_ORDER and not (catplot_params is not None and param_order_name in catplot_params):\n    #         default_params[param_order_name] = constants.PLOT_ORDER[default_params[param_name]]\n\n    # Create boxplot using seaborn\n    g = sns.catplot(**default_params)\n\n    g.set_xticklabels(rotation=45, ha=\"right\")\n    g.set_titles(title)\n\n    # Only try to modify legend if it exists\n    if g.legend is not None:\n        g.legend.set_loc(\"center left\")\n        g.legend.set_bbox_to_anchor((1.0, 0.5))\n\n    # Add grid to y-axis for all subplots\n    for ax in g.axes.flat:\n        ax.yaxis.grid(True, linestyle=\"--\", which=\"major\", color=\"grey\", alpha=0.25)\n\n    groupby_test = [default_params[x] for x in [\"x\", \"col\", \"hue\"] if default_params[x] is not None]\n    match norm_test:\n        case None:\n            pass\n        case \"D-Agostino\":\n            normality_test = self._run_normaltest(df, feature, groupby_test)\n            print(f\"D-Agostino normality test: {normality_test}\")\n        case \"log-D-Agostino\":\n            df_log = df.copy()\n            df_log[feature] = np.log(df_log[feature])\n            normality_test = self._run_normaltest(df_log, feature, groupby_test)\n            print(f\"D-Agostino log-transformed normality test: {normality_test}\")\n        case \"K-S\":\n            normality_test = self._run_kstest(df, feature, groupby_test)\n            print(f\"K-S normality test: {normality_test}\")\n        case _:\n            raise ValueError(f\"{norm_test} is not supported\")\n\n    if stat_pairs:\n        annot_params = default_params.copy()\n        for (i, j, k), df in g.facet_data():\n            ax = g.facet_axis(i, j)\n            match stat_pairs:\n                case \"all\":\n                    items = core.utils._get_groupby_keys(df, [default_params[\"x\"], default_params[\"hue\"]])\n                    pairs = core.utils._get_pairwise_combinations(items)\n                case \"x\":\n                    items_x = core.utils._get_groupby_keys(df, default_params[\"x\"])\n                    pairs_x = core.utils._get_pairwise_combinations(items_x)\n                    items_hue = core.utils._get_groupby_keys(df, default_params[\"hue\"])\n                    pairs = [\n                        ((pair[0], hue_item), (pair[1], hue_item)) for hue_item in items_hue for pair in pairs_x\n                    ]\n                    logging.debug(f\"pairs: {pairs}\")\n                case \"hue\":\n                    items_hue = core.utils._get_groupby_keys(df, default_params[\"hue\"])\n                    pairs_hue = core.utils._get_pairwise_combinations(items_hue)\n                    items_x = core.utils._get_groupby_keys(df, default_params[\"x\"])\n                    pairs = [((x_item, pair[0]), (x_item, pair[1])) for x_item in items_x for pair in pairs_hue]\n                    logging.debug(f\"pairs: {pairs}\")\n                case list():\n                    pairs = stat_pairs\n                case _:\n                    raise ValueError(f\"{stat_pairs} is not supported\")\n\n            if not pairs:\n                logging.warning(\"No pairs found for annotation\")\n                continue\n\n            annot_params[\"data\"] = annot_params[\"data\"].dropna()\n            annotator = Annotator(ax, pairs, verbose=0, **annot_params)\n            annotator.configure(test=stat_test, text_format=\"star\", loc=\"inside\", verbose=1)\n            annotator.apply_test(nan_policy=\"omit\")\n            annotator.annotate()\n\n    plt.tight_layout()\n\n    return g\n</code></pre>"},{"location":"reference/visualization/experiment/#neurodent.visualization.plotting.experiment.ExperimentPlotter.plot_diffheatmap","title":"<code>plot_diffheatmap(feature, groupby, baseline_key, baseline_groupby=None, operation='subtract', remove_baseline=False, df=None, col=None, row=None, channels='all', collapse_channels=False, average_groupby=False, cmap='RdBu_r', norm=None, height=3, aspect=1)</code>","text":"<p>Create a 2D feature plot of differences between groups. Baseline is subtracted from other groups.</p>"},{"location":"reference/visualization/experiment/#neurodent.visualization.plotting.experiment.ExperimentPlotter.plot_diffheatmap--parameters","title":"Parameters:","text":"<p>cmap : str, default=\"RdBu_r\"     Colormap name or matplotlib colormap object norm : matplotlib.colors.Normalize, optional     Normalization object. If None, will use CenteredNorm with auto-detected range.     Common options:     - colors.CenteredNorm(vcenter=0)  # Auto-detect range around 0     - colors.Normalize(vmin=-1, vmax=1)  # Fixed range     - colors.LogNorm()  # Logarithmic scale</p> Source code in <code>src/neurodent/visualization/plotting/experiment.py</code> <pre><code>def plot_diffheatmap(\n    self,\n    feature: str,\n    groupby: str | list[str],\n    baseline_key: str | bool | tuple[str, ...],\n    baseline_groupby: str | list[str] = None,\n    operation: Literal[\"subtract\", \"divide\"] = \"subtract\",\n    remove_baseline: bool = False,\n    df: pd.DataFrame = None,\n    col: str = None,\n    row: str = None,\n    channels: str | list[str] = \"all\",\n    collapse_channels: bool = False,\n    average_groupby: bool = False,\n    cmap: str = \"RdBu_r\",\n    norm: colors.Normalize | None = None,\n    height: float = 3,\n    aspect: float = 1,\n):\n    \"\"\"\n    Create a 2D feature plot of differences between groups. Baseline is subtracted from other groups.\n\n    Parameters:\n    -----------\n    cmap : str, default=\"RdBu_r\"\n        Colormap name or matplotlib colormap object\n    norm : matplotlib.colors.Normalize, optional\n        Normalization object. If None, will use CenteredNorm with auto-detected range.\n        Common options:\n        - colors.CenteredNorm(vcenter=0)  # Auto-detect range around 0\n        - colors.Normalize(vmin=-1, vmax=1)  # Fixed range\n        - colors.LogNorm()  # Logarithmic scale\n    \"\"\"\n    if feature not in constants.MATRIX_FEATURES:\n        raise ValueError(f\"{feature} is not supported for 2D feature plots\")\n\n    if isinstance(groupby, str):\n        groupby = [groupby]\n\n    if df is None:\n        df = self.pull_timeseries_dataframe(feature, groupby, channels, collapse_channels, average_groupby)\n\n    facet_vars = {\n        \"col\": groupby[0] if len(groupby) &gt; 0 else None,\n        \"row\": groupby[1] if len(groupby) &gt; 1 else None,\n        \"height\": height,\n        \"aspect\": aspect,\n    }\n    if col is not None:\n        facet_vars[\"col\"] = col\n    if row is not None:\n        facet_vars[\"row\"] = row\n\n    # Check that col and row parameters exist in dataframe columns\n    for param_name in [\"col\", \"row\"]:\n        if facet_vars[param_name] == feature:\n            raise ValueError(f\"'{param_name}' cannot be the same as 'feature'\")\n        if facet_vars[param_name] is not None and facet_vars[param_name] not in df.columns:\n            raise ValueError(\n                f\"Parameter '{param_name}={facet_vars[param_name]}' not found in dataframe columns: {df.columns.tolist()}\"\n            )\n\n    # Subtract baseline from feature\n    groupby = [x for x in [facet_vars[\"col\"], facet_vars[\"row\"]] if x is not None]\n    df = df_normalize_baseline(\n        df=df,\n        feature=feature,\n        groupby=groupby,\n        baseline_key=baseline_key,\n        baseline_groupby=baseline_groupby,\n        operation=operation,\n        remove_baseline=remove_baseline,\n    )\n\n    if norm is None:\n        norm = colors.CenteredNorm(vcenter=0, halfrange=0.5)\n\n    # Create FacetGrid\n    g = sns.FacetGrid(df, **facet_vars)\n\n    # Map the plotting function\n    g.map_dataframe(self._plot_matrix, feature=feature, color_palette=cmap, norm=norm)\n\n    # Adjust layout\n    plt.tight_layout()\n\n    return g\n</code></pre>"},{"location":"reference/visualization/experiment/#neurodent.visualization.plotting.experiment.ExperimentPlotter.plot_heatmap","title":"<code>plot_heatmap(feature, groupby, df=None, col=None, row=None, channels='all', collapse_channels=False, average_groupby=False, cmap='RdBu_r', norm=None, height=3, aspect=1)</code>","text":"<p>Create a 2D feature plot.</p>"},{"location":"reference/visualization/experiment/#neurodent.visualization.plotting.experiment.ExperimentPlotter.plot_heatmap--parameters","title":"Parameters:","text":"<p>cmap : str, default=\"RdBu_r\"     Colormap name or matplotlib colormap object norm : matplotlib.colors.Normalize, optional     Normalization object. If None, will use CenteredNorm with auto-detected range.     Common options:     - colors.CenteredNorm(vcenter=0)  # Auto-detect range around 0     - colors.Normalize(vmin=-1, vmax=1)  # Fixed range     - colors.LogNorm()  # Logarithmic scale</p> Source code in <code>src/neurodent/visualization/plotting/experiment.py</code> <pre><code>def plot_heatmap(\n    self,\n    feature: str,\n    groupby: str | list[str],\n    df: pd.DataFrame = None,\n    col: str = None,\n    row: str = None,\n    channels: str | list[str] = \"all\",  # REVIEW this might not be needed, since all channels should be visualized\n    collapse_channels: bool = False,  # REVIEW Unable to plot a single cell, this parameter is not needed, if needed just use a catplot\n    average_groupby: bool = False,  # REVIEW average groupby might be a redundant parameter, since matrix plotting already averages\n    cmap: str = \"RdBu_r\",\n    norm: colors.Normalize | None = None,\n    height: float = 3,\n    aspect: float = 1,\n):\n    \"\"\"\n    Create a 2D feature plot.\n\n    Parameters:\n    -----------\n    cmap : str, default=\"RdBu_r\"\n        Colormap name or matplotlib colormap object\n    norm : matplotlib.colors.Normalize, optional\n        Normalization object. If None, will use CenteredNorm with auto-detected range.\n        Common options:\n        - colors.CenteredNorm(vcenter=0)  # Auto-detect range around 0\n        - colors.Normalize(vmin=-1, vmax=1)  # Fixed range\n        - colors.LogNorm()  # Logarithmic scale\n    \"\"\"\n    if feature not in constants.MATRIX_FEATURES:\n        raise ValueError(f\"{feature} is not supported for 2D feature plots\")\n\n    if isinstance(groupby, str):\n        groupby = [groupby]\n\n    if df is None:\n        df = self.pull_timeseries_dataframe(feature, groupby, channels, collapse_channels, average_groupby)\n\n    # Create FacetGrid\n    facet_vars = {\n        \"col\": groupby[0] if len(groupby) &gt; 0 else None,\n        \"row\": groupby[1] if len(groupby) &gt; 1 else None,\n        \"height\": height,\n        \"aspect\": aspect,\n    }\n    if col is not None:\n        facet_vars[\"col\"] = col\n    if row is not None:\n        facet_vars[\"row\"] = row\n\n    # Check that col and row parameters exist in dataframe columns\n    for param_name in [\"col\", \"row\"]:\n        if facet_vars[param_name] == feature:\n            raise ValueError(f\"'{param_name}' cannot be the same as 'feature'\")\n        if facet_vars[param_name] is not None and facet_vars[param_name] not in df.columns:\n            raise ValueError(\n                f\"Parameter '{param_name}={facet_vars[param_name]}' not found in dataframe columns: {df.columns.tolist()}\"\n            )\n\n    g = sns.FacetGrid(df, **facet_vars)\n\n    # Map the plotting function\n    g.map_dataframe(self._plot_matrix, feature=feature, color_palette=cmap, norm=norm)\n\n    # Adjust layout\n    plt.tight_layout()\n\n    return g\n</code></pre>"},{"location":"reference/visualization/experiment/#neurodent.visualization.plotting.experiment.ExperimentPlotter.plot_qqplot","title":"<code>plot_qqplot(feature, groupby, df=None, col=None, row=None, log=False, channels='all', collapse_channels=False, height=3, aspect=1, **kwargs)</code>","text":"<p>Create a QQ plot of the feature data.</p> Source code in <code>src/neurodent/visualization/plotting/experiment.py</code> <pre><code>def plot_qqplot(\n    self,\n    feature: str,\n    groupby: str | list[str],\n    df: pd.DataFrame = None,\n    col: str = None,\n    row: str = None,\n    log: bool = False,\n    channels: str | list[str] = \"all\",\n    collapse_channels: bool = False,\n    height: float = 3,\n    aspect: float = 1,\n    **kwargs,\n):\n    \"\"\"\n    Create a QQ plot of the feature data.\n    \"\"\"\n    if feature in constants.MATRIX_FEATURES and not collapse_channels:\n        raise ValueError(\"To plot matrix features, collapse_channels must be True\")\n    if feature in constants.HIST_FEATURES:\n        raise ValueError(f\"'{feature}' is a histogram feature and is not supported in plot_qqplot\")\n\n    if isinstance(groupby, str):\n        groupby = [groupby]\n\n    if df is None:\n        df = self.pull_timeseries_dataframe(feature, groupby, channels, collapse_channels, average_groupby=False)\n\n    # Create FacetGrid\n    facet_vars = {\n        \"col\": groupby[0],\n        \"row\": groupby[1] if len(groupby) &gt; 1 else None,\n        \"height\": height,\n        \"aspect\": aspect,\n    }\n    if col is not None:\n        facet_vars[\"col\"] = col\n    if row is not None:\n        facet_vars[\"row\"] = row\n\n    # Check that col and row parameters exist in dataframe columns\n    for param_name in [\"col\", \"row\"]:\n        if facet_vars[param_name] == feature:\n            raise ValueError(f\"'{param_name}' cannot be the same as 'feature'\")\n        if facet_vars[param_name] is not None and facet_vars[param_name] not in df.columns:\n            raise ValueError(\n                f\"Parameter '{param_name}={facet_vars[param_name]}' not found in dataframe columns: {df.columns.tolist()}\"\n            )\n\n    g = sns.FacetGrid(df, margin_titles=True, **facet_vars)\n    g.map_dataframe(self._plot_qqplot, feature=feature, log=log, **kwargs)\n\n    g.set_titles(row_template=\"{row_name}\", col_template=\"{col_name}\")\n\n    plt.tight_layout()\n\n    return g\n</code></pre>"},{"location":"reference/visualization/experiment/#neurodent.visualization.plotting.experiment.ExperimentPlotter.pull_timeseries_dataframe","title":"<code>pull_timeseries_dataframe(feature, groupby, channels='all', collapse_channels=False, average_groupby=False, strict_groupby=False)</code>","text":"<p>Process feature data for plotting.</p>"},{"location":"reference/visualization/experiment/#neurodent.visualization.plotting.experiment.ExperimentPlotter.pull_timeseries_dataframe--parameters","title":"Parameters","text":"<p>feature : str     The feature to get. groupby : str or list[str]     The variable(s) to group by. channels : str or list[str], optional     The channels to get. If 'all', all channels are used. collapse_channels : bool, optional     Whether to average the channels to one value. average_groupby : bool, optional     Whether to average the groupby variable(s). strict_groupby : bool, optional     If True, raise an exception when groupby columns contain NaN values.     If False (default), only issue a warning.</p>"},{"location":"reference/visualization/experiment/#neurodent.visualization.plotting.experiment.ExperimentPlotter.pull_timeseries_dataframe--returns","title":"Returns","text":"<p>df : pd.DataFrame     A DataFrame with the feature data.</p> Source code in <code>src/neurodent/visualization/plotting/experiment.py</code> <pre><code>def pull_timeseries_dataframe(\n    self,\n    feature: str,\n    groupby: str | list[str],\n    channels: str | list[str] = \"all\",\n    collapse_channels: bool = False,\n    average_groupby: bool = False,\n    strict_groupby: bool = False,\n):\n    \"\"\"\n    Process feature data for plotting.\n\n    Parameters\n    ----------\n    feature : str\n        The feature to get.\n    groupby : str or list[str]\n        The variable(s) to group by.\n    channels : str or list[str], optional\n        The channels to get. If 'all', all channels are used.\n    collapse_channels : bool, optional\n        Whether to average the channels to one value.\n    average_groupby : bool, optional\n        Whether to average the groupby variable(s).\n    strict_groupby : bool, optional\n        If True, raise an exception when groupby columns contain NaN values.\n        If False (default), only issue a warning.\n\n    Returns\n    -------\n    df : pd.DataFrame\n        A DataFrame with the feature data.\n    \"\"\"\n    if \"band\" in groupby or groupby == \"band\":\n        raise ValueError(\n            # NOTE band not existing is potentially confusing error message if you are just using pull_timesereies_dataframe, there must be a more elegant way to handle this\n            \"'band' is not supported as a groupby variable. Use 'band' as a col/row/hue/x variable instead.\"\n        )\n    logging.info(\n        f\"feature: {feature}, groupby: {groupby}, channels: {channels}, collapse_channels: {collapse_channels}, average_groupby: {average_groupby}\"\n    )\n    if channels == \"all\":\n        channels = self.all_channel_names\n    elif not isinstance(channels, list):\n        channels = [channels]\n    df = self.concat_df_wars.copy()  # Use the combined DataFrame from __init__\n\n    if isinstance(groupby, str):\n        groupby = [groupby]\n\n    # Validate groupby columns exist and check for NaN values\n    missing_cols = [col for col in groupby if col not in df.columns]\n    if missing_cols:\n        raise ValueError(\n            f\"Groupby columns not found in data: {missing_cols}. Available columns: {df.columns.tolist()}\"\n        )\n\n    # Check for NaN values in groupby columns\n    nan_cols = []\n    for col in groupby:\n        if df[col].isna().any():\n            nan_count = df[col].isna().sum()\n            total_count = len(df)\n            nan_cols.append(f\"{col} ({nan_count}/{total_count} NaN values)\")\n\n    if nan_cols:\n        error_msg = (\n            f\"Groupby columns contain NaN values: {', '.join(nan_cols)}. \"\n            \"This may result from previous aggregation operations (e.g., aggregate_time_windows) \"\n            \"where these columns were not included in the groupby. \"\n            \"Consider: 1) Including these columns in your aggregation groupby, \"\n            \"2) Filtering out NaN rows before plotting, or \"\n            \"3) Using different groupby columns.\"\n        )\n        if strict_groupby:\n            raise ValueError(error_msg)\n        else:\n            warnings.warn(error_msg, UserWarning, stacklevel=2)\n\n    groups = list(df.groupby(groupby).groups.keys())\n    logging.debug(f\"groups: {groups}\")\n\n    # Check if grouping resulted in any groups\n    if not groups:  # FIXME this isn't triggering for empty groupby\n        raise ValueError(\n            f\"No valid groups found when grouping by {groupby}. \"\n            \"This may be due to all values being NaN in one or more groupby columns. \"\n            \"Check your data and groupby parameters.\"\n        )\n\n    # first iterate through animals, since that determines channel idx\n    # then pull out feature into matrix, assign channels, and add to dataframe\n    # meanwhile carrying over the groupby feature\n\n    dataframes = []\n    for i, war in enumerate(self.results):\n        df_war = self.df_wars[i]\n        ch_to_idx = self.channel_to_idx[i]\n        ch_names = self.channel_names[i]\n\n        if feature not in df_war.columns:\n            raise ValueError(f\"'{feature}' feature not found in {war}\")\n\n        match feature:\n            case _ if feature in constants.LINEAR_FEATURES + constants.BAND_FEATURES:\n                if feature in constants.BAND_FEATURES:\n                    df_bands = pd.DataFrame(df_war[feature].tolist())\n                    vals = np.array(df_bands.values.tolist())\n                    vals = vals.transpose((0, 2, 1))\n                else:\n                    vals = np.array(df_war[feature].tolist())\n\n                if collapse_channels:\n                    vals = np.nanmean(vals, axis=1)\n                    logging.debug(f\"vals.shape: {vals.shape}\")\n                    vals = {\"average\": vals.tolist()}\n                else:\n                    logging.debug(f\"vals.shape: {vals.shape}\")\n                    vals = {ch: vals[:, ch_to_idx[ch]].tolist() for ch in channels if ch in ch_names}\n                vals = df_war[groupby].to_dict(\"list\") | vals\n\n            case \"pcorr\" | \"zpcorr\":\n                vals = np.array(df_war[feature].tolist())\n                if collapse_channels:\n                    # Get lower triangular elements (excluding diagonal)\n                    tril_indices = np.tril_indices(vals.shape[1], k=-1)\n                    # Take mean across pairs\n                    vals = np.nanmean(vals[:, tril_indices[0], tril_indices[1]], axis=-1)\n                    logging.debug(f\"vals.shape: {vals.shape}\")\n                    vals = {\"average\": vals.tolist()}\n                else:\n                    logging.debug(f\"vals.shape: {vals.shape}\")\n                    vals = {\"all\": vals.tolist()}\n                vals = df_war[groupby].to_dict(\"list\") | vals\n\n            case \"cohere\" | \"zcohere\" | \"imcoh\" | \"zimcoh\":\n                df_bands = pd.DataFrame(df_war[feature].tolist())\n                vals = np.array(df_bands.values.tolist())\n                logging.debug(f\"vals.shape: {vals.shape}\")\n\n                if collapse_channels:\n                    tril_indices = np.tril_indices(vals.shape[1], k=-1)\n                    vals = np.nanmean(vals[:, :, tril_indices[0], tril_indices[1]], axis=-1)\n                    logging.debug(f\"vals.shape: {vals.shape}\")\n                    vals = {\"average\": vals.tolist()}\n                else:\n                    logging.debug(f\"vals.shape: {vals.shape}\")\n                    vals = {\"all\": vals.tolist()}\n                vals = df_war[groupby].to_dict(\"list\") | vals\n\n            # ANCHOR\n            case _ if feature in constants.HIST_FEATURES:\n                # REVIEW revise this, splitting up frequencys and values and instead making both of them independent features\n                # this way they can be averaged, processed, etc. without worrying about tuple things\n                # if freq present, explode it\n                psd_data = df_war[feature].tolist()\n\n                freq_vals = np.array(\n                    [item[0] if isinstance(item, tuple) and len(item) == 2 else item for item in psd_data]\n                )\n                n_unique_freq_vals = np.unique(freq_vals, axis=0).shape[0]\n                if n_unique_freq_vals &gt; 1:\n                    raise ValueError(\n                        f\"Multiple frequency bin values found in {feature}: {n_unique_freq_vals} values\"\n                    )\n\n                psd_vals = np.array(\n                    [item[1] if isinstance(item, tuple) and len(item) == 2 else item for item in psd_data]\n                )\n                psd_vals = psd_vals.transpose((0, 2, 1))\n\n                logging.debug(f\"freq_vals.shape: {freq_vals.shape}, psd_vals.shape: {psd_vals.shape}\")\n\n                # freq_vals.shape: (8, 501), psd_vals.shape: (8, 10, 501)\n                if collapse_channels:\n                    psd_vals = np.nanmean(psd_vals, axis=1)\n                    logging.debug(f\"psd_vals.shape: {psd_vals.shape}\")  # (8, 501)\n                    psd_vals = {\"average\": psd_vals.tolist()}\n                else:\n                    logging.debug(f\"psd_vals.shape: {psd_vals.shape}\")  # (8, 10, 501)\n                    psd_vals = {\n                        ch: psd_vals[:, ch_to_idx[ch], :].tolist() for ch in channels if ch in ch_names\n                    }  # (8, 10, 501)\n                psd_vals = psd_vals | {\"freq\": freq_vals.tolist()}\n                vals = df_war[groupby].to_dict(\"list\") | psd_vals\n\n            case _:\n                raise ValueError(f\"{feature} is not supported in _pull_timeseries_dataframe\")\n\n        df_feature = pd.DataFrame.from_dict(vals, orient=\"columns\")\n        dataframes.append(df_feature)\n\n    df = pd.concat(dataframes, axis=0, ignore_index=True)\n\n    if feature in constants.HIST_FEATURES:\n        melt_groupby = groupby + [\"freq\"]\n    else:\n        melt_groupby = groupby\n    feature_cols = [col for col in df.columns if col not in melt_groupby]\n    df = df.melt(id_vars=melt_groupby, value_vars=feature_cols, var_name=\"channel\", value_name=feature)\n\n    if feature == \"psd\":\n        df = df.explode([\"psd\", \"freq\"])\n\n    if feature == \"psdslope\":\n        if df[feature].isna().any():\n            logging.warning(f\"{feature} contains NaNs\")\n            df = df[df[feature].notna()]\n        df[feature] = df[feature].apply(lambda x: x[0])  # get slope from [slope, intercept]\n    elif feature in constants.BAND_FEATURES + [\"cohere\", \"zcohere\", \"imcoh\", \"zimcoh\"]:\n        df[feature] = df[feature].apply(lambda x: list(zip(x, constants.BAND_NAMES)))\n        df = df.explode(feature)\n        df[[feature, \"band\"]] = pd.DataFrame(df[feature].tolist(), index=df.index)\n\n    df.reset_index(drop=True, inplace=True)\n\n    # REVIEW is this averaging correctly, i.e. animals lumped then lump together.\n    # it appears to average everything at once, but animals should be averaged first?\n    # is this just a user thing?\n    # it is just a user thing, but users should know about this detail\n    # namely, individual animaldays are proccessed so fundamentally its like an underlying \"animalday\" is part of groupby\n    # intuitively you'd think the groupbys are applied to the aggregated full array but that's not so\n    # this shouldn't be the case but maybe merging all the DFs is a memory intensive process\n    # look into this\n    if average_groupby:\n        groupby_cols = df.columns.drop(feature).tolist()\n        logging.debug(f\"groupby_cols: {groupby_cols}\")\n        grouped = df.groupby(groupby_cols, sort=False, dropna=False)\n        df = grouped[feature].apply(core.utils.nanmean_series_of_np).reset_index()\n\n    # baseline_means = (df_base\n    #                   .groupby(remaining_groupby)[feature]\n    #                   .apply(nanmean_series_of_np))\n\n    # Validate plot order against the DataFrame that will be sorted\n    self.validate_plot_order(df)\n\n    df = core.utils.sort_dataframe_by_plot_order(df, self._plot_order)\n\n    return df\n</code></pre>"},{"location":"reference/visualization/experiment/#neurodent.visualization.plotting.experiment.ExperimentPlotter.validate_plot_order","title":"<code>validate_plot_order(df, raise_errors=False)</code>","text":"<p>Validate that the current plot_order contains all necessary categories for the data.</p>"},{"location":"reference/visualization/experiment/#neurodent.visualization.plotting.experiment.ExperimentPlotter.validate_plot_order--parameters","title":"Parameters","text":"<p>df : pd.DataFrame     DataFrame to validate against (should be the DataFrame that will be sorted). raise_errors : bool, optional     Whether to raise errors for validation issues. Default is False.</p>"},{"location":"reference/visualization/experiment/#neurodent.visualization.plotting.experiment.ExperimentPlotter.validate_plot_order--returns","title":"Returns","text":"<p>dict     Dictionary with validation results for each column</p> Source code in <code>src/neurodent/visualization/plotting/experiment.py</code> <pre><code>def validate_plot_order(self, df: pd.DataFrame, raise_errors: bool = False) -&gt; dict:\n    \"\"\"\n    Validate that the current plot_order contains all necessary categories for the data.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        DataFrame to validate against (should be the DataFrame that will be sorted).\n    raise_errors : bool, optional\n        Whether to raise errors for validation issues. Default is False.\n\n    Returns\n    -------\n    dict\n        Dictionary with validation results for each column\n    \"\"\"\n    validation_results = {}\n\n    # Only validate columns that exist in the DataFrame\n    columns_to_validate = [col for col in self._plot_order.keys() if col in df.columns]\n\n    for col in columns_to_validate:\n        categories = self._plot_order[col]\n        unique_values = set(df[col].dropna().unique())\n        missing_in_order = unique_values - set(categories)\n\n        validation_results[col] = {\n            \"status\": \"valid\" if not missing_in_order else \"issues\",\n            \"unique_values\": list(unique_values),\n            \"defined_categories\": categories,\n            \"missing_in_order\": list(missing_in_order),\n        }\n\n        if missing_in_order:\n            if raise_errors:\n                raise ValueError(\n                    f'Plot order for column \"{col}\" is missing values found in data: {missing_in_order}'\n                )\n            else:\n                warnings.warn(\n                    f'Plot order for column \"{col}\" is missing values found in data: {missing_in_order}',\n                    UserWarning,\n                    stacklevel=2,\n                )\n\n    return validation_results\n</code></pre>"},{"location":"reference/visualization/experiment/#neurodent.visualization.plotting.experiment.df_normalize_baseline","title":"<code>df_normalize_baseline(df, feature, groupby, baseline_key, baseline_groupby=None, operation='subtract', remove_baseline=False, strict_groupby=False)</code>","text":"<p>Subtract the baseline from the feature data.</p> Source code in <code>src/neurodent/visualization/plotting/experiment.py</code> <pre><code>def df_normalize_baseline(\n    df: pd.DataFrame,\n    feature: str,\n    groupby: str | list[str],\n    baseline_key: str | bool | tuple[str, ...],\n    baseline_groupby: str | list[str] = None,\n    operation: Literal[\"subtract\", \"divide\"] = \"subtract\",\n    remove_baseline: bool = False,\n    strict_groupby: bool = False,  # TODO implement strict_groupby in plot_diffheatmap\n):\n    \"\"\"\n    Subtract the baseline from the feature data.\n    \"\"\"\n    # Handle input types\n    if isinstance(groupby, str):\n        groupby = [groupby]\n    if baseline_groupby is None:\n        # If baseline_groupby not specified, use groupby\n        baseline_groupby = groupby\n    if isinstance(baseline_groupby, str):\n        baseline_groupby = [baseline_groupby]\n    if isinstance(baseline_key, str):\n        baseline_key = (baseline_key,)\n    if isinstance(baseline_key, bool):\n        baseline_key = (baseline_key,)\n    remaining_groupby = [col for col in groupby if col not in baseline_groupby]\n    logging.debug(f\"Groupby: {groupby}\")\n    logging.debug(f\"Baseline groupby: {baseline_groupby}\")\n    logging.debug(f\"Baseline key: {baseline_key}\")\n    logging.debug(f\"Remaining groupby: {remaining_groupby}\")\n\n    # Validate columns exist\n    all_groupby_cols = list(set(groupby + baseline_groupby))\n    missing_cols = [col for col in all_groupby_cols if col not in df.columns]\n    if missing_cols:\n        raise ValueError(f\"Groupby columns not found in data: {missing_cols}. Available columns: {df.columns.tolist()}\")\n\n    # Check for NaN values in groupby columns\n    nan_cols = []\n    for col in all_groupby_cols:\n        if df[col].isna().any():\n            nan_count = df[col].isna().sum()\n            total_count = len(df)\n            nan_cols.append(f\"{col} ({nan_count}/{total_count} NaN values)\")\n\n    if nan_cols:\n        error_msg = (\n            f\"Groupby columns contain NaN values: {', '.join(nan_cols)}. \"\n            \"This may result from previous aggregation operations where these columns were not included in the groupby. \"\n            \"Consider: 1) Including these columns in your aggregation groupby, \"\n            \"2) Filtering out NaN rows before baseline subtraction, or \"\n            \"3) Using different groupby columns.\"\n        )\n        if strict_groupby:\n            raise ValueError(error_msg)\n        else:\n            warnings.warn(error_msg, UserWarning, stacklevel=2)\n\n    # Validate baseline_key length matches baseline_groupby length\n    if len(baseline_key) != len(baseline_groupby):\n        raise ValueError(\n            f\"baseline_key length ({len(baseline_key)}) must match baseline_groupby length ({len(baseline_groupby)})\"\n        )\n\n    try:\n        df_base = df.groupby(baseline_groupby, as_index=False).get_group(\n            baseline_key\n        )  # REVIEW maybe these should use dropna=True\n    except KeyError:\n        available_keys = list(df.groupby(baseline_groupby).groups.keys())\n        raise ValueError(\n            f\"Baseline key {baseline_key} not found in groupby keys: {available_keys}. \"\n            \"Check your baseline_key and baseline_groupby parameters.\"\n        )\n\n    if remaining_groupby:\n        baseline_means = df_base.groupby(remaining_groupby)[feature].apply(core.utils.nanmean_series_of_np)\n        df_merge = df.merge(baseline_means, how=\"left\", on=remaining_groupby, suffixes=(\"\", \"_baseline\"))\n    else:\n        baseline_means = df_base.groupby(baseline_groupby)[feature].apply(\n            core.utils.nanmean_series_of_np\n        )  # Global baseline\n        assert len(baseline_means) == 1\n        df_merge = df.assign(**{f\"{feature}_baseline\": [baseline_means.iloc[0] for _ in range(len(df))]})\n\n    if remove_baseline:\n        df_merge = df_merge.loc[~(df_merge[baseline_groupby] == baseline_key).all(axis=1)]\n        if df_merge.empty:\n            raise ValueError(f\"No rows found for {groupby} != {baseline_key}\")\n\n    # Normalize the feature\n    if operation == \"subtract\":\n        df_merge[feature] = df_merge[feature].subtract(df_merge[f\"{feature}_baseline\"], fill_value=0)\n    elif operation == \"divide\":\n        df_merge[feature] = df_merge[feature].divide(df_merge[f\"{feature}_baseline\"], fill_value=0)\n    else:\n        raise ValueError(f\"Invalid operation: {operation}\")\n\n    return df_merge\n</code></pre>"},{"location":"reference/visualization/results/","title":"Results","text":""},{"location":"reference/visualization/results/#neurodent.visualization.results.AnimalOrganizer","title":"<code>AnimalOrganizer</code>","text":"<p>               Bases: <code>AnimalFeatureParser</code></p> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>class AnimalOrganizer(AnimalFeatureParser):\n    def __init__(\n        self,\n        base_folder_path,\n        anim_id: str,\n        day_sep: str | None = None,\n        mode: Literal[\"nest\", \"concat\", \"base\", \"noday\"] = \"concat\",\n        assume_from_number=False,\n        skip_days: list[str] = [],\n        truncate: bool | int = False,\n        lro_kwargs: dict = {},\n    ) -&gt; None:\n        \"\"\"\n        AnimalOrganizer is used to organize data from a single animal into a format that can be used for analysis.\n        It is used to organize data from a single animal into a format that can be used for analysis.\n\n        Args:\n            base_folder_path (str): The path to the base folder of the animal data.\n            anim_id (str): The ID of the animal. This should correspond to only one animal.\n            day_sep (str, optional): Separator for day in folder name. Set to None or empty string to get all folders. Defaults to None.\n            mode (Literal[\"nest\", \"concat\", \"base\", \"noday\"], optional): The mode of the AnimalOrganizer. Defaults to \"concat\".\n                File structure patterns (where * indicates search location):\n                \"nest\": base_folder_path / animal_id / *date_format* (looks for folders/files within animal_id subdirectories)\n                \"concat\": base_folder_path / *animal_id*date_format* (looks for folders/files with animal_id+date in name at base level)\n                \"base\": base_folder_path / * (looks for folders/files directly in base_folder_path)\n                \"noday\": base_folder_path / *animal_id* (same as concat but expects single unique match, no date filtering)\n            assume_from_number (bool, optional): Whether to assume the animal ID is a number. Defaults to False.\n            skip_days (list[str], optional): The days to skip. Defaults to [].\n            truncate (bool|int, optional): Whether to truncate the data. Defaults to False.\n            lro_kwargs (dict, optional): Keyword arguments for LongRecordingOrganizer. Defaults to {}.\n        \"\"\"\n\n        self.base_folder_path = Path(base_folder_path)\n        self.anim_id = anim_id\n        self.animal_param = [anim_id]\n        self.day_sep = day_sep\n        self.read_mode = mode\n        self.assume_from_number = assume_from_number\n\n        match mode:\n            case \"nest\":\n                self.bin_folder_pattern = self.base_folder_path / f\"*{self.anim_id}*\" / \"*\"\n            case \"concat\" | \"noday\":\n                self.bin_folder_pattern = self.base_folder_path / f\"*{self.anim_id}*\"\n                # self.bin_folder_pat = self.base_folder_path / f\"*{self.anim_id}*{self.date_format}*\"\n            case \"base\":\n                self.bin_folder_pattern = self.base_folder_path\n            # case 'noday':\n            #     self.bin_folder_pat = self.base_folder_path / f\"*{self.anim_id}*\"\n            case _:\n                raise ValueError(f\"Invalid mode: {mode}\")\n\n        self._bin_folders = glob.glob(str(self.bin_folder_pattern))\n\n        # Filter to only include directories (LongRecordingOrganizer expects folder paths)\n        before_filter_count = len(self._bin_folders)\n        self._bin_folders = [x for x in self._bin_folders if Path(x).is_dir()]\n        after_filter_count = len(self._bin_folders)\n\n        if before_filter_count &gt; after_filter_count:\n            filtered_count = before_filter_count - after_filter_count\n            logging.info(f\"Filtered out {filtered_count} non-directory items (files) from glob results\")\n\n        # if mode != 'noday':\n        #     self.__bin_folders = [x for x in self.__bin_folders if datetime.strptime(Path(x).name, self.date_format)]\n        truncate = core.utils.parse_truncate(truncate)\n        if truncate:\n            warnings.warn(f\"AnimalOrganizer will be truncated to the first {truncate} LongRecordings\")\n            self._bin_folders = self._bin_folders[:truncate]\n        self._bin_folders = [x for x in self._bin_folders if not any(y in x for y in skip_days)]\n        self.bin_folder_names = [Path(x).name for x in self._bin_folders]\n        logging.info(f\"bin_folder_pattern: {self.bin_folder_pattern}\")\n        logging.info(f\"self._bin_folders: {self._bin_folders}\")\n        logging.info(f\"self.bin_folder_names: {self.bin_folder_names}\")\n\n        if mode == \"noday\" and len(self._bin_folders) &gt; 1:\n            raise ValueError(f\"Animal ID '{self.anim_id}' is not unique, found: {', '.join(self._bin_folders)}\")\n        elif len(self._bin_folders) == 0:\n            raise ValueError(f\"No directories found for animal ID {self.anim_id} (pattern: {self.bin_folder_pattern})\")\n\n        self._animalday_dicts = [\n            core.parse_path_to_animalday(e, animal_param=self.animal_param, day_sep=self.day_sep, mode=self.read_mode)\n            for e in self._bin_folders\n        ]\n\n        # Group folders by parsed animalday to handle overlapping days\n        animalday_to_folders = {}\n        for folder, animalday_dict in zip(self._bin_folders, self._animalday_dicts):\n            animalday = animalday_dict[\"animalday\"]\n            if animalday not in animalday_to_folders:\n                animalday_to_folders[animalday] = []\n            animalday_to_folders[animalday].append(folder)\n\n        # Store grouping info\n        self._animalday_folder_groups = animalday_to_folders\n        self.unique_animaldays = list(animalday_to_folders.keys())\n\n        # Log merging operations for overlapping days\n        overlapping_days = 0\n        for animalday, folders in animalday_to_folders.items():\n            if len(folders) &gt; 1:\n                overlapping_days += 1\n                logging.info(f\"Merging {len(folders)} folders for {animalday}: {[Path(f).name for f in folders]}\")\n\n        if overlapping_days &gt; 0:\n            logging.info(f\"Found {overlapping_days} animaldays with overlapping folders\")\n\n        # Update animaldays to reflect unique days (not total folders)\n        self.animaldays = self.unique_animaldays\n        logging.info(f\"self.animaldays (unique): {self.animaldays}\")\n\n        genotypes = [x[\"genotype\"] for x in self._animalday_dicts]\n        if len(set(genotypes)) &gt; 1:\n            warnings.warn(f\"Inconsistent genotypes in {genotypes}\")\n        self.genotype = genotypes[0]\n        logging.info(f\"self.genotype: {self.genotype}\")\n\n        self.long_analyzers: list[core.LongRecordingAnalyzer] = []\n        logging.debug(f\"Creating {len(self.unique_animaldays)} LongRecordings (one per unique animalday)\")\n\n        # Process manual_datetimes if provided in lro_kwargs\n        if \"manual_datetimes\" in lro_kwargs:\n            logging.info(\"Processing manual_datetimes configuration\")\n            base_lro_kwargs = lro_kwargs.copy()\n            base_lro_kwargs[\"manual_datetimes\"] = datetime(2000, 1, 1, 0, 0, 0)\n\n            self._processed_timestamps = self._process_all_timestamps(\n                lro_kwargs[\"manual_datetimes\"], self._animalday_folder_groups, base_lro_kwargs\n            )\n            # Remove from lro_kwargs since we'll handle it manually\n            lro_kwargs = base_lro_kwargs\n        else:\n            self._processed_timestamps = None\n\n        # Create LongRecordingOrganizer instances\n        self._create_long_recordings(lro_kwargs)\n\n    def _resolve_timestamp_input(self, input_spec, folder_path: Path):\n        \"\"\"\n        Recursively resolve any timestamp input type to concrete datetime(s).\n\n        Args:\n            input_spec: datetime, List[datetime], or Callable returning either\n            folder_path: Path to folder for function execution context\n\n        Returns:\n            Union[datetime, List[datetime]]: Resolved timestamp(s)\n\n        Raises:\n            TypeError: If input_spec is not a supported type\n            Exception: If user function fails (wrapped with context)\n        \"\"\"\n        if isinstance(input_spec, datetime):\n            return input_spec\n\n        elif isinstance(input_spec, list):\n            # Validate that all items are datetime objects\n            if not all(isinstance(dt, datetime) for dt in input_spec):\n                raise TypeError(\n                    f\"All items in timestamp list must be datetime objects, got: {[type(dt) for dt in input_spec]}\"\n                )\n            return input_spec\n\n        elif callable(input_spec):\n            try:\n                logging.debug(f\"Executing user timestamp function on folder: {folder_path}\")\n                result = input_spec(folder_path)\n                # Recursively process the result (functions can return datetime or list)\n                return self._resolve_timestamp_input(result, folder_path)\n            except Exception as e:\n                logging.error(f\"User timestamp function failed on folder '{folder_path}': {e}\")\n                raise Exception(f\"User timestamp function failed on folder '{folder_path}': {e}\") from e\n\n        else:\n            raise TypeError(\n                f\"Invalid timestamp input type: {type(input_spec)}. Expected: datetime, List[datetime], or Callable\"\n            )\n\n    def _find_folder_by_name(self, folder_name: str, animalday_to_folders: dict) -&gt; Path:\n        \"\"\"Find folder path by name in the animalday groups.\"\"\"\n        for animalday, folders in animalday_to_folders.items():\n            for folder in folders:\n                if Path(folder).name == folder_name:\n                    return Path(folder)\n\n        available_names = []\n        for folders in animalday_to_folders.values():\n            available_names.extend([Path(f).name for f in folders])\n\n        raise ValueError(f\"Folder name '{folder_name}' not found. Available folders: {available_names}\")\n\n    def _compute_global_timeline(\n        self, base_datetime: datetime, animalday_to_folders: dict, base_lro_kwargs: dict\n    ) -&gt; dict:\n        \"\"\"\n        Compute contiguous timeline for all folders starting from base_datetime.\n\n        This uses a two-pass approach:\n        1. Create temporary LROs to determine durations\n        2. Compute continuous start times based on cumulative durations\n        3. Return timeline mapping for final LRO creation\n\n        Args:\n            base_datetime: Starting datetime for the timeline\n            animalday_to_folders: Mapping of animalday -&gt; list of folder paths\n            base_lro_kwargs: Base kwargs for LRO construction (without manual_datetimes)\n\n        Returns:\n            dict: Mapping of folder_name -&gt; start_datetime for continuous timeline\n        \"\"\"\n        total_folders = sum(len(folders) for folders in animalday_to_folders.values())\n        total_animaldays = len(animalday_to_folders)\n\n        logging.info(\n            f\"Computing continuous timeline for {total_animaldays} animaldays ({total_folders} total folders) \"\n            f\"starting at {base_datetime}\"\n        )\n\n        # Step 1: Create temporary LROs to determine durations\n        # We need to create LROs in the order they will appear in the final timeline\n        ordered_folders = []\n        for animalday in sorted(animalday_to_folders.keys()):\n            folders = animalday_to_folders[animalday]\n            if len(folders) &gt; 1:\n                # For overlapping folders, we need to sort them by temporal order\n                # Create temp LROs to get timing info for sorting\n                folder_lro_pairs = []\n                for folder in folders:\n                    try:\n                        temp_lro = core.LongRecordingOrganizer(folder, **base_lro_kwargs)\n                        folder_lro_pairs.append((folder, temp_lro))\n                    except Exception as e:\n                        logging.warning(f\"Failed to create temp LRO for duration estimation in {folder}: {e}\")\n                        # Use folder order as fallback\n                        folder_lro_pairs.append((folder, None))\n\n                # Sort by median time if possible\n                sorted_pairs = self._sort_lros_by_median_time(folder_lro_pairs)\n                ordered_folders.extend([folder for folder, _ in sorted_pairs])\n            else:\n                ordered_folders.extend(folders)\n\n        # Step 2: Estimate total duration for each folder\n        folder_durations = {}\n\n        for folder in ordered_folders:\n            # Create temporary LRO to get duration\n            temp_lro = core.LongRecordingOrganizer(folder, **base_lro_kwargs)\n            duration = (\n                temp_lro.LongRecording.get_duration()\n                if hasattr(temp_lro, \"LongRecording\") and temp_lro.LongRecording\n                else 0.0\n            )\n            folder_durations[folder] = duration\n            logging.debug(f\"Folder {Path(folder).name}: estimated duration = {duration:.1f}s\")\n\n        # Step 3: Compute continuous start times\n        result = {}\n        current_start_time = base_datetime\n\n        for folder in ordered_folders:\n            folder_name = Path(folder).name\n            result[folder_name] = current_start_time\n\n            # Move to next start time (current start + duration)\n            duration = folder_durations[folder]\n            current_start_time = current_start_time + timedelta(seconds=duration)\n\n            logging.debug(f\"Timeline: {folder_name} starts at {result[folder_name]}, duration {duration:.1f}s\")\n\n        total_timeline_duration = sum(folder_durations.values())\n        logging.info(\n            f\"Continuous timeline computed: {len(result)} folders, total duration {total_timeline_duration:.1f}s\"\n        )\n\n        return result\n\n    def _process_all_timestamps(self, manual_datetimes, animalday_to_folders: dict, base_lro_kwargs: dict) -&gt; dict:\n        \"\"\"\n        Process the top-level manual_datetimes input and return folder_name -&gt; resolved_timestamps mapping.\n\n        Args:\n            manual_datetimes: Any supported timestamp input type\n            animalday_to_folders: Mapping of animalday -&gt; list of folder paths\n            base_lro_kwargs: Base kwargs for LRO construction (without manual_datetimes)\n\n        Returns:\n            dict: Mapping of folder_name -&gt; Union[datetime, List[datetime]]\n        \"\"\"\n        if isinstance(manual_datetimes, dict):\n            # Per-folder specification\n            logging.info(\"Processing per-folder timestamp specification\")\n            resolved = {}\n            for folder_name, folder_spec in manual_datetimes.items():\n                folder_path = self._find_folder_by_name(folder_name, animalday_to_folders)\n                resolved[folder_name] = self._resolve_timestamp_input(folder_spec, folder_path)\n                logging.debug(f\"Resolved timestamps for {folder_name}: {resolved[folder_name]}\")\n            return resolved\n\n        elif isinstance(manual_datetimes, datetime):\n            # Global timeline - compute contiguous spacing\n            logging.info(f\"Processing global timeline starting at {manual_datetimes}\")\n            return self._compute_global_timeline(manual_datetimes, animalday_to_folders, base_lro_kwargs)\n\n        else:\n            # Function or list at top level - apply to all folders\n            logging.info(\"Processing timestamp input for all folders\")\n            resolved = {}\n            for animalday, folders in animalday_to_folders.items():\n                for folder in folders:\n                    folder_name = Path(folder).name\n                    resolved[folder_name] = self._resolve_timestamp_input(manual_datetimes, Path(folder))\n                    logging.debug(f\"Resolved timestamps for {folder_name}: {resolved[folder_name]}\")\n            return resolved\n\n    def _get_lro_kwargs_for_folder(self, folder_path: str, base_lro_kwargs: dict) -&gt; dict:\n        \"\"\"\n        Get the appropriate lro_kwargs for a specific folder, including processed timestamps if available.\n\n        Args:\n            folder_path: Path to the folder\n            base_lro_kwargs: Base kwargs to extend\n\n        Returns:\n            dict: lro_kwargs with manual_datetimes added if available\n        \"\"\"\n        if self._processed_timestamps is None:\n            return base_lro_kwargs\n\n        folder_name = Path(folder_path).name\n        if folder_name in self._processed_timestamps:\n            # Add the processed timestamps for this folder\n            kwargs = base_lro_kwargs.copy()\n            kwargs[\"manual_datetimes\"] = self._processed_timestamps[folder_name]\n            logging.debug(f\"Using processed timestamps for folder {folder_name}: {kwargs['manual_datetimes']}\")\n            return kwargs\n        else:\n            # No processed timestamps for this folder - use base kwargs\n            logging.debug(f\"No processed timestamps for folder {folder_name}, using base kwargs\")\n            return base_lro_kwargs\n\n    def _log_timeline_summary(self):\n        \"\"\"Log timeline summary for debugging purposes.\"\"\"\n\n        lines = [\"AnimalOrganizer Timeline Summary:\"]\n\n        if not self.long_recordings:\n            lines.append(\"No LongRecordings created\")\n        else:\n            for i, lro in enumerate(self.long_recordings):\n                try:\n                    start_time = self._get_lro_start_time(lro)\n                    end_time = self._get_lro_end_time(lro)\n                    duration = (\n                        lro.LongRecording.get_duration() if hasattr(lro, \"LongRecording\") and lro.LongRecording else 0\n                    )\n                    n_files = len(lro.file_durations) if hasattr(lro, \"file_durations\") and lro.file_durations else 1\n                    folder_path = getattr(lro, \"base_folder_path\", \"unknown\")\n\n                    lines.append(\n                        f\"LRO {i}: {start_time} \u2192 {end_time} \"\n                        f\"(duration: {duration:.1f}s, files: {n_files}, folder: {Path(folder_path).name})\"\n                    )\n                except Exception as e:\n                    lines.append(f\"Failed to get timeline info for LRO {i}: {e}\")\n\n        logging.info(\"\\n\".join(lines))\n\n    def _get_lro_start_time(self, lro):\n        \"\"\"Get the start time of an LRO.\"\"\"\n        if hasattr(lro, \"file_end_datetimes\") and lro.file_end_datetimes:\n            if hasattr(lro, \"file_durations\") and lro.file_durations:\n                # Calculate start time from first end time and duration\n                first_end = next(dt for dt in lro.file_end_datetimes if dt is not None)\n                first_duration = lro.file_durations[0]\n                return first_end - timedelta(seconds=first_duration)\n        return \"unknown\"\n\n    def _get_lro_end_time(self, lro):\n        \"\"\"Get the end time of an LRO.\"\"\"\n        if hasattr(lro, \"file_end_datetimes\") and lro.file_end_datetimes:\n            # Get the last non-None end time\n            end_times = [dt for dt in lro.file_end_datetimes if dt is not None]\n            if end_times:\n                return max(end_times)\n        return \"unknown\"\n\n    def get_timeline_summary(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Get timeline summary as a DataFrame for user inspection and debugging.\n\n        Returns:\n            pd.DataFrame: Timeline information with columns:\n                - lro_index: Index of the LRO\n                - start_time: Start datetime of the LRO\n                - end_time: End datetime of the LRO\n                - duration_s: Duration in seconds\n                - n_files: Number of files in the LRO\n                - folder_path: Base folder path\n                - animalday: Parsed animalday identifier\n        \"\"\"\n        if not self.long_recordings:\n            return pd.DataFrame()\n\n        timeline_data = []\n        for i, lro in enumerate(self.long_recordings):\n            try:\n                start_time = self._get_lro_start_time(lro)\n                end_time = self._get_lro_end_time(lro)\n                duration = (\n                    lro.LongRecording.get_duration() if hasattr(lro, \"LongRecording\") and lro.LongRecording else 0\n                )\n                n_files = len(lro.file_durations) if hasattr(lro, \"file_durations\") and lro.file_durations else 1\n                folder_path = getattr(lro, \"base_folder_path\", \"unknown\")\n\n                timeline_data.append(\n                    {\n                        \"lro_index\": i,\n                        \"start_time\": start_time,\n                        \"end_time\": end_time,\n                        \"duration_s\": duration,\n                        \"n_files\": n_files,\n                        \"folder_path\": str(folder_path),\n                        \"folder_name\": Path(folder_path).name if folder_path != \"unknown\" else \"unknown\",\n                        \"animalday\": getattr(\n                            lro, \"_animalday\", \"unknown\"\n                        ),  # This might not exist, but useful if it does\n                    }\n                )\n            except Exception as e:\n                # Include failed LROs in the summary for debugging\n                timeline_data.append(\n                    {\n                        \"lro_index\": i,\n                        \"start_time\": \"error\",\n                        \"end_time\": \"error\",\n                        \"duration_s\": 0,\n                        \"n_files\": 0,\n                        \"folder_path\": \"error\",\n                        \"folder_name\": \"error\",\n                        \"animalday\": \"error\",\n                        \"error\": str(e),\n                    }\n                )\n\n        return pd.DataFrame(timeline_data)\n\n    def _create_long_recordings(self, lro_kwargs: dict):\n        \"\"\"Create LongRecordingOrganizer instances for each unique animalday.\"\"\"\n        # Create one LRO per unique animalday (not per folder)\n        self.long_recordings: list[core.LongRecordingOrganizer] = []\n        for animalday, folders in self._animalday_folder_groups.items():\n            if len(folders) == 1:\n                # Single folder - use processed timestamps if available\n                folder_kwargs = self._get_lro_kwargs_for_folder(folders[0], lro_kwargs)\n                lro = core.LongRecordingOrganizer(folders[0], **folder_kwargs)\n            else:\n                # Multiple folders - create individual LROs then sort and merge\n                logging.info(f\"Creating individual LROs for {len(folders)} folders for {animalday}\")\n\n                # Create individual LROs first, each with their own processed timestamps\n                folder_lro_pairs = []\n                for folder in folders:\n                    folder_kwargs = self._get_lro_kwargs_for_folder(folder, lro_kwargs)\n                    individual_lro = core.LongRecordingOrganizer(folder, **folder_kwargs)\n                    folder_lro_pairs.append((folder, individual_lro))\n\n                # Sort by median time using constructed LROs\n                sorted_folder_lro_pairs = self._sort_lros_by_median_time(folder_lro_pairs)\n\n                # Debug logging to show the order of LROs being merged\n                logging.info(\"LRO merge order for overlapping animalday:\")\n                for i, (folder, lro) in enumerate(sorted_folder_lro_pairs):\n                    folder_name = Path(folder).name\n                    # Handle mock objects gracefully\n                    try:\n                        duration = (\n                            lro.LongRecording.get_duration()\n                            if hasattr(lro, \"LongRecording\") and lro.LongRecording\n                            else 0\n                        )\n                        duration_str = f\"{float(duration):.1f}s\"\n                    except (TypeError, ValueError):\n                        duration_str = \"mock\"\n                    logging.info(f\"  {i + 1}. {folder_name} (duration: {duration_str})\")\n\n                # Merge all LROs into the first one (in temporal order)\n                merged_lro = sorted_folder_lro_pairs[0][1]  # Get the LRO from first tuple\n                logging.info(f\"Base LRO: {Path(sorted_folder_lro_pairs[0][0]).name}\")\n\n                for i, (folder, lro) in enumerate(sorted_folder_lro_pairs[1:], 1):\n                    folder_name = Path(folder).name\n                    logging.info(f\"Merging LRO {i}: {folder_name} into base LRO\")\n                    merged_lro.merge(lro)\n\n                lro = merged_lro\n                logging.info(f\"Successfully merged {len(sorted_folder_lro_pairs)} LROs for {animalday}\")\n\n            self.long_recordings.append(lro)\n\n        # Log timeline summary for debugging\n        self._log_timeline_summary()\n\n        channel_names = [x.channel_names for x in self.long_recordings]\n        if len(set([\" \".join(x) for x in channel_names])) &gt; 1:\n            warnings.warn(f\"Inconsistent channel names in long_recordings: {channel_names}\")\n        self.channel_names = channel_names[0]\n        self.bad_channels_dict = {}\n\n        animal_ids = [x[\"animal\"] for x in self._animalday_dicts]\n        if len(set(animal_ids)) &gt; 1:\n            warnings.warn(f\"Inconsistent animal IDs in {animal_ids}\")\n        self.animal_id = animal_ids[0]\n\n        self.features_df: pd.DataFrame = pd.DataFrame()\n        self.features_avg_df: pd.DataFrame = pd.DataFrame()\n\n    def _sort_lros_by_median_time(self, folder_lro_pairs):\n        \"\"\"Sort LROs by median timestamp of their constituent recordings.\n\n        Args:\n            folder_lro_pairs (list): List of (folder_path, lro) tuples\n\n        Returns:\n            list: Sorted (folder_path, lro) tuples in temporal order based on median timestamp\n\n        Note:\n            Extracts file_end_datetimes from each LRO (timestamps from LastEdit fields in metadata CSV files),\n            calculates the median timestamp of constituent recordings within each LRO, and sorts LROs\n            by this median timestamp. This ensures proper temporal ordering based on actual recording\n            content rather than folder naming conventions. Falls back to folder modification time if\n            no valid timestamps are available.\n        \"\"\"\n        if len(folder_lro_pairs) &lt;= 1:\n            return folder_lro_pairs\n\n        folder_lro_times = []\n\n        for folder_path, lro in folder_lro_pairs:\n            try:\n                # Get median timestamp from constituent recordings within the LRO\n                if hasattr(lro, \"file_end_datetimes\") and lro.file_end_datetimes:\n                    try:\n                        valid_timestamps = [ts for ts in lro.file_end_datetimes if ts is not None]\n                    except TypeError:\n                        valid_timestamps = []\n\n                    if valid_timestamps:\n                        # Sort timestamps and get the median\n                        valid_timestamps.sort()\n                        n_timestamps = len(valid_timestamps)\n\n                        if n_timestamps % 2 == 1:\n                            # Odd number of timestamps - take middle one\n                            median_timestamp = valid_timestamps[n_timestamps // 2]\n                        else:\n                            # Even number of timestamps - take average of two middle ones\n                            mid1 = valid_timestamps[n_timestamps // 2 - 1]\n                            mid2 = valid_timestamps[n_timestamps // 2]\n                            median_timestamp = mid1 + (mid2 - mid1) / 2\n\n                        # Convert to seconds since epoch for sorting\n                        median_time_seconds = median_timestamp.timestamp()\n                        logging.debug(\n                            f\"LRO {Path(folder_path).name}: {n_timestamps} recordings, median timestamp: {median_timestamp}\"\n                        )\n                    else:\n                        raise ValueError(f\"No file_end_datetimes available in LRO {Path(folder_path).name}, cannot determine temporal order\")\n                else:\n                    raise ValueError(f\"No file_end_datetimes available in LRO {Path(folder_path).name}, cannot determine temporal order\")\n\n                folder_lro_times.append((folder_path, lro, median_time_seconds))\n\n            except Exception as e:\n                logging.warning(f\"Could not extract timing from {folder_path}: {e}\")\n                raise\n\n        # Sort by median time\n        sorted_folder_lro_times = sorted(folder_lro_times, key=lambda x: x[2])\n        sorted_folder_lro_pairs = [(folder, lro) for folder, lro, _ in sorted_folder_lro_times]\n\n        # Log the sorting for debugging\n        if len(folder_lro_pairs) &gt; 1:\n            logging.info(\"LRO temporal sorting details:\")\n            for i, (folder, lro, median_time_seconds) in enumerate(sorted_folder_lro_times):\n                folder_name = Path(folder).name\n\n                # Convert back to datetime for readable logging\n                try:\n                    from datetime import datetime\n\n                    median_datetime = datetime.fromtimestamp(median_time_seconds)\n                    median_time_str = median_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n                except (TypeError, ValueError, OSError):\n                    median_time_str = f\"{median_time_seconds:.1f}s\"\n\n                # Handle mock objects gracefully for duration\n                try:\n                    duration = (\n                        lro.LongRecording.get_duration() if hasattr(lro, \"LongRecording\") and lro.LongRecording else 0\n                    )\n                    duration_str = f\"{float(duration):.1f}s\"\n                except (TypeError, ValueError):\n                    duration_str = \"mock\"\n\n                # Show number of recordings in LRO\n                try:\n                    n_recordings = (\n                        len(lro.file_end_datetimes)\n                        if hasattr(lro, \"file_end_datetimes\") and lro.file_end_datetimes\n                        else 0\n                    )\n                except (TypeError, AttributeError):\n                    n_recordings = \"unknown\"\n\n                logging.info(\n                    f\"  {i + 1}. {folder_name}: median_timestamp={median_time_str}, {n_recordings} recordings, duration={duration_str}\"\n                )\n\n            # Summary line for quick reference\n            folder_names = [Path(f).name for f, _, _ in sorted_folder_lro_times]\n            median_times = []\n            for _, _, median_time_seconds in sorted_folder_lro_times:\n                median_datetime = datetime.fromtimestamp(median_time_seconds)\n                median_times.append(median_datetime.strftime(\"%H:%M:%S\"))\n\n            logging.info(f\"Final sort order: {list(zip(folder_names, median_times))}\")\n\n        return sorted_folder_lro_pairs\n\n    def convert_colbins_to_rowbins(self, overwrite=False, multiprocess_mode: Literal[\"dask\", \"serial\"] = \"serial\"):\n        for lrec in tqdm(self.long_recordings, desc=\"Converting column bins to row bins\"):\n            lrec.convert_colbins_to_rowbins(overwrite=overwrite, multiprocess_mode=multiprocess_mode)\n\n    def convert_rowbins_to_rec(self, multiprocess_mode: Literal[\"dask\", \"serial\"] = \"serial\"):\n        for lrec in tqdm(self.long_recordings, desc=\"Converting row bins to recs\"):\n            lrec.convert_rowbins_to_rec(multiprocess_mode=multiprocess_mode)\n\n    def cleanup_rec(self):\n        for lrec in self.long_recordings:\n            lrec.cleanup_rec()\n\n    def compute_bad_channels(self, lof_threshold: float = None, force_recompute: bool = False):\n        \"\"\"Compute bad channels using LOF analysis for all recordings.\n\n        Args:\n            lof_threshold (float, optional): Threshold for determining bad channels from LOF scores.\n                                           If None, only computes/loads scores without setting bad_channel_names.\n            force_recompute (bool): Whether to recompute LOF scores even if they exist.\n        \"\"\"\n        logging.info(\n            f\"Computing bad channels for {len(self.long_recordings)} recordings with threshold={lof_threshold}\"\n        )\n        for i, lrec in enumerate(self.long_recordings):\n            logging.debug(f\"Computing bad channels for recording {i}: {self.animaldays[i]}\")\n            lrec.compute_bad_channels(lof_threshold=lof_threshold, force_recompute=force_recompute)\n            logging.debug(\n                f\"Recording {i} LOF scores computed: {hasattr(lrec, 'lof_scores') and lrec.lof_scores is not None}\"\n            )\n\n        # Update bad channels dict if threshold was applied\n        if lof_threshold is not None:\n            self.bad_channels_dict = {\n                animalday: lrec.bad_channel_names for animalday, lrec in zip(self.animaldays, self.long_recordings)\n            }\n\n    def apply_lof_threshold(self, lof_threshold: float):\n        \"\"\"Apply threshold to existing LOF scores to determine bad channels for all recordings.\n\n        Args:\n            lof_threshold (float): Threshold for determining bad channels.\n        \"\"\"\n        for lrec in self.long_recordings:\n            lrec.apply_lof_threshold(lof_threshold)\n\n        self.bad_channels_dict = {\n            animalday: lrec.bad_channel_names for animalday, lrec in zip(self.animaldays, self.long_recordings)\n        }\n\n    def get_all_lof_scores(self) -&gt; dict:\n        \"\"\"Get LOF scores for all recordings.\n\n        Returns:\n            dict: Dictionary mapping animal days to LOF score dictionaries.\n        \"\"\"\n        return {animalday: lrec.get_lof_scores() for animalday, lrec in zip(self.animaldays, self.long_recordings)}\n\n    def compute_windowed_analysis(\n        self,\n        features: list[str],\n        exclude: list[str] = [],\n        window_s=4,\n        multiprocess_mode: Literal[\"dask\", \"serial\"] = \"serial\",\n        suppress_short_interval_error=False,\n        apply_notch_filter=True,\n        **kwargs,\n    ):\n        \"\"\"Computes windowed analysis of animal recordings. The data is divided into windows (time bins), then features are extracted from each window. The result is\n        formatted to a Dataframe and wrapped into a WindowAnalysisResult object.\n\n        Args:\n            features (list[str]): List of features to compute. See individual compute_...() functions for output format\n            exclude (list[str], optional): List of features to ignore. Will override the features parameter. Defaults to [].\n            window_s (int, optional): Length of each window in seconds. Note that some features break with very short window times. Defaults to 4.\n            suppress_short_interval_error (bool, optional): If True, suppress ValueError for short intervals between timestamps in resulting WindowAnalysisResult. Useful for aggregated WARs. Defaults to False.\n            apply_notch_filter (bool, optional): Whether to apply notch filtering to remove line noise. Uses constants.LINE_FREQ. Defaults to True.\n\n        Raises:\n            AttributeError: If a feature's compute_...() function was not implemented, this error will be raised.\n\n        Returns:\n            window_analysis_result: a WindowAnalysisResult object\n        \"\"\"\n        features = _sanitize_feature_request(features, exclude)\n\n        dataframes = []\n        for lrec in self.long_recordings:  # Iterate over all long recordings\n            logging.info(f\"Computing windowed analysis for {lrec.base_folder_path}\")\n            lan = core.LongRecordingAnalyzer(lrec, fragment_len_s=window_s, apply_notch_filter=apply_notch_filter)\n            if lan.n_fragments == 0:\n                logging.warning(f\"No fragments found for {lrec.base_folder_path}. Skipping.\")\n                continue\n\n            logging.debug(f\"Processing {lan.n_fragments} fragments\")\n            miniters = int(lan.n_fragments / 100)\n            match multiprocess_mode:\n                case \"dask\":\n                    # The last fragment is not included because it makes the dask array ragged\n                    logging.debug(\"Converting LongRecording to numpy array\")\n\n                    n_fragments_war = max(lan.n_fragments - 1, 1)\n                    first_fragment = lan.get_fragment_np(0)\n                    np_fragments = np.empty((n_fragments_war,) + first_fragment.shape, dtype=first_fragment.dtype)\n                    logging.debug(f\"np_fragments.shape: {np_fragments.shape}\")\n                    for idx in range(n_fragments_war):\n                        np_fragments[idx] = lan.get_fragment_np(idx)\n\n                    # Cache fragments to zarr\n                    tmppath, _ = core.utils.cache_fragments_to_zarr(np_fragments, n_fragments_war)\n                    del np_fragments\n\n                    logging.debug(\"Processing metadata serially\")\n                    metadatas = [self._process_fragment_metadata(idx, lan, window_s) for idx in range(n_fragments_war)]\n                    meta_df = pd.DataFrame(metadatas)\n\n                    logging.debug(\"Processing features in parallel\")\n                    np_fragments_reconstruct = da.from_zarr(tmppath, chunks=(\"auto\", -1, -1))\n                    logging.debug(f\"Dask array shape: {np_fragments_reconstruct.shape}\")\n                    logging.debug(f\"Dask array chunks: {np_fragments_reconstruct.chunks}\")\n\n                    # Create delayed tasks for each fragment using efficient dependency resolution\n                    feature_values = [\n                        delayed(FragmentAnalyzer.process_fragment_with_dependencies)(\n                            np_fragments_reconstruct[idx], lan.f_s, features, kwargs\n                        )\n                        for idx in range(n_fragments_war)\n                    ]\n\n                    # Compute features in parallel\n                    feature_values = dask.compute(*feature_values)\n\n                    # Clean up temp directory after processing\n                    logging.debug(\"Cleaning up temp directory\")\n                    try:\n                        import shutil\n\n                        shutil.rmtree(tmppath)\n                    except (OSError, FileNotFoundError) as e:\n                        logging.warning(f\"Failed to remove temporary directory {tmppath}: {e}\")\n\n                    logging.debug(\"Combining metadata and feature values\")\n                    feat_df = pd.DataFrame(feature_values)\n                    lan_df = pd.concat([meta_df, feat_df], axis=1)\n\n                case _:\n                    logging.debug(\"Processing serially\")\n                    lan_df = []\n                    for idx in tqdm(range(lan.n_fragments), desc=\"Processing rows\", miniters=miniters):\n                        lan_df.append(self._process_fragment_serial(idx, features, lan, window_s, kwargs))\n\n            lan_df = pd.DataFrame(lan_df)\n\n            logging.debug(\"Validating timestamps\")\n            core.validate_timestamps(lan_df[\"timestamp\"].tolist())\n            lan_df = lan_df.sort_values(\"timestamp\").reset_index(drop=True)\n\n            self.long_analyzers.append(lan)\n            dataframes.append(lan_df)\n\n        self.features_df = pd.concat(dataframes)\n        self.features_df = self.features_df\n\n        # Collect LOF scores from long recordings\n        lof_scores_dict = {}\n        for animalday, lrec in zip(self.animaldays, self.long_recordings):\n            logging.debug(\n                f\"Checking LOF scores for {animalday}: has_attr={hasattr(lrec, 'lof_scores')}, \"\n                f\"is_not_none={getattr(lrec, 'lof_scores', None) is not None}\"\n            )\n            if hasattr(lrec, \"lof_scores\") and lrec.lof_scores is not None:\n                lof_scores_dict[animalday] = {\n                    \"lof_scores\": lrec.lof_scores.tolist(),\n                    \"channel_names\": lrec.channel_names,\n                }\n                logging.info(f\"Added LOF scores for {animalday}: {len(lrec.lof_scores)} channels\")\n\n        logging.info(f\"Total LOF scores collected: {len(lof_scores_dict)} animal days\")\n\n        self.window_analysis_result = WindowAnalysisResult(\n            self.features_df,\n            self.animal_id,\n            self.genotype,\n            self.channel_names,\n            self.assume_from_number,\n            self.bad_channels_dict,\n            suppress_short_interval_error,\n            lof_scores_dict,\n        )\n\n        return self.window_analysis_result\n\n    def compute_spike_analysis(self, multiprocess_mode: Literal[\"dask\", \"serial\"] = \"serial\"):\n        \"\"\"Compute spike sorting on all long recordings and return a list of SpikeAnalysisResult objects\n\n        Args:\n            multiprocess_mode (Literal['dask', 'serial']): Whether to use Dask for parallel processing. Defaults to 'serial'.\n\n        Returns:\n            spike_analysis_results: list[SpikeAnalysisResult]. Each SpikeAnalysisResult object corresponds to a LongRecording object,\n            typically a different day or recording session.\n\n        Raises:\n            ImportError: If mountainsort5 is not available.\n        \"\"\"\n        # Check if mountainsort5 is available\n        if not MOUNTAINSORT_AVAILABLE:\n            raise ImportError(\"Spike analysis requires mountainsort5. Install it with: pip install mountainsort5\")\n        sars = []\n        lrec_sorts = []\n        lrec_recs = []\n        recs = [lrec.LongRecording for lrec in self.long_recordings]\n        logging.info(f\"Sorting {len(recs)} recordings\")\n        for rec in recs:\n            if rec.get_total_samples() == 0:\n                logging.warning(f\"Skipping {rec.__str__()} because it has no samples\")\n                sortings, recordings = [], []\n            else:\n                sortings, recordings = core.MountainSortAnalyzer.sort_recording(\n                    rec, multiprocess_mode=multiprocess_mode\n                )\n            lrec_sorts.append(sortings)\n            lrec_recs.append(recordings)\n\n        if multiprocess_mode == \"dask\":\n            lrec_sorts = dask.compute(*lrec_sorts)\n\n        lrec_sas = [\n            [\n                si.create_sorting_analyzer(sorting, recording, sparse=False)\n                for sorting, recording in zip(sortings, recordings)\n            ]\n            for sortings, recordings in zip(lrec_sorts, lrec_recs)\n        ]\n        sars = [\n            SpikeAnalysisResult(\n                result_sas=sas,\n                result_mne=None,\n                animal_id=self.animal_id,\n                genotype=self.genotype,\n                animal_day=self.animaldays[i],\n                bin_folder_name=self.bin_folder_names[i],\n                metadata=self.long_recordings[i].meta,\n                channel_names=self.channel_names,\n                assume_from_number=self.assume_from_number,\n            )\n            for i, sas in enumerate(lrec_sas)\n        ]\n\n        self.spike_analysis_results = sars\n        return self.spike_analysis_results\n\n    def compute_frequency_domain_spike_analysis(\n        self,\n        detection_params: dict = None,\n        max_length: int = None,\n        multiprocess_mode: Literal[\"dask\", \"serial\"] = \"serial\"\n    ):\n        \"\"\"\n        Compute frequency-domain spike detection on all long recordings.\n\n        Args:\n            detection_params (dict, optional): Detection parameters. Uses defaults if None.\n            max_length (int, optional): Maximum length in samples to analyze per recording\n            multiprocess_mode (Literal[\"dask\", \"serial\"]): Processing mode\n\n        Returns:\n            list[FrequencyDomainSpikeAnalysisResult]: Results for each recording session\n\n        Raises:\n            ImportError: If SpikeInterface is not available\n        \"\"\"\n        # Import here to avoid circular imports\n        from .frequency_domain_results import FrequencyDomainSpikeAnalysisResult\n\n        fdsar_list = []\n        recs = [lrec.LongRecording for lrec in self.long_recordings]\n\n        logging.info(f\"Running frequency-domain spike detection on {len(recs)} recordings\")\n        logging.info(f\"Detection parameters: {detection_params}\")\n\n        for i, rec in enumerate(recs):\n            if rec.get_total_samples() == 0:\n                logging.warning(f\"Skipping {rec} because it has no samples\")\n                continue\n\n            try:\n                # Run frequency domain spike detection\n                spike_indices_per_channel, mne_raw_with_annotations = (\n                    FrequencyDomainSpikeDetector.detect_spikes_recording(\n                        rec,\n                        detection_params=detection_params,\n                        max_length=max_length,\n                        multiprocess_mode=multiprocess_mode\n                    )\n                )\n\n                # Create FrequencyDomainSpikeAnalysisResult\n                fdsar = FrequencyDomainSpikeAnalysisResult.from_detection_results(\n                    spike_indices_per_channel=spike_indices_per_channel,\n                    mne_raw_with_annotations=mne_raw_with_annotations,\n                    detection_params=detection_params or {},\n                    animal_id=self.animal_id,\n                    genotype=self.genotype,\n                    animal_day=self.animaldays[i],\n                    bin_folder_name=self.bin_folder_names[i],\n                    metadata=self.long_recordings[i].meta,\n                    assume_from_number=self.assume_from_number,\n                )\n\n                fdsar_list.append(fdsar)\n\n                # Log results\n                total_spikes = sum(len(spikes) for spikes in spike_indices_per_channel)\n                logging.info(f\"Recording {i+1}/{len(recs)}: Detected {total_spikes} spikes across {len(spike_indices_per_channel)} channels\")\n\n            except Exception as e:\n                logging.error(f\"Error processing recording {i+1}/{len(recs)}: {e}\")\n                raise\n\n        # Store results for later access\n        self.frequency_domain_spike_analysis_results = fdsar_list\n\n        logging.info(f\"Completed frequency-domain spike detection. Total recordings processed: {len(fdsar_list)}\")\n        return fdsar_list\n\n    def _process_fragment_serial(self, idx, features, lan: core.LongRecordingAnalyzer, window_s, kwargs: dict):\n        row = self._process_fragment_metadata(idx, lan, window_s)\n        row.update(self._process_fragment_features(idx, features, lan, kwargs))\n        return row\n\n    def _process_fragment_metadata(self, idx, lan: core.LongRecordingAnalyzer, window_s):\n        row = {}\n\n        lan_folder = lan.LongRecording.base_folder_path\n        animalday_dict = core.parse_path_to_animalday(\n            lan_folder, animal_param=self.animal_param, day_sep=self.day_sep, mode=self.read_mode\n        )\n        row[\"animalday\"] = animalday_dict[\"animalday\"]\n        row[\"animal\"] = animalday_dict[\"animal\"]\n        row[\"day\"] = animalday_dict[\"day\"]\n        row[\"genotype\"] = animalday_dict[\"genotype\"]\n        row[\"duration\"] = lan.LongRecording.get_dur_fragment(window_s, idx)\n        row[\"endfile\"] = lan.get_file_end(idx)\n\n        frag_dt = lan.LongRecording.get_datetime_fragment(window_s, idx)\n        row[\"timestamp\"] = frag_dt\n        row[\"isday\"] = core.utils.is_day(frag_dt)\n\n        return row\n\n    def _process_fragment_features(self, idx, features, lan: core.LongRecordingAnalyzer, kwargs: dict):\n        row = {}\n        for feat in features:\n            func = getattr(lan, f\"compute_{feat}\")\n            if callable(func):\n                row[feat] = func(idx, **kwargs)\n            else:\n                raise AttributeError(f\"Invalid function {func}\")\n        return row\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.AnimalOrganizer.__init__","title":"<code>__init__(base_folder_path, anim_id, day_sep=None, mode='concat', assume_from_number=False, skip_days=[], truncate=False, lro_kwargs={})</code>","text":"<p>AnimalOrganizer is used to organize data from a single animal into a format that can be used for analysis. It is used to organize data from a single animal into a format that can be used for analysis.</p> <p>Parameters:</p> Name Type Description Default <code>base_folder_path</code> <code>str</code> <p>The path to the base folder of the animal data.</p> required <code>anim_id</code> <code>str</code> <p>The ID of the animal. This should correspond to only one animal.</p> required <code>day_sep</code> <code>str</code> <p>Separator for day in folder name. Set to None or empty string to get all folders. Defaults to None.</p> <code>None</code> <code>mode</code> <code>Literal['nest', 'concat', 'base', 'noday']</code> <p>The mode of the AnimalOrganizer. Defaults to \"concat\". File structure patterns (where * indicates search location): \"nest\": base_folder_path / animal_id / date_format (looks for folders/files within animal_id subdirectories) \"concat\": base_folder_path / animal_iddate_format (looks for folders/files with animal_id+date in name at base level) \"base\": base_folder_path / * (looks for folders/files directly in base_folder_path) \"noday\": base_folder_path / animal_id* (same as concat but expects single unique match, no date filtering)</p> <code>'concat'</code> <code>assume_from_number</code> <code>bool</code> <p>Whether to assume the animal ID is a number. Defaults to False.</p> <code>False</code> <code>skip_days</code> <code>list[str]</code> <p>The days to skip. Defaults to [].</p> <code>[]</code> <code>truncate</code> <code>bool | int</code> <p>Whether to truncate the data. Defaults to False.</p> <code>False</code> <code>lro_kwargs</code> <code>dict</code> <p>Keyword arguments for LongRecordingOrganizer. Defaults to {}.</p> <code>{}</code> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def __init__(\n    self,\n    base_folder_path,\n    anim_id: str,\n    day_sep: str | None = None,\n    mode: Literal[\"nest\", \"concat\", \"base\", \"noday\"] = \"concat\",\n    assume_from_number=False,\n    skip_days: list[str] = [],\n    truncate: bool | int = False,\n    lro_kwargs: dict = {},\n) -&gt; None:\n    \"\"\"\n    AnimalOrganizer is used to organize data from a single animal into a format that can be used for analysis.\n    It is used to organize data from a single animal into a format that can be used for analysis.\n\n    Args:\n        base_folder_path (str): The path to the base folder of the animal data.\n        anim_id (str): The ID of the animal. This should correspond to only one animal.\n        day_sep (str, optional): Separator for day in folder name. Set to None or empty string to get all folders. Defaults to None.\n        mode (Literal[\"nest\", \"concat\", \"base\", \"noday\"], optional): The mode of the AnimalOrganizer. Defaults to \"concat\".\n            File structure patterns (where * indicates search location):\n            \"nest\": base_folder_path / animal_id / *date_format* (looks for folders/files within animal_id subdirectories)\n            \"concat\": base_folder_path / *animal_id*date_format* (looks for folders/files with animal_id+date in name at base level)\n            \"base\": base_folder_path / * (looks for folders/files directly in base_folder_path)\n            \"noday\": base_folder_path / *animal_id* (same as concat but expects single unique match, no date filtering)\n        assume_from_number (bool, optional): Whether to assume the animal ID is a number. Defaults to False.\n        skip_days (list[str], optional): The days to skip. Defaults to [].\n        truncate (bool|int, optional): Whether to truncate the data. Defaults to False.\n        lro_kwargs (dict, optional): Keyword arguments for LongRecordingOrganizer. Defaults to {}.\n    \"\"\"\n\n    self.base_folder_path = Path(base_folder_path)\n    self.anim_id = anim_id\n    self.animal_param = [anim_id]\n    self.day_sep = day_sep\n    self.read_mode = mode\n    self.assume_from_number = assume_from_number\n\n    match mode:\n        case \"nest\":\n            self.bin_folder_pattern = self.base_folder_path / f\"*{self.anim_id}*\" / \"*\"\n        case \"concat\" | \"noday\":\n            self.bin_folder_pattern = self.base_folder_path / f\"*{self.anim_id}*\"\n            # self.bin_folder_pat = self.base_folder_path / f\"*{self.anim_id}*{self.date_format}*\"\n        case \"base\":\n            self.bin_folder_pattern = self.base_folder_path\n        # case 'noday':\n        #     self.bin_folder_pat = self.base_folder_path / f\"*{self.anim_id}*\"\n        case _:\n            raise ValueError(f\"Invalid mode: {mode}\")\n\n    self._bin_folders = glob.glob(str(self.bin_folder_pattern))\n\n    # Filter to only include directories (LongRecordingOrganizer expects folder paths)\n    before_filter_count = len(self._bin_folders)\n    self._bin_folders = [x for x in self._bin_folders if Path(x).is_dir()]\n    after_filter_count = len(self._bin_folders)\n\n    if before_filter_count &gt; after_filter_count:\n        filtered_count = before_filter_count - after_filter_count\n        logging.info(f\"Filtered out {filtered_count} non-directory items (files) from glob results\")\n\n    # if mode != 'noday':\n    #     self.__bin_folders = [x for x in self.__bin_folders if datetime.strptime(Path(x).name, self.date_format)]\n    truncate = core.utils.parse_truncate(truncate)\n    if truncate:\n        warnings.warn(f\"AnimalOrganizer will be truncated to the first {truncate} LongRecordings\")\n        self._bin_folders = self._bin_folders[:truncate]\n    self._bin_folders = [x for x in self._bin_folders if not any(y in x for y in skip_days)]\n    self.bin_folder_names = [Path(x).name for x in self._bin_folders]\n    logging.info(f\"bin_folder_pattern: {self.bin_folder_pattern}\")\n    logging.info(f\"self._bin_folders: {self._bin_folders}\")\n    logging.info(f\"self.bin_folder_names: {self.bin_folder_names}\")\n\n    if mode == \"noday\" and len(self._bin_folders) &gt; 1:\n        raise ValueError(f\"Animal ID '{self.anim_id}' is not unique, found: {', '.join(self._bin_folders)}\")\n    elif len(self._bin_folders) == 0:\n        raise ValueError(f\"No directories found for animal ID {self.anim_id} (pattern: {self.bin_folder_pattern})\")\n\n    self._animalday_dicts = [\n        core.parse_path_to_animalday(e, animal_param=self.animal_param, day_sep=self.day_sep, mode=self.read_mode)\n        for e in self._bin_folders\n    ]\n\n    # Group folders by parsed animalday to handle overlapping days\n    animalday_to_folders = {}\n    for folder, animalday_dict in zip(self._bin_folders, self._animalday_dicts):\n        animalday = animalday_dict[\"animalday\"]\n        if animalday not in animalday_to_folders:\n            animalday_to_folders[animalday] = []\n        animalday_to_folders[animalday].append(folder)\n\n    # Store grouping info\n    self._animalday_folder_groups = animalday_to_folders\n    self.unique_animaldays = list(animalday_to_folders.keys())\n\n    # Log merging operations for overlapping days\n    overlapping_days = 0\n    for animalday, folders in animalday_to_folders.items():\n        if len(folders) &gt; 1:\n            overlapping_days += 1\n            logging.info(f\"Merging {len(folders)} folders for {animalday}: {[Path(f).name for f in folders]}\")\n\n    if overlapping_days &gt; 0:\n        logging.info(f\"Found {overlapping_days} animaldays with overlapping folders\")\n\n    # Update animaldays to reflect unique days (not total folders)\n    self.animaldays = self.unique_animaldays\n    logging.info(f\"self.animaldays (unique): {self.animaldays}\")\n\n    genotypes = [x[\"genotype\"] for x in self._animalday_dicts]\n    if len(set(genotypes)) &gt; 1:\n        warnings.warn(f\"Inconsistent genotypes in {genotypes}\")\n    self.genotype = genotypes[0]\n    logging.info(f\"self.genotype: {self.genotype}\")\n\n    self.long_analyzers: list[core.LongRecordingAnalyzer] = []\n    logging.debug(f\"Creating {len(self.unique_animaldays)} LongRecordings (one per unique animalday)\")\n\n    # Process manual_datetimes if provided in lro_kwargs\n    if \"manual_datetimes\" in lro_kwargs:\n        logging.info(\"Processing manual_datetimes configuration\")\n        base_lro_kwargs = lro_kwargs.copy()\n        base_lro_kwargs[\"manual_datetimes\"] = datetime(2000, 1, 1, 0, 0, 0)\n\n        self._processed_timestamps = self._process_all_timestamps(\n            lro_kwargs[\"manual_datetimes\"], self._animalday_folder_groups, base_lro_kwargs\n        )\n        # Remove from lro_kwargs since we'll handle it manually\n        lro_kwargs = base_lro_kwargs\n    else:\n        self._processed_timestamps = None\n\n    # Create LongRecordingOrganizer instances\n    self._create_long_recordings(lro_kwargs)\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.AnimalOrganizer.apply_lof_threshold","title":"<code>apply_lof_threshold(lof_threshold)</code>","text":"<p>Apply threshold to existing LOF scores to determine bad channels for all recordings.</p> <p>Parameters:</p> Name Type Description Default <code>lof_threshold</code> <code>float</code> <p>Threshold for determining bad channels.</p> required Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def apply_lof_threshold(self, lof_threshold: float):\n    \"\"\"Apply threshold to existing LOF scores to determine bad channels for all recordings.\n\n    Args:\n        lof_threshold (float): Threshold for determining bad channels.\n    \"\"\"\n    for lrec in self.long_recordings:\n        lrec.apply_lof_threshold(lof_threshold)\n\n    self.bad_channels_dict = {\n        animalday: lrec.bad_channel_names for animalday, lrec in zip(self.animaldays, self.long_recordings)\n    }\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.AnimalOrganizer.compute_bad_channels","title":"<code>compute_bad_channels(lof_threshold=None, force_recompute=False)</code>","text":"<p>Compute bad channels using LOF analysis for all recordings.</p> <p>Parameters:</p> Name Type Description Default <code>lof_threshold</code> <code>float</code> <p>Threshold for determining bad channels from LOF scores.                            If None, only computes/loads scores without setting bad_channel_names.</p> <code>None</code> <code>force_recompute</code> <code>bool</code> <p>Whether to recompute LOF scores even if they exist.</p> <code>False</code> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def compute_bad_channels(self, lof_threshold: float = None, force_recompute: bool = False):\n    \"\"\"Compute bad channels using LOF analysis for all recordings.\n\n    Args:\n        lof_threshold (float, optional): Threshold for determining bad channels from LOF scores.\n                                       If None, only computes/loads scores without setting bad_channel_names.\n        force_recompute (bool): Whether to recompute LOF scores even if they exist.\n    \"\"\"\n    logging.info(\n        f\"Computing bad channels for {len(self.long_recordings)} recordings with threshold={lof_threshold}\"\n    )\n    for i, lrec in enumerate(self.long_recordings):\n        logging.debug(f\"Computing bad channels for recording {i}: {self.animaldays[i]}\")\n        lrec.compute_bad_channels(lof_threshold=lof_threshold, force_recompute=force_recompute)\n        logging.debug(\n            f\"Recording {i} LOF scores computed: {hasattr(lrec, 'lof_scores') and lrec.lof_scores is not None}\"\n        )\n\n    # Update bad channels dict if threshold was applied\n    if lof_threshold is not None:\n        self.bad_channels_dict = {\n            animalday: lrec.bad_channel_names for animalday, lrec in zip(self.animaldays, self.long_recordings)\n        }\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.AnimalOrganizer.compute_frequency_domain_spike_analysis","title":"<code>compute_frequency_domain_spike_analysis(detection_params=None, max_length=None, multiprocess_mode='serial')</code>","text":"<p>Compute frequency-domain spike detection on all long recordings.</p> <p>Parameters:</p> Name Type Description Default <code>detection_params</code> <code>dict</code> <p>Detection parameters. Uses defaults if None.</p> <code>None</code> <code>max_length</code> <code>int</code> <p>Maximum length in samples to analyze per recording</p> <code>None</code> <code>multiprocess_mode</code> <code>Literal['dask', 'serial']</code> <p>Processing mode</p> <code>'serial'</code> <p>Returns:</p> Type Description <p>list[FrequencyDomainSpikeAnalysisResult]: Results for each recording session</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If SpikeInterface is not available</p> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def compute_frequency_domain_spike_analysis(\n    self,\n    detection_params: dict = None,\n    max_length: int = None,\n    multiprocess_mode: Literal[\"dask\", \"serial\"] = \"serial\"\n):\n    \"\"\"\n    Compute frequency-domain spike detection on all long recordings.\n\n    Args:\n        detection_params (dict, optional): Detection parameters. Uses defaults if None.\n        max_length (int, optional): Maximum length in samples to analyze per recording\n        multiprocess_mode (Literal[\"dask\", \"serial\"]): Processing mode\n\n    Returns:\n        list[FrequencyDomainSpikeAnalysisResult]: Results for each recording session\n\n    Raises:\n        ImportError: If SpikeInterface is not available\n    \"\"\"\n    # Import here to avoid circular imports\n    from .frequency_domain_results import FrequencyDomainSpikeAnalysisResult\n\n    fdsar_list = []\n    recs = [lrec.LongRecording for lrec in self.long_recordings]\n\n    logging.info(f\"Running frequency-domain spike detection on {len(recs)} recordings\")\n    logging.info(f\"Detection parameters: {detection_params}\")\n\n    for i, rec in enumerate(recs):\n        if rec.get_total_samples() == 0:\n            logging.warning(f\"Skipping {rec} because it has no samples\")\n            continue\n\n        try:\n            # Run frequency domain spike detection\n            spike_indices_per_channel, mne_raw_with_annotations = (\n                FrequencyDomainSpikeDetector.detect_spikes_recording(\n                    rec,\n                    detection_params=detection_params,\n                    max_length=max_length,\n                    multiprocess_mode=multiprocess_mode\n                )\n            )\n\n            # Create FrequencyDomainSpikeAnalysisResult\n            fdsar = FrequencyDomainSpikeAnalysisResult.from_detection_results(\n                spike_indices_per_channel=spike_indices_per_channel,\n                mne_raw_with_annotations=mne_raw_with_annotations,\n                detection_params=detection_params or {},\n                animal_id=self.animal_id,\n                genotype=self.genotype,\n                animal_day=self.animaldays[i],\n                bin_folder_name=self.bin_folder_names[i],\n                metadata=self.long_recordings[i].meta,\n                assume_from_number=self.assume_from_number,\n            )\n\n            fdsar_list.append(fdsar)\n\n            # Log results\n            total_spikes = sum(len(spikes) for spikes in spike_indices_per_channel)\n            logging.info(f\"Recording {i+1}/{len(recs)}: Detected {total_spikes} spikes across {len(spike_indices_per_channel)} channels\")\n\n        except Exception as e:\n            logging.error(f\"Error processing recording {i+1}/{len(recs)}: {e}\")\n            raise\n\n    # Store results for later access\n    self.frequency_domain_spike_analysis_results = fdsar_list\n\n    logging.info(f\"Completed frequency-domain spike detection. Total recordings processed: {len(fdsar_list)}\")\n    return fdsar_list\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.AnimalOrganizer.compute_spike_analysis","title":"<code>compute_spike_analysis(multiprocess_mode='serial')</code>","text":"<p>Compute spike sorting on all long recordings and return a list of SpikeAnalysisResult objects</p> <p>Parameters:</p> Name Type Description Default <code>multiprocess_mode</code> <code>Literal['dask', 'serial']</code> <p>Whether to use Dask for parallel processing. Defaults to 'serial'.</p> <code>'serial'</code> <p>Returns:</p> Name Type Description <code>spike_analysis_results</code> <p>list[SpikeAnalysisResult]. Each SpikeAnalysisResult object corresponds to a LongRecording object,</p> <p>typically a different day or recording session.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If mountainsort5 is not available.</p> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def compute_spike_analysis(self, multiprocess_mode: Literal[\"dask\", \"serial\"] = \"serial\"):\n    \"\"\"Compute spike sorting on all long recordings and return a list of SpikeAnalysisResult objects\n\n    Args:\n        multiprocess_mode (Literal['dask', 'serial']): Whether to use Dask for parallel processing. Defaults to 'serial'.\n\n    Returns:\n        spike_analysis_results: list[SpikeAnalysisResult]. Each SpikeAnalysisResult object corresponds to a LongRecording object,\n        typically a different day or recording session.\n\n    Raises:\n        ImportError: If mountainsort5 is not available.\n    \"\"\"\n    # Check if mountainsort5 is available\n    if not MOUNTAINSORT_AVAILABLE:\n        raise ImportError(\"Spike analysis requires mountainsort5. Install it with: pip install mountainsort5\")\n    sars = []\n    lrec_sorts = []\n    lrec_recs = []\n    recs = [lrec.LongRecording for lrec in self.long_recordings]\n    logging.info(f\"Sorting {len(recs)} recordings\")\n    for rec in recs:\n        if rec.get_total_samples() == 0:\n            logging.warning(f\"Skipping {rec.__str__()} because it has no samples\")\n            sortings, recordings = [], []\n        else:\n            sortings, recordings = core.MountainSortAnalyzer.sort_recording(\n                rec, multiprocess_mode=multiprocess_mode\n            )\n        lrec_sorts.append(sortings)\n        lrec_recs.append(recordings)\n\n    if multiprocess_mode == \"dask\":\n        lrec_sorts = dask.compute(*lrec_sorts)\n\n    lrec_sas = [\n        [\n            si.create_sorting_analyzer(sorting, recording, sparse=False)\n            for sorting, recording in zip(sortings, recordings)\n        ]\n        for sortings, recordings in zip(lrec_sorts, lrec_recs)\n    ]\n    sars = [\n        SpikeAnalysisResult(\n            result_sas=sas,\n            result_mne=None,\n            animal_id=self.animal_id,\n            genotype=self.genotype,\n            animal_day=self.animaldays[i],\n            bin_folder_name=self.bin_folder_names[i],\n            metadata=self.long_recordings[i].meta,\n            channel_names=self.channel_names,\n            assume_from_number=self.assume_from_number,\n        )\n        for i, sas in enumerate(lrec_sas)\n    ]\n\n    self.spike_analysis_results = sars\n    return self.spike_analysis_results\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.AnimalOrganizer.compute_windowed_analysis","title":"<code>compute_windowed_analysis(features, exclude=[], window_s=4, multiprocess_mode='serial', suppress_short_interval_error=False, apply_notch_filter=True, **kwargs)</code>","text":"<p>Computes windowed analysis of animal recordings. The data is divided into windows (time bins), then features are extracted from each window. The result is formatted to a Dataframe and wrapped into a WindowAnalysisResult object.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>list[str]</code> <p>List of features to compute. See individual compute_...() functions for output format</p> required <code>exclude</code> <code>list[str]</code> <p>List of features to ignore. Will override the features parameter. Defaults to [].</p> <code>[]</code> <code>window_s</code> <code>int</code> <p>Length of each window in seconds. Note that some features break with very short window times. Defaults to 4.</p> <code>4</code> <code>suppress_short_interval_error</code> <code>bool</code> <p>If True, suppress ValueError for short intervals between timestamps in resulting WindowAnalysisResult. Useful for aggregated WARs. Defaults to False.</p> <code>False</code> <code>apply_notch_filter</code> <code>bool</code> <p>Whether to apply notch filtering to remove line noise. Uses constants.LINE_FREQ. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If a feature's compute_...() function was not implemented, this error will be raised.</p> <p>Returns:</p> Name Type Description <code>window_analysis_result</code> <p>a WindowAnalysisResult object</p> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def compute_windowed_analysis(\n    self,\n    features: list[str],\n    exclude: list[str] = [],\n    window_s=4,\n    multiprocess_mode: Literal[\"dask\", \"serial\"] = \"serial\",\n    suppress_short_interval_error=False,\n    apply_notch_filter=True,\n    **kwargs,\n):\n    \"\"\"Computes windowed analysis of animal recordings. The data is divided into windows (time bins), then features are extracted from each window. The result is\n    formatted to a Dataframe and wrapped into a WindowAnalysisResult object.\n\n    Args:\n        features (list[str]): List of features to compute. See individual compute_...() functions for output format\n        exclude (list[str], optional): List of features to ignore. Will override the features parameter. Defaults to [].\n        window_s (int, optional): Length of each window in seconds. Note that some features break with very short window times. Defaults to 4.\n        suppress_short_interval_error (bool, optional): If True, suppress ValueError for short intervals between timestamps in resulting WindowAnalysisResult. Useful for aggregated WARs. Defaults to False.\n        apply_notch_filter (bool, optional): Whether to apply notch filtering to remove line noise. Uses constants.LINE_FREQ. Defaults to True.\n\n    Raises:\n        AttributeError: If a feature's compute_...() function was not implemented, this error will be raised.\n\n    Returns:\n        window_analysis_result: a WindowAnalysisResult object\n    \"\"\"\n    features = _sanitize_feature_request(features, exclude)\n\n    dataframes = []\n    for lrec in self.long_recordings:  # Iterate over all long recordings\n        logging.info(f\"Computing windowed analysis for {lrec.base_folder_path}\")\n        lan = core.LongRecordingAnalyzer(lrec, fragment_len_s=window_s, apply_notch_filter=apply_notch_filter)\n        if lan.n_fragments == 0:\n            logging.warning(f\"No fragments found for {lrec.base_folder_path}. Skipping.\")\n            continue\n\n        logging.debug(f\"Processing {lan.n_fragments} fragments\")\n        miniters = int(lan.n_fragments / 100)\n        match multiprocess_mode:\n            case \"dask\":\n                # The last fragment is not included because it makes the dask array ragged\n                logging.debug(\"Converting LongRecording to numpy array\")\n\n                n_fragments_war = max(lan.n_fragments - 1, 1)\n                first_fragment = lan.get_fragment_np(0)\n                np_fragments = np.empty((n_fragments_war,) + first_fragment.shape, dtype=first_fragment.dtype)\n                logging.debug(f\"np_fragments.shape: {np_fragments.shape}\")\n                for idx in range(n_fragments_war):\n                    np_fragments[idx] = lan.get_fragment_np(idx)\n\n                # Cache fragments to zarr\n                tmppath, _ = core.utils.cache_fragments_to_zarr(np_fragments, n_fragments_war)\n                del np_fragments\n\n                logging.debug(\"Processing metadata serially\")\n                metadatas = [self._process_fragment_metadata(idx, lan, window_s) for idx in range(n_fragments_war)]\n                meta_df = pd.DataFrame(metadatas)\n\n                logging.debug(\"Processing features in parallel\")\n                np_fragments_reconstruct = da.from_zarr(tmppath, chunks=(\"auto\", -1, -1))\n                logging.debug(f\"Dask array shape: {np_fragments_reconstruct.shape}\")\n                logging.debug(f\"Dask array chunks: {np_fragments_reconstruct.chunks}\")\n\n                # Create delayed tasks for each fragment using efficient dependency resolution\n                feature_values = [\n                    delayed(FragmentAnalyzer.process_fragment_with_dependencies)(\n                        np_fragments_reconstruct[idx], lan.f_s, features, kwargs\n                    )\n                    for idx in range(n_fragments_war)\n                ]\n\n                # Compute features in parallel\n                feature_values = dask.compute(*feature_values)\n\n                # Clean up temp directory after processing\n                logging.debug(\"Cleaning up temp directory\")\n                try:\n                    import shutil\n\n                    shutil.rmtree(tmppath)\n                except (OSError, FileNotFoundError) as e:\n                    logging.warning(f\"Failed to remove temporary directory {tmppath}: {e}\")\n\n                logging.debug(\"Combining metadata and feature values\")\n                feat_df = pd.DataFrame(feature_values)\n                lan_df = pd.concat([meta_df, feat_df], axis=1)\n\n            case _:\n                logging.debug(\"Processing serially\")\n                lan_df = []\n                for idx in tqdm(range(lan.n_fragments), desc=\"Processing rows\", miniters=miniters):\n                    lan_df.append(self._process_fragment_serial(idx, features, lan, window_s, kwargs))\n\n        lan_df = pd.DataFrame(lan_df)\n\n        logging.debug(\"Validating timestamps\")\n        core.validate_timestamps(lan_df[\"timestamp\"].tolist())\n        lan_df = lan_df.sort_values(\"timestamp\").reset_index(drop=True)\n\n        self.long_analyzers.append(lan)\n        dataframes.append(lan_df)\n\n    self.features_df = pd.concat(dataframes)\n    self.features_df = self.features_df\n\n    # Collect LOF scores from long recordings\n    lof_scores_dict = {}\n    for animalday, lrec in zip(self.animaldays, self.long_recordings):\n        logging.debug(\n            f\"Checking LOF scores for {animalday}: has_attr={hasattr(lrec, 'lof_scores')}, \"\n            f\"is_not_none={getattr(lrec, 'lof_scores', None) is not None}\"\n        )\n        if hasattr(lrec, \"lof_scores\") and lrec.lof_scores is not None:\n            lof_scores_dict[animalday] = {\n                \"lof_scores\": lrec.lof_scores.tolist(),\n                \"channel_names\": lrec.channel_names,\n            }\n            logging.info(f\"Added LOF scores for {animalday}: {len(lrec.lof_scores)} channels\")\n\n    logging.info(f\"Total LOF scores collected: {len(lof_scores_dict)} animal days\")\n\n    self.window_analysis_result = WindowAnalysisResult(\n        self.features_df,\n        self.animal_id,\n        self.genotype,\n        self.channel_names,\n        self.assume_from_number,\n        self.bad_channels_dict,\n        suppress_short_interval_error,\n        lof_scores_dict,\n    )\n\n    return self.window_analysis_result\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.AnimalOrganizer.get_all_lof_scores","title":"<code>get_all_lof_scores()</code>","text":"<p>Get LOF scores for all recordings.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary mapping animal days to LOF score dictionaries.</p> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def get_all_lof_scores(self) -&gt; dict:\n    \"\"\"Get LOF scores for all recordings.\n\n    Returns:\n        dict: Dictionary mapping animal days to LOF score dictionaries.\n    \"\"\"\n    return {animalday: lrec.get_lof_scores() for animalday, lrec in zip(self.animaldays, self.long_recordings)}\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.AnimalOrganizer.get_timeline_summary","title":"<code>get_timeline_summary()</code>","text":"<p>Get timeline summary as a DataFrame for user inspection and debugging.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Timeline information with columns: - lro_index: Index of the LRO - start_time: Start datetime of the LRO - end_time: End datetime of the LRO - duration_s: Duration in seconds - n_files: Number of files in the LRO - folder_path: Base folder path - animalday: Parsed animalday identifier</p> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def get_timeline_summary(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Get timeline summary as a DataFrame for user inspection and debugging.\n\n    Returns:\n        pd.DataFrame: Timeline information with columns:\n            - lro_index: Index of the LRO\n            - start_time: Start datetime of the LRO\n            - end_time: End datetime of the LRO\n            - duration_s: Duration in seconds\n            - n_files: Number of files in the LRO\n            - folder_path: Base folder path\n            - animalday: Parsed animalday identifier\n    \"\"\"\n    if not self.long_recordings:\n        return pd.DataFrame()\n\n    timeline_data = []\n    for i, lro in enumerate(self.long_recordings):\n        try:\n            start_time = self._get_lro_start_time(lro)\n            end_time = self._get_lro_end_time(lro)\n            duration = (\n                lro.LongRecording.get_duration() if hasattr(lro, \"LongRecording\") and lro.LongRecording else 0\n            )\n            n_files = len(lro.file_durations) if hasattr(lro, \"file_durations\") and lro.file_durations else 1\n            folder_path = getattr(lro, \"base_folder_path\", \"unknown\")\n\n            timeline_data.append(\n                {\n                    \"lro_index\": i,\n                    \"start_time\": start_time,\n                    \"end_time\": end_time,\n                    \"duration_s\": duration,\n                    \"n_files\": n_files,\n                    \"folder_path\": str(folder_path),\n                    \"folder_name\": Path(folder_path).name if folder_path != \"unknown\" else \"unknown\",\n                    \"animalday\": getattr(\n                        lro, \"_animalday\", \"unknown\"\n                    ),  # This might not exist, but useful if it does\n                }\n            )\n        except Exception as e:\n            # Include failed LROs in the summary for debugging\n            timeline_data.append(\n                {\n                    \"lro_index\": i,\n                    \"start_time\": \"error\",\n                    \"end_time\": \"error\",\n                    \"duration_s\": 0,\n                    \"n_files\": 0,\n                    \"folder_path\": \"error\",\n                    \"folder_name\": \"error\",\n                    \"animalday\": \"error\",\n                    \"error\": str(e),\n                }\n            )\n\n    return pd.DataFrame(timeline_data)\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.SpikeAnalysisResult","title":"<code>SpikeAnalysisResult</code>","text":"<p>               Bases: <code>AnimalFeatureParser</code></p> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>class SpikeAnalysisResult(AnimalFeatureParser):\n    def __init__(\n        self,\n        result_sas: list[si.SortingAnalyzer],\n        result_mne: mne.io.RawArray = None,\n        animal_id: str = None,\n        genotype: str = None,\n        animal_day: str = None,\n        bin_folder_name: str = None,\n        metadata: core.DDFBinaryMetadata = None,\n        channel_names: list[str] = None,\n        assume_from_number=False,\n    ) -&gt; None:\n        \"\"\"\n        Args:\n            result (list[si.SortingAnalyzer]): Result comes from AnimalOrganizer.compute_spike_analysis(). Each SortingAnalyzer is a single channel.\n            animal_id (str, optional): Identifier for the animal where result was computed from. Defaults to None.\n            genotype (str, optional): Genotype of animal. Defaults to None.\n            channel_names (list[str], optional): List of channel names. Defaults to None.\n            assume_channels (bool, optional): If true, assumes channel names according to AnimalFeatureParser.DEFAULT_CHNUM_TO_NAME. Defaults to False.\n        \"\"\"\n        self.result_sas = result_sas\n        self.result_mne = result_mne\n        if (result_mne is None) == (result_sas is None):\n            raise ValueError(\"Exactly one of result_sas or result_mne must be provided\")\n        self.animal_id = animal_id\n        self.genotype = genotype\n        self.animal_day = animal_day\n        self.bin_folder_name = bin_folder_name\n        self.metadata = metadata\n        self.channel_names = channel_names\n        self.assume_from_number = assume_from_number\n        self.channel_abbrevs = [\n            core.parse_chname_to_abbrev(x, assume_from_number=assume_from_number) for x in self.channel_names\n        ]\n\n        logging.info(f\"Channel names: \\t{self.channel_names}\")\n        logging.info(f\"Channel abbreviations: \\t{self.channel_abbrevs}\")\n\n    def convert_to_mne(self, chunk_len: float = 60, save_raw=True) -&gt; mne.io.RawArray:\n        if self.result_mne is None:\n            result_mne = SpikeAnalysisResult.convert_sas_to_mne(self.result_sas, chunk_len)\n            if save_raw:\n                self.result_mne = result_mne\n            else:\n                return result_mne\n        return self.result_mne\n\n    def save_fif_and_json(\n        self,\n        folder: str | Path,\n        convert_to_mne=True,\n        make_folder=True,\n        slugify_filebase=True,\n        save_abbrevs_as_chnames=False,\n        overwrite=False,\n    ):\n        \"\"\"Archive spike analysis result into the folder specified, as a fif and json file.\n\n        Args:\n            folder (str | Path): Destination folder to save results to\n            convert_to_mne (bool, optional): If True, convert the SortingAnalyzers to a MNE RawArray if self.result_mne is None. Defaults to True.\n            make_folder (bool, optional): If True, create the folder if it doesn't exist. Defaults to True.\n            slugify_filebase (bool, optional): If True, slugify the filebase (replace special characters). Defaults to True.\n            save_abbrevs_as_chnames (bool, optional): If True, save the channel abbreviations as the channel names in the json file. Defaults to False.\n            overwrite (bool, optional): If True, overwrite the existing files. Defaults to False.\n        \"\"\"\n        if self.result_mne is None:\n            if convert_to_mne:\n                result_mne = self.convert_to_mne(save_raw=True)\n                if result_mne is None:\n                    warnings.warn(\"No SortingAnalyzers found, skipping saving\")\n                    return\n            else:\n                raise ValueError(\"No MNE RawArray found, and convert_to_mne is False. Run convert_to_mne() first.\")\n        else:\n            result_mne = self.result_mne\n\n        folder = Path(folder)\n        if make_folder:\n            folder.mkdir(parents=True, exist_ok=True)\n\n        if slugify_filebase:\n            filebase = folder / slugify(f\"{self.animal_id}-{self.genotype}-{self.animal_day}\")\n        else:\n            filebase = folder / f\"{self.animal_id}-{self.genotype}-{self.animal_day}\"\n        filebase = str(filebase)\n\n        if not overwrite:\n            if filebase + \".json\" in folder.glob(\"*.json\"):\n                raise FileExistsError(f\"File {filebase}.json already exists\")\n            if filebase + \".fif\" in folder.glob(\"*.fif\"):\n                raise FileExistsError(f\"File {filebase}.fif already exists\")\n        else:\n            for f in folder.glob(\"*\"):\n                f.unlink()\n        result_mne.save(filebase + \"-raw.fif\", overwrite=overwrite)\n        del result_mne\n\n        json_dict = {\n            \"animal_id\": self.animal_id,\n            \"genotype\": self.genotype,\n            \"animal_day\": self.animal_day,\n            \"bin_folder_name\": self.bin_folder_name,\n            \"metadata\": self.metadata.metadata_path,\n            \"channel_names\": self.channel_abbrevs if save_abbrevs_as_chnames else self.channel_names,\n            \"assume_from_number\": False if save_abbrevs_as_chnames else self.assume_from_number,\n        }\n        with open(filebase + \".json\", \"w\") as f:\n            json.dump(json_dict, f, indent=2)\n\n    @classmethod\n    def load_fif_and_json(cls, folder: str | Path):\n        folder = Path(folder)\n        if not folder.exists():\n            raise ValueError(f\"Folder {folder} does not exist\")\n\n        fif_files = list(folder.glob(\"*.fif\"))  # there may be more than 1 fif file\n        json_files = list(folder.glob(\"*.json\"))\n\n        if len(json_files) != 1:\n            raise ValueError(f\"Expected exactly one json file in {folder}\")\n\n        fif_path = fif_files[0]\n        json_path = json_files[0]\n\n        with open(json_path, \"r\") as f:\n            data = json.load(f)\n        # data['metadata'] = core.DDFBinaryMetadata(data['metadata'])\n        data[\"result_mne\"] = mne.io.read_raw_fif(fif_path)\n        data[\"result_sas\"] = None\n        return cls(**data)\n\n    @staticmethod\n    def convert_sas_to_mne(sas: list[si.SortingAnalyzer], chunk_len: float = 60) -&gt; mne.io.RawArray:\n        \"\"\"Convert a list of SortingAnalyzers to a MNE RawArray.\n\n        Args:\n            sas (list[si.SortingAnalyzer]): The list of SortingAnalyzers to convert\n            chunk_len (float, optional): The length of the chunks to use for the conversion. Defaults to 60.\n\n        Returns:\n            mne.io.RawArray: The converted RawArray, with spikes labeled as annotations\n        \"\"\"\n        if len(sas) == 0:\n            return None\n\n        # Check that all SortingAnalyzers have the same sampling frequency\n        sfreqs = [sa.recording.get_sampling_frequency() for sa in sas]\n        if not all(sf == sfreqs[0] for sf in sfreqs):\n            raise ValueError(f\"All SortingAnalyzers must have the same sampling frequency. Got frequencies: {sfreqs}\")\n\n        # Preallocate data array\n        total_frames = int(sas[0].recording.get_duration() * sfreqs[0])\n        n_channels = len(sas)\n        data = np.empty((n_channels, total_frames))\n        logging.debug(f\"Data shape: {data.shape}\")\n\n        # Fill data array one channel at a time\n        for i, sa in enumerate(sas):\n            logging.debug(f\"Converting channel {i + 1} of {n_channels}\")\n            data[i, :] = SpikeAnalysisResult.convert_sa_to_np(sa, chunk_len)\n\n        channel_names = [str(sa.recording.get_channel_ids().item()) for sa in sas]\n        logging.debug(f\"Channel names: {channel_names}\")\n        sfreq = sfreqs[0]\n\n        # Extract spike times for each unit and create annotations\n        onset = []\n        description = []\n        for sa in sas:\n            for unit_id in sa.sorting.get_unit_ids():\n                spike_train = sa.sorting.get_unit_spike_train(unit_id)\n                # Convert to seconds and filter to recording duration\n                spike_times = spike_train / sa.sorting.get_sampling_frequency()\n                mask = spike_times &lt; sa.recording.get_duration()\n                spike_times = spike_times[mask]\n\n                # Create annotation for each spike\n                onset.extend(spike_times)\n                description.extend(\n                    [sa.recording.get_channel_ids().item()] * len(spike_times)\n                )  # collapse all units into 1 spike train\n        annotations = mne.Annotations(onset, duration=0, description=description)\n\n        info = mne.create_info(ch_names=channel_names, sfreq=sfreq, ch_types=\"eeg\")\n        raw = mne.io.RawArray(data=data, info=info)\n        raw = raw.set_annotations(annotations)\n        return raw\n\n    @staticmethod\n    def convert_sa_to_np(sa: si.SortingAnalyzer, chunk_len: float = 60) -&gt; np.ndarray:\n        \"\"\"Convert a SortingAnalyzer to an MNE RawArray.\n\n        Args:\n            sa (si.SortingAnalyzer): The SortingAnalyzer to convert. Must have only 1 channel.\n            chunk_len (float, optional): The length of the chunks to use for the conversion. Defaults to 60.\n        Returns:\n            np.ndarray: The converted traces\n        \"\"\"\n        # Check that SortingAnalyzer only has 1 channel\n        if len(sa.recording.get_channel_ids()) != 1:\n            raise ValueError(\n                f\"Expected SortingAnalyzer to have 1 channel, but got {len(sa.recording.get_channel_ids())} channels\"\n            )\n\n        rec = sa.recording\n        logging.debug(f\"Recording info: {rec}\")\n\n        # Calculate total number of frames and chunks\n        total_frames = int(rec.get_duration() * rec.get_sampling_frequency())\n        frames_per_chunk = round(chunk_len * rec.get_sampling_frequency())\n        n_chunks = total_frames // frames_per_chunk\n\n        traces = np.empty(total_frames)\n\n        for j in range(n_chunks):\n            start_frame = j * frames_per_chunk\n            if j == n_chunks - 1:\n                end_frame = total_frames\n            else:\n                end_frame = (j + 1) * frames_per_chunk\n            traces[start_frame:end_frame] = rec.get_traces(\n                start_frame=start_frame, end_frame=end_frame, return_scaled=True\n            ).flatten()\n        traces *= 1e-6  # convert from uV to V\n        return traces\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.SpikeAnalysisResult.__init__","title":"<code>__init__(result_sas, result_mne=None, animal_id=None, genotype=None, animal_day=None, bin_folder_name=None, metadata=None, channel_names=None, assume_from_number=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>result</code> <code>list[SortingAnalyzer]</code> <p>Result comes from AnimalOrganizer.compute_spike_analysis(). Each SortingAnalyzer is a single channel.</p> required <code>animal_id</code> <code>str</code> <p>Identifier for the animal where result was computed from. Defaults to None.</p> <code>None</code> <code>genotype</code> <code>str</code> <p>Genotype of animal. Defaults to None.</p> <code>None</code> <code>channel_names</code> <code>list[str]</code> <p>List of channel names. Defaults to None.</p> <code>None</code> <code>assume_channels</code> <code>bool</code> <p>If true, assumes channel names according to AnimalFeatureParser.DEFAULT_CHNUM_TO_NAME. Defaults to False.</p> required Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def __init__(\n    self,\n    result_sas: list[si.SortingAnalyzer],\n    result_mne: mne.io.RawArray = None,\n    animal_id: str = None,\n    genotype: str = None,\n    animal_day: str = None,\n    bin_folder_name: str = None,\n    metadata: core.DDFBinaryMetadata = None,\n    channel_names: list[str] = None,\n    assume_from_number=False,\n) -&gt; None:\n    \"\"\"\n    Args:\n        result (list[si.SortingAnalyzer]): Result comes from AnimalOrganizer.compute_spike_analysis(). Each SortingAnalyzer is a single channel.\n        animal_id (str, optional): Identifier for the animal where result was computed from. Defaults to None.\n        genotype (str, optional): Genotype of animal. Defaults to None.\n        channel_names (list[str], optional): List of channel names. Defaults to None.\n        assume_channels (bool, optional): If true, assumes channel names according to AnimalFeatureParser.DEFAULT_CHNUM_TO_NAME. Defaults to False.\n    \"\"\"\n    self.result_sas = result_sas\n    self.result_mne = result_mne\n    if (result_mne is None) == (result_sas is None):\n        raise ValueError(\"Exactly one of result_sas or result_mne must be provided\")\n    self.animal_id = animal_id\n    self.genotype = genotype\n    self.animal_day = animal_day\n    self.bin_folder_name = bin_folder_name\n    self.metadata = metadata\n    self.channel_names = channel_names\n    self.assume_from_number = assume_from_number\n    self.channel_abbrevs = [\n        core.parse_chname_to_abbrev(x, assume_from_number=assume_from_number) for x in self.channel_names\n    ]\n\n    logging.info(f\"Channel names: \\t{self.channel_names}\")\n    logging.info(f\"Channel abbreviations: \\t{self.channel_abbrevs}\")\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.SpikeAnalysisResult.convert_sa_to_np","title":"<code>convert_sa_to_np(sa, chunk_len=60)</code>  <code>staticmethod</code>","text":"<p>Convert a SortingAnalyzer to an MNE RawArray.</p> <p>Parameters:</p> Name Type Description Default <code>sa</code> <code>SortingAnalyzer</code> <p>The SortingAnalyzer to convert. Must have only 1 channel.</p> required <code>chunk_len</code> <code>float</code> <p>The length of the chunks to use for the conversion. Defaults to 60.</p> <code>60</code> <p>Returns:     np.ndarray: The converted traces</p> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>@staticmethod\ndef convert_sa_to_np(sa: si.SortingAnalyzer, chunk_len: float = 60) -&gt; np.ndarray:\n    \"\"\"Convert a SortingAnalyzer to an MNE RawArray.\n\n    Args:\n        sa (si.SortingAnalyzer): The SortingAnalyzer to convert. Must have only 1 channel.\n        chunk_len (float, optional): The length of the chunks to use for the conversion. Defaults to 60.\n    Returns:\n        np.ndarray: The converted traces\n    \"\"\"\n    # Check that SortingAnalyzer only has 1 channel\n    if len(sa.recording.get_channel_ids()) != 1:\n        raise ValueError(\n            f\"Expected SortingAnalyzer to have 1 channel, but got {len(sa.recording.get_channel_ids())} channels\"\n        )\n\n    rec = sa.recording\n    logging.debug(f\"Recording info: {rec}\")\n\n    # Calculate total number of frames and chunks\n    total_frames = int(rec.get_duration() * rec.get_sampling_frequency())\n    frames_per_chunk = round(chunk_len * rec.get_sampling_frequency())\n    n_chunks = total_frames // frames_per_chunk\n\n    traces = np.empty(total_frames)\n\n    for j in range(n_chunks):\n        start_frame = j * frames_per_chunk\n        if j == n_chunks - 1:\n            end_frame = total_frames\n        else:\n            end_frame = (j + 1) * frames_per_chunk\n        traces[start_frame:end_frame] = rec.get_traces(\n            start_frame=start_frame, end_frame=end_frame, return_scaled=True\n        ).flatten()\n    traces *= 1e-6  # convert from uV to V\n    return traces\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.SpikeAnalysisResult.convert_sas_to_mne","title":"<code>convert_sas_to_mne(sas, chunk_len=60)</code>  <code>staticmethod</code>","text":"<p>Convert a list of SortingAnalyzers to a MNE RawArray.</p> <p>Parameters:</p> Name Type Description Default <code>sas</code> <code>list[SortingAnalyzer]</code> <p>The list of SortingAnalyzers to convert</p> required <code>chunk_len</code> <code>float</code> <p>The length of the chunks to use for the conversion. Defaults to 60.</p> <code>60</code> <p>Returns:</p> Type Description <code>RawArray</code> <p>mne.io.RawArray: The converted RawArray, with spikes labeled as annotations</p> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>@staticmethod\ndef convert_sas_to_mne(sas: list[si.SortingAnalyzer], chunk_len: float = 60) -&gt; mne.io.RawArray:\n    \"\"\"Convert a list of SortingAnalyzers to a MNE RawArray.\n\n    Args:\n        sas (list[si.SortingAnalyzer]): The list of SortingAnalyzers to convert\n        chunk_len (float, optional): The length of the chunks to use for the conversion. Defaults to 60.\n\n    Returns:\n        mne.io.RawArray: The converted RawArray, with spikes labeled as annotations\n    \"\"\"\n    if len(sas) == 0:\n        return None\n\n    # Check that all SortingAnalyzers have the same sampling frequency\n    sfreqs = [sa.recording.get_sampling_frequency() for sa in sas]\n    if not all(sf == sfreqs[0] for sf in sfreqs):\n        raise ValueError(f\"All SortingAnalyzers must have the same sampling frequency. Got frequencies: {sfreqs}\")\n\n    # Preallocate data array\n    total_frames = int(sas[0].recording.get_duration() * sfreqs[0])\n    n_channels = len(sas)\n    data = np.empty((n_channels, total_frames))\n    logging.debug(f\"Data shape: {data.shape}\")\n\n    # Fill data array one channel at a time\n    for i, sa in enumerate(sas):\n        logging.debug(f\"Converting channel {i + 1} of {n_channels}\")\n        data[i, :] = SpikeAnalysisResult.convert_sa_to_np(sa, chunk_len)\n\n    channel_names = [str(sa.recording.get_channel_ids().item()) for sa in sas]\n    logging.debug(f\"Channel names: {channel_names}\")\n    sfreq = sfreqs[0]\n\n    # Extract spike times for each unit and create annotations\n    onset = []\n    description = []\n    for sa in sas:\n        for unit_id in sa.sorting.get_unit_ids():\n            spike_train = sa.sorting.get_unit_spike_train(unit_id)\n            # Convert to seconds and filter to recording duration\n            spike_times = spike_train / sa.sorting.get_sampling_frequency()\n            mask = spike_times &lt; sa.recording.get_duration()\n            spike_times = spike_times[mask]\n\n            # Create annotation for each spike\n            onset.extend(spike_times)\n            description.extend(\n                [sa.recording.get_channel_ids().item()] * len(spike_times)\n            )  # collapse all units into 1 spike train\n    annotations = mne.Annotations(onset, duration=0, description=description)\n\n    info = mne.create_info(ch_names=channel_names, sfreq=sfreq, ch_types=\"eeg\")\n    raw = mne.io.RawArray(data=data, info=info)\n    raw = raw.set_annotations(annotations)\n    return raw\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.SpikeAnalysisResult.save_fif_and_json","title":"<code>save_fif_and_json(folder, convert_to_mne=True, make_folder=True, slugify_filebase=True, save_abbrevs_as_chnames=False, overwrite=False)</code>","text":"<p>Archive spike analysis result into the folder specified, as a fif and json file.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str | Path</code> <p>Destination folder to save results to</p> required <code>convert_to_mne</code> <code>bool</code> <p>If True, convert the SortingAnalyzers to a MNE RawArray if self.result_mne is None. Defaults to True.</p> <code>True</code> <code>make_folder</code> <code>bool</code> <p>If True, create the folder if it doesn't exist. Defaults to True.</p> <code>True</code> <code>slugify_filebase</code> <code>bool</code> <p>If True, slugify the filebase (replace special characters). Defaults to True.</p> <code>True</code> <code>save_abbrevs_as_chnames</code> <code>bool</code> <p>If True, save the channel abbreviations as the channel names in the json file. Defaults to False.</p> <code>False</code> <code>overwrite</code> <code>bool</code> <p>If True, overwrite the existing files. Defaults to False.</p> <code>False</code> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def save_fif_and_json(\n    self,\n    folder: str | Path,\n    convert_to_mne=True,\n    make_folder=True,\n    slugify_filebase=True,\n    save_abbrevs_as_chnames=False,\n    overwrite=False,\n):\n    \"\"\"Archive spike analysis result into the folder specified, as a fif and json file.\n\n    Args:\n        folder (str | Path): Destination folder to save results to\n        convert_to_mne (bool, optional): If True, convert the SortingAnalyzers to a MNE RawArray if self.result_mne is None. Defaults to True.\n        make_folder (bool, optional): If True, create the folder if it doesn't exist. Defaults to True.\n        slugify_filebase (bool, optional): If True, slugify the filebase (replace special characters). Defaults to True.\n        save_abbrevs_as_chnames (bool, optional): If True, save the channel abbreviations as the channel names in the json file. Defaults to False.\n        overwrite (bool, optional): If True, overwrite the existing files. Defaults to False.\n    \"\"\"\n    if self.result_mne is None:\n        if convert_to_mne:\n            result_mne = self.convert_to_mne(save_raw=True)\n            if result_mne is None:\n                warnings.warn(\"No SortingAnalyzers found, skipping saving\")\n                return\n        else:\n            raise ValueError(\"No MNE RawArray found, and convert_to_mne is False. Run convert_to_mne() first.\")\n    else:\n        result_mne = self.result_mne\n\n    folder = Path(folder)\n    if make_folder:\n        folder.mkdir(parents=True, exist_ok=True)\n\n    if slugify_filebase:\n        filebase = folder / slugify(f\"{self.animal_id}-{self.genotype}-{self.animal_day}\")\n    else:\n        filebase = folder / f\"{self.animal_id}-{self.genotype}-{self.animal_day}\"\n    filebase = str(filebase)\n\n    if not overwrite:\n        if filebase + \".json\" in folder.glob(\"*.json\"):\n            raise FileExistsError(f\"File {filebase}.json already exists\")\n        if filebase + \".fif\" in folder.glob(\"*.fif\"):\n            raise FileExistsError(f\"File {filebase}.fif already exists\")\n    else:\n        for f in folder.glob(\"*\"):\n            f.unlink()\n    result_mne.save(filebase + \"-raw.fif\", overwrite=overwrite)\n    del result_mne\n\n    json_dict = {\n        \"animal_id\": self.animal_id,\n        \"genotype\": self.genotype,\n        \"animal_day\": self.animal_day,\n        \"bin_folder_name\": self.bin_folder_name,\n        \"metadata\": self.metadata.metadata_path,\n        \"channel_names\": self.channel_abbrevs if save_abbrevs_as_chnames else self.channel_names,\n        \"assume_from_number\": False if save_abbrevs_as_chnames else self.assume_from_number,\n    }\n    with open(filebase + \".json\", \"w\") as f:\n        json.dump(json_dict, f, indent=2)\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.WindowAnalysisResult","title":"<code>WindowAnalysisResult</code>","text":"<p>               Bases: <code>AnimalFeatureParser</code></p> <p>Wrapper for output of windowed analysis. Has useful functions like group-wise and global averaging, filtering, and saving</p> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>class WindowAnalysisResult(AnimalFeatureParser):\n    \"\"\"\n    Wrapper for output of windowed analysis. Has useful functions like group-wise and global averaging, filtering, and saving\n    \"\"\"\n\n    def __init__(\n        self,\n        result: pd.DataFrame,\n        animal_id: str = None,\n        genotype: str = None,\n        channel_names: list[str] = None,\n        assume_from_number=False,\n        bad_channels_dict: dict[str, list[str]] = {},\n        suppress_short_interval_error=False,\n        lof_scores_dict: dict[str, dict] = {},\n    ) -&gt; None:\n        \"\"\"\n        Args:\n            result (pd.DataFrame): Result comes from AnimalOrganizer.compute_windowed_analysis()\n            animal_id (str, optional): Identifier for the animal where result was computed from. Defaults to None.\n            genotype (str, optional): Genotype of animal. Defaults to None.\n            channel_names (list[str], optional): List of channel names. Defaults to None.\n            assume_channels (bool, optional): If true, assumes channel names according to AnimalFeatureParser.DEFAULT_CHNUM_TO_NAME. Defaults to False.\n            bad_channels_dict (dict[str, list[str]], optional): Dictionary of channels to reject for each recording session. Defaults to {}.\n            suppress_short_interval_error (bool, optional): If True, suppress ValueError for short intervals between timestamps. Useful for aggregated WARs with large window sizes. Defaults to False.\n        \"\"\"\n        self.result = result\n        self.animal_id = animal_id\n        self.genotype = genotype\n        self.channel_names = channel_names\n        self.assume_from_number = assume_from_number\n        self.bad_channels_dict = bad_channels_dict.copy()\n        self.suppress_short_interval_error = suppress_short_interval_error\n        self.lof_scores_dict = lof_scores_dict\n\n        self.__update_instance_vars()\n\n        logging.info(f\"Channel names: \\t{self.channel_names}\")\n        logging.info(f\"Channel abbreviations: \\t{self.channel_abbrevs}\")\n\n    def __str__(self) -&gt; str:\n        return f\"{self.animaldays}\"\n\n    def __update_instance_vars(self):\n        \"\"\"Run after updating self.result, or other init values\"\"\"\n        if \"index\" in self.result.columns:\n            warnings.warn(\"Dropping column 'index'\")\n            self.result = self.result.drop(columns=[\"index\"])\n\n        # Check if timestamps are sorted and sort if needed\n        if \"timestamp\" in self.result.columns:\n            if not self.result[\"timestamp\"].is_monotonic_increasing:\n                warnings.warn(\"Timestamps are not sorted. Sorting result DataFrame by timestamp.\")\n                self.result = self.result.sort_values(\"timestamp\")\n\n        # Check for unusually short intervals between timestamps\n        if \"timestamp\" in self.result.columns and \"duration\" in self.result.columns:\n            median_duration = self.result[\"duration\"].median()\n            timestamp_diffs = self.result[\"timestamp\"].diff()\n            short_intervals = timestamp_diffs &lt; pd.Timedelta(seconds=median_duration)\n\n            # Skip first row since diff() produces NaT\n            short_intervals = short_intervals[1:]\n\n            if short_intervals.any():\n                n_short = short_intervals.sum()\n                pct_short = (n_short / len(short_intervals)) * 100\n\n                warning_msg = (\n                    f\"Found {n_short} intervals ({pct_short:.1f}%) between timestamps \"\n                    f\"that are shorter than the median duration of {median_duration:.1f}s\"\n                )\n\n                if pct_short &gt; 1.0 and not self.suppress_short_interval_error:  # More than 1% of intervals are short\n                    raise ValueError(warning_msg)\n                elif not self.suppress_short_interval_error:\n                    warnings.warn(warning_msg)\n\n        if \"animal\" in self.result.columns:\n            unique_animals = self.result[\"animal\"].unique()\n            if len(unique_animals) &gt; 1:\n                raise ValueError(f\"Multiple animals found in result: {unique_animals}\")\n            if unique_animals[0] != self.animal_id:\n                raise ValueError(\n                    f\"Animal ID mismatch: result has {unique_animals[0]}, but self.animal_id is {self.animal_id}\"\n                )\n\n        self._feature_columns = [x for x in self.result.columns if x in constants.FEATURES]\n        self._nonfeature_columns = [x for x in self.result.columns if x not in constants.FEATURES]\n        self.animaldays = self.result.loc[:, \"animalday\"].unique()\n\n        self.channel_abbrevs = [\n            core.parse_chname_to_abbrev(x, assume_from_number=self.assume_from_number) for x in self.channel_names\n        ]\n\n    def reorder_and_pad_channels(\n        self, target_channels: list[str], use_abbrevs: bool = True, inplace: bool = True\n    ) -&gt; pd.DataFrame:\n        \"\"\"Reorder and pad channels to match a target channel list.\n\n        This method ensures that the data has a consistent channel order and structure\n        by reordering existing channels and padding missing channels with NaNs.\n\n        Args:\n            target_channels (list[str]): List of target channel names to match\n            use_abbrevs (bool, optional): If True, target channel names are read as channel abbreviations instead of channel names. Defaults to True.\n            inplace (bool, optional): If True, modify the result in place. Defaults to True.\n        Returns:\n            pd.DataFrame: DataFrame with reordered and padded channels\n        \"\"\"\n        duplicates = [ch for ch in target_channels if target_channels.count(ch) &gt; 1]\n        if duplicates:\n            raise ValueError(f\"Target channels must be unique. Found duplicates: {duplicates}\")\n\n        result = self.result.copy()\n\n        channel_map = {ch: i for i, ch in enumerate(target_channels)}\n        channel_names = self.channel_names if not use_abbrevs else self.channel_abbrevs\n\n        valid_channels = [ch for ch in channel_names if ch in channel_map]\n        if not valid_channels:\n            warnings.warn(\n                f\"None of the channel names {channel_names} were found in target channels {target_channels}. Is use_abbrevs correctly set?\"\n            )\n\n        for feature in self._feature_columns:\n            match feature:\n                case _ if feature in constants.LINEAR_FEATURES + constants.BAND_FEATURES:\n                    if feature in constants.BAND_FEATURES:\n                        df_bands = pd.DataFrame(result[feature].tolist())\n                        vals = np.array(df_bands.values.tolist())\n                        vals = vals.transpose((0, 2, 1))\n                        keys = df_bands.keys()\n                    else:\n                        vals = np.array(result[feature].tolist())\n\n                    new_vals = np.full((vals.shape[0], len(target_channels), *vals.shape[2:]), np.nan)  # dubious\n\n                    for i, ch in enumerate(channel_names):\n                        if ch in channel_map:\n                            new_vals[:, channel_map[ch]] = vals[:, i]\n\n                    if feature in constants.BAND_FEATURES:\n                        new_vals = new_vals.transpose((0, 2, 1))\n                        result[feature] = [dict(zip(keys, vals)) for vals in new_vals]\n                    else:\n                        result[feature] = [list(x) for x in new_vals]\n\n                case _ if feature in constants.MATRIX_FEATURES:\n                    if feature in [\"cohere\", \"zcohere\", \"imcoh\", \"zimcoh\"]:\n                        df_bands = pd.DataFrame(result[feature].tolist())\n                        vals = np.array(df_bands.values.tolist())\n                        keys = df_bands.keys()\n                    else:\n                        vals = np.array(result[feature].tolist())\n\n                    logging.debug(f\"vals.shape: {vals.shape}\")\n                    new_shape = list(vals.shape[:-2]) + [len(target_channels), len(target_channels)]\n                    new_vals = np.full(new_shape, np.nan)\n\n                    # Map original channels to target channels\n                    for i, ch1 in enumerate(channel_names):\n                        if ch1 in channel_map:\n                            for j, ch2 in enumerate(channel_names):\n                                if ch2 in channel_map:\n                                    new_vals[..., channel_map[ch1], channel_map[ch2]] = vals[..., i, j]\n\n                    if feature in [\"cohere\", \"zcohere\", \"imcoh\", \"zimcoh\"]:\n                        result[feature] = [dict(zip(keys, vals)) for vals in new_vals]\n                    else:\n                        result[feature] = [list(x) for x in new_vals]\n\n                case _ if feature in constants.HIST_FEATURES:\n                    coords = np.array([x[0] for x in result[feature].tolist()])\n                    vals = np.array([x[1] for x in result[feature].tolist()])\n                    new_vals = np.full((*vals.shape[0:-1], len(target_channels)), np.nan)\n\n                    for i, ch in enumerate(channel_names):\n                        if ch in channel_map:\n                            new_vals[:, ..., channel_map[ch]] = vals[:, ..., i]\n\n                    result[feature] = [(coords[i], new_vals[i]) for i in range(len(coords))]\n\n                case _:\n                    raise ValueError(f\"Invalid feature: {feature}\")\n\n        if inplace:\n            self.result = result\n\n            logging.debug(f\"Old channel names: {self.channel_names}\")\n            self.channel_names = target_channels\n            logging.debug(f\"New channel names: {self.channel_names}\")\n\n            logging.debug(f\"Old channel abbreviations: {self.channel_abbrevs}\")\n            self.__update_instance_vars()\n            logging.debug(f\"New channel abbreviations: {self.channel_abbrevs}\")\n\n        return result\n\n    def read_sars_spikes(\n        self,\n        sars: list[Union[\"SpikeAnalysisResult\", \"FrequencyDomainSpikeAnalysisResult\"]],\n        read_mode: Literal[\"sa\", \"mne\"] = \"sa\",\n        inplace=True\n    ):\n        \"\"\"\n        Integrate spike analysis results into WAR by adding nspike/lognspike features.\n\n        This method extracts spike timing information from spike detection results and bins\n        them according to the WAR's time windows, adding spike count features to each row.\n\n        Args:\n            sars: List of SpikeAnalysisResult or FrequencyDomainSpikeAnalysisResult objects.\n                  One result per recording session (animalday).\n            read_mode: Mode for extracting spike data:\n                - \"sa\": Read from SortingAnalyzer objects (result_sas attribute)\n                - \"mne\": Read from MNE RawArray objects (result_mne attribute)\n            inplace: If True, modifies self.result and returns self.\n                    If False, returns a new WindowAnalysisResult.\n\n        Returns:\n            WindowAnalysisResult: WAR object with added spike features (nspike, lognspike).\n                - If inplace=True: returns self with modified result DataFrame\n                - If inplace=False: returns new WAR object with enhanced result DataFrame\n\n        Notes:\n            - The number of sars must match the number of unique animaldays in self.result\n            - Spikes are binned into time windows matching the existing WAR fragments\n            - nspike: array of spike counts per channel for each time window\n            - lognspike: log-transformed spike counts using core.log_transform()\n\n        Example:\n            &gt;&gt;&gt; # After computing WAR and spike detection\n            &gt;&gt;&gt; enhanced_war = war.read_sars_spikes(fdsar_list, read_mode=\"sa\", inplace=False)\n            &gt;&gt;&gt; enhanced_war.result['nspike']  # Spike counts per channel per window\n        \"\"\"\n        match read_mode:\n            case \"sa\":\n                spikes_all = []\n                for sar in sars:  # for each continuous recording session\n                    spikes_channel = []\n                    for i, sa in enumerate(sar.result_sas):  # for each channel\n                        spike_times = []\n                        for unit in sa.sorting.get_unit_ids():  # Flatten units\n                            spike_times.extend(sa.sorting.get_unit_spike_train(unit_id=unit).tolist())\n                        spike_times = np.array(spike_times) / sa.sorting.get_sampling_frequency()\n                        spikes_channel.append(spike_times)\n                    spikes_all.append(spikes_channel)\n                return self._read_from_spikes_all(spikes_all, inplace=inplace)\n            case \"mne\":\n                raws = [sar.result_mne for sar in sars]\n                return self.read_mnes_spikes(raws, inplace=inplace)\n            case _:\n                raise ValueError(f\"Invalid read_mode: {read_mode}\")\n\n    def read_mnes_spikes(self, raws: list[mne.io.RawArray], inplace=True):\n        \"\"\"\n        Extract spike features from MNE RawArray objects with spike annotations.\n\n        This method extracts spike timing from MNE annotations (where spikes are marked\n        with channel-specific event labels) and bins them into WAR time windows.\n\n        Args:\n            raws: List of MNE RawArray objects with spike annotations. One per recording\n                  session (animalday). Each should have annotations with channel names\n                  as event labels (e.g., 'LMot', 'RMot', etc.).\n            inplace: If True, modifies self.result and returns self.\n                    If False, returns a new WindowAnalysisResult.\n\n        Returns:\n            WindowAnalysisResult: WAR object with added spike features (nspike, lognspike).\n\n        Notes:\n            - Expects MNE annotations with channel names as event descriptions\n            - Spike times are extracted from event onsets and binned to WAR windows\n            - Channels not found in annotations will have empty spike arrays\n            - Delegates to _read_from_spikes_all() for the actual binning logic\n\n        Example:\n            &gt;&gt;&gt; # From MNE spike annotations\n            &gt;&gt;&gt; enhanced_war = war.read_mnes_spikes([mne_raw1, mne_raw2], inplace=False)\n        \"\"\"\n        spikes_all = []\n        for raw in raws:\n            # each mne is a contiguous recording session\n            events, event_id = mne.events_from_annotations(raw)\n            event_id = {k.item(): v for k, v in event_id.items()}\n\n            spikes_channel = []\n            for channel in raw.ch_names:\n                if channel not in event_id.keys():\n                    logging.warning(f\"Channel {channel} not found in event_id\")\n                    spikes_channel.append([])\n                    continue\n                event_id_channel = event_id[channel]\n                spike_times = events[events[:, 2] == event_id_channel, 0]\n                spike_times = spike_times / raw.info[\"sfreq\"]\n                spikes_channel.append(spike_times)\n            spikes_all.append(spikes_channel)\n        return self._read_from_spikes_all(spikes_all, inplace=inplace)\n\n    def _read_from_spikes_all(self, spikes_all: list[list[list[float]]], inplace=True):\n        \"\"\"\n        Internal method to bin spike times into WAR time windows and add as features.\n\n        This is the common endpoint for both read_sars_spikes() and read_mnes_spikes().\n        It bins spike times according to the WAR's time windows and adds nspike/lognspike\n        features to the result DataFrame.\n\n        Args:\n            spikes_all: Nested list structure of spike times in seconds:\n                - Outer list: recording sessions (one per animalday)\n                - Middle list: channels (one per EEG channel)\n                - Inner list/array: spike times in seconds for that channel\n                Example: [[[0.5, 1.2], [0.8]], [[1.1, 2.3], []]]\n                         = 2 sessions, 2 channels each\n            inplace: If True, modifies self.result and returns self.\n                    If False, returns a new WindowAnalysisResult with enhanced data.\n\n        Returns:\n            WindowAnalysisResult: WAR object with spike features added to result DataFrame.\n\n        Notes:\n            - Groups self.result by 'animalday' and matches to spikes_all by index\n            - Uses _bin_spike_df() helper to count spikes within each time window\n            - Adds two new columns:\n                - 'nspike': array of spike counts per channel for each window\n                - 'lognspike': log-transformed spike counts via core.log_transform()\n            - Warns if spike count size doesn't match result DataFrame size\n        \"\"\"\n        # Each groupby animalday is a recording session\n        grouped = self.result.groupby(\"animalday\")\n        animaldays = grouped.groups.keys()\n        logging.debug(f\"Animal days: {animaldays}\")\n        spike_counts = dict(zip(animaldays, spikes_all))\n        spike_counts = grouped.apply(lambda x: _bin_spike_df(x, spikes_channel=spike_counts[x.name]))\n        spike_counts: pd.Series = spike_counts.explode()\n\n        if spike_counts.size != self.result.shape[0]:\n            logging.warning(f\"Spike counts size {spike_counts.size} does not match result size {self.result.shape[0]}\")\n\n        result = self.result.copy()\n        result[\"nspike\"] = spike_counts.tolist()\n        result[\"lognspike\"] = list(core.log_transform(np.stack(result[\"nspike\"].tolist(), axis=0)))\n        if inplace:\n            self.result = result\n            return self\n        else:\n            # Create a new WindowAnalysisResult\n            new_war = copy.deepcopy(self)\n            new_war.result = result\n            return new_war\n\n    def get_info(self):\n        \"\"\"Returns a formatted string with basic information about the WindowAnalysisResult object\"\"\"\n        info = []\n        info.append(f\"feature names: {', '.join(self._feature_columns)}\")\n        info.append(f\"animaldays: {', '.join(self.result['animalday'].unique())}\")\n        info.append(\n            f\"animal_id: {self.result['animal'].unique()[0] if 'animal' in self.result.columns else self.animal_id}\"\n        )\n        info.append(\n            f\"genotype: {self.result['genotype'].unique()[0] if 'genotype' in self.result.columns else self.genotype}\"\n        )\n        info.append(f\"channel_names: {', '.join(self.channel_names) if self.channel_names else 'None'}\")\n\n        return \"\\n\".join(info)\n\n    def get_result(self, features: list[str], exclude: list[str] = [], allow_missing=False):\n        \"\"\"Get windowed analysis result dataframe, with helpful filters\n\n        Args:\n            features (list[str]): List of features to get from result\n            exclude (list[str], optional): List of features to exclude from result; will override the features parameter. Defaults to [].\n            allow_missing (bool, optional): If True, will return all requested features as columns regardless if they exist in result. Defaults to False.\n\n        Returns:\n            result: pd.DataFrame object with features in columns and windows in rows\n        \"\"\"\n        features = _sanitize_feature_request(features, exclude)\n        if not allow_missing:\n            return self.result.loc[:, self._nonfeature_columns + features]\n        else:\n            return self.result.reindex(columns=self._nonfeature_columns + features)\n\n    def get_groupavg_result(\n        self, features: list[str], exclude: list[str] = [], df: pd.DataFrame = None, groupby=\"animalday\"\n    ):\n        \"\"\"Group result and average within groups. Preserves data structure and shape for each feature.\n\n        Args:\n            features (list[str]): List of features to get from result\n            exclude (list[str], optional): List of features to exclude from result. Will override the features parameter. Defaults to [].\n            df (pd.DataFrame, optional): If not None, this function will use this dataframe instead of self.result. Defaults to None.\n            groupby (str, optional): Feature or list of features to group by before averaging. Passed to the `by` parameter in pd.DataFrame.groupby(). Defaults to \"animalday\".\n\n        Returns:\n            grouped_result: result grouped by `groupby` and averaged for each group.\n        \"\"\"\n        result_grouped, result_validcols = self.__get_groups(features=features, exclude=exclude, df=df, groupby=groupby)\n        features = _sanitize_feature_request(features, exclude)\n\n        avg_results = []\n        for f in features:\n            if f in result_validcols:\n                avg_result_col = result_grouped.apply(self._average_feature, f, \"duration\", include_groups=False)\n                avg_result_col.name = f\n                avg_results.append(avg_result_col)\n            else:\n                logging.warning(f\"{f} not calculated, skipping\")\n\n        return pd.concat(avg_results, axis=1)\n\n    def __get_groups(self, features: list[str], exclude: list[str] = [], df: pd.DataFrame = None, groupby=\"animalday\"):\n        features = _sanitize_feature_request(features, exclude)\n        result_win = self.result if df is None else df\n        return result_win.groupby(groupby), result_win.columns\n\n    def get_grouprows_result(\n        self,\n        features: list[str],\n        exclude: list[str] = [],\n        df: pd.DataFrame = None,\n        multiindex=[\"animalday\", \"animal\", \"genotype\"],\n        include=[\"duration\", \"endfile\"],\n    ):\n        features = _sanitize_feature_request(features, exclude)\n        result_win = self.result if df is None else df\n        result_win = result_win.filter(features + multiindex + include)\n        return result_win.set_index(multiindex)\n\n    def get_filter_logrms_range(self, df: pd.DataFrame = None, z_range=3, **kwargs):\n        \"\"\"Filter windows based on log(rms).\n\n        Args:\n            df (pd.DataFrame, optional): If not None, this function will use this dataframe instead of self.result. Defaults to None.\n            z_range (float, optional): The z-score range to filter by. Values outside this range will be set to NaN.\n\n        Returns:\n            out: np.ndarray of bool, (M fragments, N channels). True = keep window, False = remove window\n        \"\"\"\n        result = df.copy() if df is not None else self.result.copy()\n        z_range = abs(z_range)\n        np_rms = np.array(result[\"rms\"].tolist())\n        np_logrms = np.log(np_rms)\n        del np_rms\n        np_logrmsz = zscore(np_logrms, axis=0, nan_policy=\"omit\")\n        np_logrms[(np_logrmsz &gt; z_range) | (np_logrmsz &lt; -z_range)] = np.nan\n\n        out = np.full(np_logrms.shape, True)\n        out[(np_logrmsz &gt; z_range) | (np_logrmsz &lt; -z_range)] = False\n        return out\n\n    def get_filter_high_rms(self, df: pd.DataFrame = None, max_rms=500, **kwargs):\n        \"\"\"Filter windows based on rms.\n\n        Args:\n            df (pd.DataFrame, optional): If not None, this function will use this dataframe instead of self.result. Defaults to None.\n            max_rms (float, optional): The maximum rms value to filter by. Values above this will be set to NaN.\n\n        Returns:\n            out: np.ndarray of bool, (M fragments, N channels). True = keep window, False = remove window\n        \"\"\"\n        result = df.copy() if df is not None else self.result.copy()\n        np_rms = np.array(result[\"rms\"].tolist())\n        np_rmsnan = np_rms.copy()\n        # Convert to float to allow NaN assignment for integer arrays\n        if np_rmsnan.dtype.kind in (\"i\", \"u\"):  # integer types\n            np_rmsnan = np_rmsnan.astype(float)\n        np_rmsnan[np_rms &gt; max_rms] = np.nan\n        result[\"rms\"] = np_rmsnan.tolist()\n\n        out = np.full(np_rms.shape, True)\n        out[np_rms &gt; max_rms] = False\n        return out\n\n    def get_filter_low_rms(self, df: pd.DataFrame = None, min_rms=30, **kwargs):\n        \"\"\"Filter windows based on rms.\n\n        Args:\n            df (pd.DataFrame, optional): If not None, this function will use this dataframe instead of self.result. Defaults to None.\n            min_rms (float, optional): The minimum rms value to filter by. Values below this will be set to NaN.\n\n        Returns:\n            out: np.ndarray of bool, (M fragments, N channels). True = keep window, False = remove window\n        \"\"\"\n        result = df.copy() if df is not None else self.result.copy()\n        np_rms = np.array(result[\"rms\"].tolist())\n        np_rmsnan = np_rms.copy()\n        np_rmsnan[np_rms &lt; min_rms] = np.nan\n        result[\"rms\"] = np_rmsnan.tolist()\n\n        out = np.full(np_rms.shape, True)\n        out[np_rms &lt; min_rms] = False\n        return out\n\n    def get_filter_high_beta(self, df: pd.DataFrame = None, max_beta_prop=0.4, **kwargs):\n        \"\"\"Filter windows based on beta power.\n\n        Args:\n            df (pd.DataFrame, optional): If not None, this function will use this dataframe instead of self.result. Defaults to None.\n            max_beta_prop (float, optional): The maximum beta power to filter by. Values above this will be set to NaN. Defaults to 0.4.\n\n        Returns:\n            out: np.ndarray of bool, (M fragments, N channels). True = keep window, False = remove window\n        \"\"\"\n        result = df.copy() if df is not None else self.result.copy()\n        if \"psdfrac\" in result.columns:\n            df_psdfrac = pd.DataFrame(result[\"psdfrac\"].tolist())\n            np_prop = np.array(df_psdfrac[\"beta\"].tolist())\n        elif \"psdband\" in result.columns and \"psdtotal\" in result.columns:\n            df_psdband = pd.DataFrame(result[\"psdband\"].tolist())\n            np_beta = np.array(df_psdband[\"beta\"].tolist())\n            np_total = np.array(result[\"psdtotal\"].tolist())\n            np_prop = np_beta / np_total\n        else:\n            raise ValueError(\"psdfrac or psdband+psdtotal required for beta power filtering\")\n\n        out = np.full(np_prop.shape, True)\n        out[np_prop &gt; max_beta_prop] = False\n        out = np.broadcast_to(np.all(out, axis=-1)[:, np.newaxis], out.shape)\n        return out\n\n    def get_filter_reject_channels(\n        self,\n        df: pd.DataFrame = None,\n        bad_channels: list[str] = None,\n        use_abbrevs: bool = None,\n        save_bad_channels: Literal[\"overwrite\", \"union\", None] = \"union\",\n        **kwargs,\n    ):\n        \"\"\"Filter channels to reject.\n\n        Args:\n            df (pd.DataFrame, optional): If not None, this function will use this dataframe instead of self.result. Defaults to None.\n            bad_channels (list[str]): List of channels to reject. Can be either full channel names or abbreviations.\n                The method will automatically detect which format is being used. If None, no filtering is performed.\n            use_abbrevs (bool, optional): Override automatic detection. If True, channels are assumed to be channel abbreviations. If False, channels are assumed to be channel names.\n                If None, channels are parsed to abbreviations and matched against self.channel_abbrevs.\n            save_bad_channels (Literal[\"overwrite\", \"union\", None], optional): How to save bad channels to self.bad_channels_dict.\n                \"overwrite\": Replace self.bad_channels_dict completely with bad channels applied to all sessions.\n                \"union\": Merge bad channels with existing self.bad_channels_dict for all sessions.\n                None: Don't save to self.bad_channels_dict. Defaults to \"union\".\n                Note: When using \"overwrite\" mode, the bad_channels parameter and bad_channels_dict parameter\n                may conflict and overwrite each other's bad channel definitions if both are provided.\n\n        Returns:\n            out: np.ndarray of bool, (M fragments, N channels). True = keep window, False = remove window\n        \"\"\"\n        n_samples = len(self.result)\n        n_channels = len(self.channel_names)\n        mask = np.ones((n_samples, n_channels), dtype=bool)\n\n        if bad_channels is None:\n            return mask\n\n        channel_targets = (\n            self.channel_abbrevs if use_abbrevs or use_abbrevs is None else self.channel_names\n        )  # Match to appropriate target\n        if use_abbrevs is None:  # Match channels as abbreviations\n            bad_channels = [\n                core.parse_chname_to_abbrev(ch, assume_from_number=self.assume_from_number) for ch in bad_channels\n            ]\n\n        # Match channels to channel_targets\n        for ch in bad_channels:\n            if ch in channel_targets:\n                mask[:, channel_targets.index(ch)] = False\n            else:\n                warnings.warn(f\"Channel {ch} not found in {channel_targets}\")\n\n        # Save bad channels to self.bad_channels_dict if requested\n        if save_bad_channels is not None:\n            # Get all unique animal days from the result\n            animaldays = self.result[\"animalday\"].unique()\n\n            # Convert bad channels to the format used in bad_channels_dict (original channel names)\n            channels_to_save = (\n                bad_channels.copy()\n                if use_abbrevs is False\n                else [\n                    core.parse_chname_to_abbrev(ch, assume_from_number=self.assume_from_number) for ch in bad_channels\n                ]\n            )\n\n            if save_bad_channels == \"overwrite\":\n                # Replace entire dict with bad channels applied to all sessions\n                self.bad_channels_dict = {animalday: channels_to_save.copy() for animalday in animaldays}\n            elif save_bad_channels == \"union\":\n                # Merge with existing bad channels for all sessions\n                updated_dict = self.bad_channels_dict.copy()\n                for animalday in animaldays:\n                    if animalday in updated_dict:\n                        # Union of existing and new channels\n                        updated_dict[animalday] = list(set(updated_dict[animalday]) | set(channels_to_save))\n                    else:\n                        updated_dict[animalday] = channels_to_save.copy()\n                self.bad_channels_dict = updated_dict\n\n        return mask\n\n    def get_filter_reject_channels_by_recording_session(\n        self,\n        df: pd.DataFrame = None,\n        bad_channels_dict: dict[str, list[str]] = None,\n        use_abbrevs: bool = None,\n        save_bad_channels: Literal[\"overwrite\", \"union\", None] = \"union\",\n        **kwargs,\n    ):\n        \"\"\"Filter channels to reject for each recording session\n\n        Args:\n            df (pd.DataFrame, optional): If not None, this function will use this dataframe instead of self.result. Defaults to None.\n            bad_channels_dict (dict[str, list[str]]): Dictionary of list of channels to reject for each recording session.\n                Can be either full channel names or abbreviations. The method will automatically detect which format is being used.\n                If None, the method will use the bad_channels_dict passed to the constructor.\n            use_abbrevs (bool, optional): Override automatic detection. If True, channels are assumed to be channel abbreviations. If False, channels are assumed to be channel names.\n                If None, channels are parsed to abbreviations and matched against self.channel_abbrevs.\n            save_bad_channels (Literal[\"overwrite\", \"union\", None], optional): How to save bad channels to self.bad_channels_dict.\n                \"overwrite\": Replace self.bad_channels_dict completely with bad_channels_dict.\n                \"union\": Merge bad_channels_dict with existing self.bad_channels_dict per session.\n                None: Don't save to self.bad_channels_dict. Defaults to \"union\".\n                Note: When using \"overwrite\" mode, the bad_channels parameter and bad_channels_dict parameter\n                may conflict and overwrite each other's bad channel definitions if both are provided.\n\n        Returns:\n            out: np.ndarray of bool, (M fragments, N channels). True = keep window, False = remove window\n        \"\"\"\n        if bad_channels_dict is None:\n            bad_channels_dict = self.bad_channels_dict.copy()\n\n        n_samples = len(self.result)\n        n_channels = len(self.channel_names)\n        mask = np.ones((n_samples, n_channels), dtype=bool)\n\n        # Group by animalday to apply filters per recording session\n        for animalday, group in self.result.groupby(\"animalday\"):\n            if bad_channels_dict:\n                if animalday not in bad_channels_dict:\n                    raise ValueError(\n                        f\"No bad channels specified for recording session {animalday}. Check that all days are present in bad_channels_dict\"\n                    )\n                bad_channels = bad_channels_dict[animalday]\n            else:\n                bad_channels = []\n\n            channel_targets = self.channel_abbrevs if use_abbrevs or use_abbrevs is None else self.channel_names\n            if use_abbrevs is None:\n                bad_channels = [\n                    core.parse_chname_to_abbrev(ch, assume_from_number=self.assume_from_number) for ch in bad_channels\n                ]\n\n            # Get indices for this recording session\n            session_indices = group.index\n\n            # Apply channel filtering for this session\n            for ch in bad_channels:\n                if ch in channel_targets:\n                    ch_idx = channel_targets.index(ch)\n                    mask[session_indices, ch_idx] = False\n                else:\n                    logging.warning(f\"Channel {ch} not found in {channel_targets} for session {animalday}\")\n\n        # Save bad channels to self.bad_channels_dict if requested\n        if save_bad_channels is not None and bad_channels_dict is not None:\n            if save_bad_channels == \"overwrite\":\n                self.bad_channels_dict = bad_channels_dict.copy()\n            elif save_bad_channels == \"union\":\n                # Merge with existing bad channels per session\n                updated_dict = self.bad_channels_dict.copy()\n                for animalday, channels in bad_channels_dict.items():\n                    if animalday in updated_dict:\n                        # Union of existing and new channels\n                        updated_dict[animalday] = list(set(updated_dict[animalday]) | set(channels))\n                    else:\n                        updated_dict[animalday] = channels.copy()\n                self.bad_channels_dict = updated_dict\n\n        return mask\n\n    def get_filter_morphological_smoothing(\n        self, filter_mask: np.ndarray, smoothing_seconds: float, **kwargs\n    ) -&gt; np.ndarray:\n        \"\"\"Apply morphological smoothing to a filter mask.\n\n        Args:\n            filter_mask (np.ndarray): Input boolean mask of shape (n_windows, n_channels)\n            smoothing_seconds (float): Time window in seconds for morphological operations\n\n        Returns:\n            np.ndarray: Smoothed boolean mask\n        \"\"\"\n        if \"duration\" not in self.result.columns:\n            raise ValueError(\"Cannot calculate window duration - 'duration' column missing\")\n\n        window_duration = self.result[\"duration\"].median()\n        structure_size = max(1, int(smoothing_seconds / window_duration))\n\n        if structure_size &lt;= 1:\n            return filter_mask\n\n        smoothed_mask = filter_mask.copy()\n        for ch_idx in range(filter_mask.shape[1]):\n            channel_mask = filter_mask[:, ch_idx]\n            # Opening removes small isolated artifacts\n            channel_mask = binary_opening(channel_mask, structure=np.ones(structure_size))\n            # Closing fills small gaps in valid data\n            channel_mask = binary_closing(channel_mask, structure=np.ones(structure_size))\n            smoothed_mask[:, ch_idx] = channel_mask\n\n        return smoothed_mask\n\n    def filter_morphological_smoothing(self, smoothing_seconds: float) -&gt; \"WindowAnalysisResult\":\n        \"\"\"Apply morphological smoothing to all data.\n\n        Args:\n            smoothing_seconds (float): Time window in seconds for morphological operations\n\n        Returns:\n            WindowAnalysisResult: New filtered instance\n        \"\"\"\n        # Start with all-True mask and smooth it\n        base_mask = np.ones((len(self.result), len(self.channel_names)), dtype=bool)\n        smoothed_mask = self.get_filter_morphological_smoothing(base_mask, smoothing_seconds)\n        return self._create_filtered_copy(smoothed_mask)\n\n    def filter_all(\n        self,\n        df: pd.DataFrame = None,\n        inplace=True,\n        # bad_channels: list[str] = None,\n        min_valid_channels=3,\n        filters: list[callable] = None,\n        morphological_smoothing_seconds: float = None,\n        # save_bad_channels: Literal[\"overwrite\", \"union\", None] = \"union\",\n        **kwargs,\n    ):\n        \"\"\"Apply a list of filters to the data. Filtering should be performed before aggregation.\n\n        Args:\n            df (pd.DataFrame, optional): If not None, this function will use this dataframe instead of self.result. Defaults to None.\n            inplace (bool, optional): If True, modify the result in place. Defaults to True.\n            bad_channels (list[str], optional): List of channels to reject. Defaults to None.\n            min_valid_channels (int, optional): Minimum number of valid channels required per window. Defaults to 3.\n            filters (list[callable], optional): List of filter functions to apply. Each function should return a boolean mask.\n                If None, uses default filters: [get_filter_logrms_range, get_filter_high_rms, get_filter_low_rms, get_filter_high_beta].\n                Defaults to None.\n            morphological_smoothing_seconds (float, optional): If provided, apply morphological opening/closing to smooth the filter mask.\n                This removes isolated false positives/negatives along the time axis for each channel independently.\n                The value specifies the time window in seconds for the morphological operations. Defaults to None.\n            save_bad_channels (Literal[\"overwrite\", \"union\", None], optional): How to save bad channels to self.bad_channels_dict.\n                This parameter is passed to the filtering functions. Defaults to \"union\".\n                Note: When using \"overwrite\" mode, the bad_channels parameter and bad_channels_dict parameter\n                may conflict and overwrite each other's bad channel definitions if both are provided.\n            **kwargs: Additional keyword arguments to pass to filter functions.\n\n        Returns:\n            WindowAnalysisResult: Filtered result\n        \"\"\"\n        if filters is None:\n            # TODO refactor these into standalone functions, which take in a war as the first parameter, then pass\n            # filt_bool = filt(self, df, **kwargs) as needed\n            filters = [\n                self.get_filter_logrms_range,\n                self.get_filter_high_rms,\n                self.get_filter_low_rms,\n                self.get_filter_high_beta,\n                self.get_filter_reject_channels_by_recording_session,\n                self.get_filter_reject_channels,\n            ]\n\n        filt_bools = []\n        # Apply each filter function\n        for filter_function in filters:\n            filt_bool = filter_function(df, **kwargs)\n            filt_bools.append(filt_bool)\n            logging.info(\n                f\"{filter_function.__name__}:\\tfiltered {filt_bool.size - np.count_nonzero(filt_bool)}/{filt_bool.size}\"\n            )\n\n        # Apply all filters\n        filt_bool_all = np.prod(np.stack(filt_bools, axis=-1), axis=-1).astype(bool)\n        logging.debug(f\"filt_bool_all.shape: {filt_bool_all.shape}\")  # (windows, channels)\n\n        # Apply morphological smoothing if requested\n        if morphological_smoothing_seconds is not None:\n            if \"duration\" not in self.result.columns:\n                raise ValueError(\"Cannot calculate window duration - 'duration' column missing from result dataframe\")\n            window_duration = self.result[\"duration\"].median()\n\n            # Calculate number of windows for the smoothing\n            structure_size = max(1, int(morphological_smoothing_seconds / window_duration))\n\n            if structure_size &gt; 1:\n                logging.info(\n                    f\"Applying morphological smoothing with {structure_size} windows ({morphological_smoothing_seconds}s / {window_duration}s per window)\"\n                )\n                # Apply channel-wise temporal smoothing (each channel processed independently)\n                # This avoids spatial assumptions while smoothing temporal artifacts\n                for ch_idx in range(filt_bool_all.shape[1]):\n                    channel_mask = filt_bool_all[:, ch_idx]\n                    # Opening removes small isolated artifacts\n                    channel_mask = binary_opening(channel_mask, structure=np.ones(structure_size))\n                    # Closing fills small gaps in valid data\n                    channel_mask = binary_closing(channel_mask, structure=np.ones(structure_size))\n                    filt_bool_all[:, ch_idx] = channel_mask\n            else:\n                logging.info(\"Skipping morphological smoothing - structure size would be 1 (no effect)\")\n\n        # Filter windows based on number of valid channels\n        valid_channels_per_window = np.sum(filt_bool_all, axis=1)  # axis 1 = channel\n        window_mask = valid_channels_per_window &gt;= min_valid_channels  # True if window has enough valid channels\n        filt_bool_all = filt_bool_all &amp; window_mask[:, np.newaxis]  # Apply window mask to all channels\n\n        filtered_result = self._apply_filter(filt_bool_all)\n        if inplace:\n            del self.result\n            self.result = filtered_result\n        return WindowAnalysisResult(\n            filtered_result,\n            self.animal_id,\n            self.genotype,\n            self.channel_names,\n            self.assume_from_number,\n            self.bad_channels_dict.copy(),\n            self.suppress_short_interval_error,\n            self.lof_scores_dict.copy(),\n        )\n\n    def _create_filtered_copy(self, filter_mask: np.ndarray) -&gt; \"WindowAnalysisResult\":\n        \"\"\"Create a new WindowAnalysisResult with the filter applied.\n\n        Args:\n            filter_mask (np.ndarray): Boolean mask of shape (n_windows, n_channels)\n\n        Returns:\n            WindowAnalysisResult: New instance with filter applied\n        \"\"\"\n        filtered_result = self._apply_filter(filter_mask)\n        return WindowAnalysisResult(\n            filtered_result,\n            self.animal_id,\n            self.genotype,\n            self.channel_names,\n            self.assume_from_number,\n            self.bad_channels_dict.copy(),\n            self.suppress_short_interval_error,\n            self.lof_scores_dict.copy(),\n        )\n\n    def filter_logrms_range(self, z_range: float = 3) -&gt; \"WindowAnalysisResult\":\n        \"\"\"Filter based on log(rms) z-score range.\n\n        Args:\n            z_range (float): Z-score range threshold. Defaults to 3.\n\n        Returns:\n            WindowAnalysisResult: New filtered instance\n        \"\"\"\n        mask = self.get_filter_logrms_range(z_range=z_range)\n        return self._create_filtered_copy(mask)\n\n    def filter_high_rms(self, max_rms: float = 500) -&gt; \"WindowAnalysisResult\":\n        \"\"\"Filter out windows with RMS above threshold.\n\n        Args:\n            max_rms (float): Maximum RMS threshold. Defaults to 500.\n\n        Returns:\n            WindowAnalysisResult: New filtered instance\n        \"\"\"\n        mask = self.get_filter_high_rms(max_rms=max_rms)\n        return self._create_filtered_copy(mask)\n\n    def filter_low_rms(self, min_rms: float = 50) -&gt; \"WindowAnalysisResult\":\n        \"\"\"Filter out windows with RMS below threshold.\n\n        Args:\n            min_rms (float): Minimum RMS threshold. Defaults to 50.\n\n        Returns:\n            WindowAnalysisResult: New filtered instance\n        \"\"\"\n        mask = self.get_filter_low_rms(min_rms=min_rms)\n        return self._create_filtered_copy(mask)\n\n    def filter_high_beta(self, max_beta_prop: float = 0.4) -&gt; \"WindowAnalysisResult\":\n        \"\"\"Filter out windows with high beta power.\n\n        Args:\n            max_beta_prop (float): Maximum beta power proportion. Defaults to 0.4.\n\n        Returns:\n            WindowAnalysisResult: New filtered instance\n        \"\"\"\n        mask = self.get_filter_high_beta(max_beta_prop=max_beta_prop)\n        return self._create_filtered_copy(mask)\n\n    def filter_reject_channels(self, bad_channels: list[str], use_abbrevs: bool = None) -&gt; \"WindowAnalysisResult\":\n        \"\"\"Filter out specified bad channels.\n\n        Args:\n            bad_channels (list[str]): List of channel names to reject\n            use_abbrevs (bool, optional): Whether to use abbreviations. Defaults to None.\n\n        Returns:\n            WindowAnalysisResult: New filtered instance\n        \"\"\"\n        mask = self.get_filter_reject_channels(bad_channels=bad_channels, use_abbrevs=use_abbrevs)\n        return self._create_filtered_copy(mask)\n\n    def filter_reject_channels_by_session(\n        self, bad_channels_dict: dict[str, list[str]] = None, use_abbrevs: bool = None\n    ) -&gt; \"WindowAnalysisResult\":\n        \"\"\"Filter out bad channels by recording session.\n\n        Args:\n            bad_channels_dict (dict[str, list[str]], optional): Dictionary mapping recording session\n                identifiers to lists of bad channel names to reject. Session identifiers are in the\n                format \"{animal_id} {genotype} {day}\" (e.g., \"A10 WT Apr-01-2023\"). Channel names\n                can be either full names (e.g., \"Left Auditory\") or abbreviations (e.g., \"LAud\").\n                If None, uses the bad_channels_dict from the constructor. Defaults to None.\n            use_abbrevs (bool, optional): Override automatic channel name format detection. If True,\n                channels are assumed to be abbreviations. If False, channels are assumed to be full\n                names. If None, automatically detects format and converts to abbreviations for matching.\n                Defaults to None.\n\n        Returns:\n            WindowAnalysisResult: New filtered instance with bad channels masked as NaN for their\n                respective recording sessions\n\n        Examples:\n            Filter specific channels per session using abbreviations:\n            &gt;&gt;&gt; bad_channels = {\n            ...     \"A10 WT Apr-01-2023\": [\"LAud\", \"RMot\"],  # Session 1: reject left auditory, right motor\n            ...     \"A10 WT Apr-02-2023\": [\"LVis\"]           # Session 2: reject left visual only\n            ... }\n            &gt;&gt;&gt; filtered_war = war.filter_reject_channels_by_session(bad_channels, use_abbrevs=True)\n\n            Filter using full channel names:\n            &gt;&gt;&gt; bad_channels = {\n            ...     \"A12 KO May-15-2023\": [\"Left Motor\", \"Right Barrel\"],\n            ...     \"A12 KO May-16-2023\": [\"Left Auditory\", \"Left Visual\", \"Right Motor\"]\n            ... }\n            &gt;&gt;&gt; filtered_war = war.filter_reject_channels_by_session(bad_channels, use_abbrevs=False)\n\n            Auto-detect channel format (recommended):\n            &gt;&gt;&gt; bad_channels = {\n            ...     \"A15 WT Jun-10-2023\": [\"LMot\", \"RBar\"],  # Will auto-detect as abbreviations\n            ...     \"A15 WT Jun-11-2023\": [\"LAud\"]\n            ... }\n            &gt;&gt;&gt; filtered_war = war.filter_reject_channels_by_session(bad_channels)\n\n        Note:\n            - Session identifiers must exactly match the \"animalday\" values in the result DataFrame\n            - Available channel abbreviations: LAud, RAud, LVis, RVis, LHip, RHip, LBar, RBar, LMot, RMot\n            - Channel names are case-insensitive and support various formats (e.g., \"left aud\", \"Left Auditory\")\n            - If a session identifier is not found in bad_channels_dict, a warning is logged but processing continues\n            - If a channel name is not recognized, a warning is logged but other channels are still processed\n        \"\"\"\n        mask = self.get_filter_reject_channels_by_recording_session(\n            bad_channels_dict=bad_channels_dict, use_abbrevs=use_abbrevs\n        )\n        return self._create_filtered_copy(mask)\n\n    def apply_filters(\n        self, filter_config: dict = None, min_valid_channels: int = 3, morphological_smoothing_seconds: float = None\n    ) -&gt; \"WindowAnalysisResult\":\n        \"\"\"Apply multiple filters using configuration.\n\n        Args:\n            filter_config (dict, optional): Dictionary of filter names and parameters.\n                Available filters: 'logrms_range', 'high_rms', 'low_rms', 'high_beta',\n                'reject_channels', 'reject_channels_by_session', 'morphological_smoothing'\n            min_valid_channels (int): Minimum valid channels per window. Defaults to 3.\n            morphological_smoothing_seconds (float, optional): Temporal smoothing window (deprecated, use config instead)\n\n        Returns:\n            WindowAnalysisResult: New filtered instance\n\n        Examples:\n            &gt;&gt;&gt; config = {\n            ...     'logrms_range': {'z_range': 3},\n            ...     'high_rms': {'max_rms': 500},\n            ...     'reject_channels': {'bad_channels': ['LMot', 'RMot']},\n            ...     'morphological_smoothing': {'smoothing_seconds': 8.0}\n            ... }\n            &gt;&gt;&gt; filtered_war = war.apply_filters(config)\n        \"\"\"\n        if filter_config is None:\n            filter_config = {\n                \"logrms_range\": {\"z_range\": 3},\n                \"high_rms\": {\"max_rms\": 500},\n                \"low_rms\": {\"min_rms\": 50},\n                \"high_beta\": {\"max_beta_prop\": 0.4},\n                \"reject_channels_by_session\": {},\n            }\n\n        filter_methods = {\n            \"logrms_range\": self.get_filter_logrms_range,\n            \"high_rms\": self.get_filter_high_rms,\n            \"low_rms\": self.get_filter_low_rms,\n            \"high_beta\": self.get_filter_high_beta,\n            \"reject_channels\": self.get_filter_reject_channels,\n            \"reject_channels_by_session\": self.get_filter_reject_channels_by_recording_session,\n        }\n\n        filt_bools = []\n        morphological_params = None\n\n        for filter_name, filter_params in filter_config.items():\n            if filter_name == \"morphological_smoothing\":\n                morphological_params = filter_params\n                continue\n\n            if filter_name not in filter_methods:\n                raise ValueError(\n                    f\"Unknown filter: {filter_name}. Available: {list(filter_methods.keys()) + ['morphological_smoothing']}\"\n                )\n\n            filter_func = filter_methods[filter_name]\n            filt_bool = filter_func(**filter_params)\n            filt_bools.append(filt_bool)\n            logging.info(f\"{filter_name}: filtered {filt_bool.size - np.count_nonzero(filt_bool)}/{filt_bool.size}\")\n\n        # Combine all filter masks\n        if filt_bools:\n            filt_bool_all = np.prod(np.stack(filt_bools, axis=-1), axis=-1).astype(bool)\n        else:\n            filt_bool_all = np.ones((len(self.result), len(self.channel_names)), dtype=bool)\n\n        # Apply morphological smoothing if requested (either from config or parameter)\n        if morphological_params or morphological_smoothing_seconds is not None:\n            if morphological_params:\n                smoothing_seconds = morphological_params[\"smoothing_seconds\"]\n            else:\n                smoothing_seconds = morphological_smoothing_seconds\n\n            filt_bool_all = self.get_filter_morphological_smoothing(filt_bool_all, smoothing_seconds)\n            logging.info(f\"Applied morphological smoothing: {smoothing_seconds}s\")\n\n        # Filter windows based on minimum valid channels\n        valid_channels_per_window = np.sum(filt_bool_all, axis=1)\n        window_mask = valid_channels_per_window &gt;= min_valid_channels\n        filt_bool_all = filt_bool_all &amp; window_mask[:, np.newaxis]\n\n        return self._create_filtered_copy(filt_bool_all)\n\n    def _apply_filter(self, filter_tfs: np.ndarray):\n        result = self.result.copy()\n        filter_tfs = np.array(filter_tfs, dtype=bool)  # (M fragments, N channels)\n        for feat in constants.FEATURES:\n            if feat not in result.columns:\n                logging.info(f\"Skipping {feat} because it is not in result\")\n                continue\n            logging.info(f\"Filtering {feat}\")\n            match feat:  # NOTE refactor this to use constants\n                case \"rms\" | \"ampvar\" | \"psdtotal\" | \"nspike\" | \"logrms\" | \"logampvar\" | \"logpsdtotal\" | \"lognspike\":\n                    vals = np.array(result[feat].tolist())\n                    # Convert to float to allow NaN assignment for integer features\n                    if vals.dtype.kind in (\"i\", \"u\"):  # integer types\n                        vals = vals.astype(float)\n                    vals[~filter_tfs] = np.nan\n                    result[feat] = vals.tolist()\n                case \"psd\":\n                    # FIXME The sampling rates have changed between computation passes so WARs have different shapes.\n                    # Add a check for same sampling frequency, other war-relevant properties etc.\n                    # The logging lines below should be removed at some point, but I'll keep it this way for now\n                    logging.info(\n                        f\"set([x[0].shape for x in result[feat].tolist()]) = {list(set([x[0].shape for x in result[feat].tolist()]))}\"\n                    )\n                    logging.info(\n                        f\"set([x[1].shape for x in result[feat].tolist()]) = {list(set([x[1].shape for x in result[feat].tolist()]))}\"\n                    )\n                    coords = np.array([x[0] for x in result[feat].tolist()])\n                    vals = np.array([x[1] for x in result[feat].tolist()])\n                    mask = np.broadcast_to(filter_tfs[:, np.newaxis, :], vals.shape)\n                    vals[~mask] = np.nan\n                    outs = [(c, vals[i, :, :]) for i, c in enumerate(coords)]\n                    result[feat] = outs\n                case \"psdband\" | \"psdfrac\" | \"logpsdband\" | \"logpsdfrac\":\n                    vals = pd.DataFrame(result[feat].tolist())\n                    for colname in vals.columns:\n                        v = np.array(vals[colname].tolist())\n                        v[~filter_tfs] = np.nan\n                        vals[colname] = v.tolist()\n                    result[feat] = vals.to_dict(\"records\")\n                case \"psdslope\":\n                    vals = np.array(result[feat].tolist())\n                    mask = np.broadcast_to(filter_tfs[:, :, np.newaxis], vals.shape)\n                    vals[~mask] = np.nan\n                    # vals = [list(map(tuple, x)) for x in vals.tolist()]\n                    result[feat] = vals.tolist()\n                case \"cohere\" | \"zcohere\" | \"imcoh\" | \"zimcoh\":\n                    vals = pd.DataFrame(result[feat].tolist())\n                    shape = np.array(vals.iloc[:, 0].tolist()).shape\n                    mask = np.broadcast_to(filter_tfs[:, :, np.newaxis], shape)\n                    for colname in vals.columns:\n                        v = np.array(vals[colname].tolist())\n                        v[~mask] = np.nan\n                        v[~mask.transpose(0, 2, 1)] = np.nan\n                        vals[colname] = v.tolist()\n                    result[feat] = vals.to_dict(\"records\")\n                case \"pcorr\" | \"zpcorr\":\n                    vals = np.array(result[feat].tolist())\n                    mask = np.broadcast_to(filter_tfs[:, :, np.newaxis], vals.shape)\n                    vals[~mask] = np.nan\n                    vals[~mask.transpose(0, 2, 1)] = np.nan\n                    result[feat] = vals.tolist()\n                case _:\n                    raise ValueError(f\"Unknown feature to filter {feat}\")\n        return result\n\n    def save_pickle_and_json(\n        self,\n        folder: str | Path,\n        make_folder=True,\n        filename: str = None,\n        slugify_filename=False,\n        save_abbrevs_as_chnames=False,\n    ):\n        \"\"\"Archive window analysis result into the folder specified, as a pickle and json file.\n\n        Args:\n            folder (str | Path): Destination folder to save results to\n            make_folder (bool, optional): If True, create the folder if it doesn't exist. Defaults to True.\n            filename (str, optional): Name of the file to save. Defaults to \"war\".\n            slugify_filename (bool, optional): If True, slugify the filename (replace special characters). Defaults to False.\n            save_abbrevs_as_chnames (bool, optional): If True, save the channel abbreviations as the channel names in the json file. Defaults to False.\n        \"\"\"\n        folder = Path(folder)\n        if make_folder:\n            folder.mkdir(parents=True, exist_ok=True)\n\n        filename = \"war\" if filename is None else filename\n        filename = slugify(filename) if slugify_filename else filename\n\n        filepath = str(folder / filename)\n\n        self.result.to_pickle(filepath + \".pkl\")\n        logging.info(f\"Saved WAR to {filepath + '.pkl'}\")\n\n        json_dict = {\n            \"animal_id\": self.animal_id,\n            \"genotype\": self.genotype,\n            \"channel_names\": self.channel_abbrevs if save_abbrevs_as_chnames else self.channel_names,\n            \"assume_from_number\": False if save_abbrevs_as_chnames else self.assume_from_number,\n            \"bad_channels_dict\": self.bad_channels_dict,\n            \"suppress_short_interval_error\": self.suppress_short_interval_error,\n            \"lof_scores_dict\": self.lof_scores_dict.copy(),\n        }\n\n        with open(filepath + \".json\", \"w\") as f:\n            json.dump(json_dict, f, indent=2)\n            logging.info(f\"Saved WAR to {filepath + '.json'}\")\n\n    def get_bad_channels_by_lof_threshold(self, lof_threshold: float) -&gt; dict:\n        \"\"\"Apply LOF threshold directly to stored scores to get bad channels.\n\n        Args:\n            lof_threshold (float): Threshold for determining bad channels.\n\n        Returns:\n            dict: Dictionary mapping animal days to lists of bad channel names.\n        \"\"\"\n        if not hasattr(self, \"lof_scores_dict\") or not self.lof_scores_dict:\n            raise ValueError(\"LOF scores not available in this WAR. Compute LOF scores first.\")\n\n        bad_channels_dict = {}\n        for animalday, lof_data in self.lof_scores_dict.items():\n            if \"lof_scores\" in lof_data and \"channel_names\" in lof_data:\n                scores = np.array(lof_data[\"lof_scores\"])\n                channel_names = lof_data[\"channel_names\"]\n\n                is_inlier = scores &lt; lof_threshold\n                bad_channels = [channel_names[i] for i in np.where(~is_inlier)[0]]\n                bad_channels_dict[animalday] = bad_channels\n            else:\n                raise ValueError(f\"LOF scores not available for {animalday}\")\n\n        return bad_channels_dict\n\n    def get_lof_scores(self) -&gt; dict:\n        \"\"\"Get LOF scores from this WAR.\n\n        Returns:\n            dict: Dictionary mapping animal days to LOF score dictionaries.\n        \"\"\"\n        if not hasattr(self, \"lof_scores_dict\") or not self.lof_scores_dict:\n            raise ValueError(\"LOF scores not available in this WAR. Compute LOF scores first.\")\n\n        result = {}\n        for animalday, lof_data in self.lof_scores_dict.items():\n            if \"lof_scores\" in lof_data and \"channel_names\" in lof_data:\n                scores = lof_data[\"lof_scores\"]\n                channel_names = lof_data[\"channel_names\"]\n                result[animalday] = dict(zip(channel_names, scores))\n            else:\n                raise ValueError(f\"LOF scores not available for {animalday}\")\n\n        return result\n\n    def evaluate_lof_threshold_binary(\n        self, ground_truth_bad_channels: dict = None, threshold: float = None, evaluation_channels: list[str] = None\n    ) -&gt; tuple:\n        \"\"\"Evaluate single threshold against ground truth for binary classification.\n\n        Args:\n            ground_truth_bad_channels: Dict mapping animal-day to bad channel sets.\n                                     If None, uses self.bad_channels_dict as ground truth.\n            threshold: LOF threshold to test\n            evaluation_channels: Subset of channels to include in evaluation. If none, uses all channels.\n\n        Returns:\n            tuple: (y_true_list, y_pred_list) for sklearn.metrics.f1_score\n                   Each element represents one channel from one animal-day\n        \"\"\"\n        if not hasattr(self, \"lof_scores_dict\") or not self.lof_scores_dict:\n            raise ValueError(\"LOF scores not available in this WAR. Run compute_bad_channels() first.\")\n\n        if threshold is None:\n            raise ValueError(\"threshold parameter is required\")\n\n        # Use self.bad_channels_dict as default ground truth\n        if ground_truth_bad_channels is None:\n            if hasattr(self, \"bad_channels_dict\") and self.bad_channels_dict:\n                ground_truth_bad_channels = {}\n\n                # Filter bad_channels_dict to only include keys that exist in lof_scores_dict\n                lof_keys = set(self.lof_scores_dict.keys())\n                bad_channels_keys = set(self.bad_channels_dict.keys())\n\n                missing_keys = bad_channels_keys - lof_keys\n                if missing_keys:\n                    raise ValueError(\n                        f\"bad_channels_dict contains keys not found in lof_scores_dict: {missing_keys}. \"\n                        f\"Available LOF keys: {sorted(lof_keys)}\"\n                    )\n\n                # Only use bad channel keys that have corresponding LOF data\n                ground_truth_bad_channels = {\n                    key: value for key, value in self.bad_channels_dict.items() if key in lof_keys\n                }\n\n                logging.info(\n                    f\"Using filtered bad_channels_dict as ground truth with {len(ground_truth_bad_channels)} animal-day sessions\"\n                )\n            else:\n                raise ValueError(\"No ground truth provided and self.bad_channels_dict is empty.\")\n\n        # Get all channels if no subset specified\n        if evaluation_channels is None:\n            evaluation_channels = self.channel_names\n\n        y_true_list = []\n        y_pred_list = []\n\n        # Debug: Log what we're working with\n        logging.debug(f\"evaluate_lof_threshold_binary: evaluation_channels = {evaluation_channels}\")\n        logging.debug(\n            f\"evaluate_lof_threshold_binary: ground_truth_bad_channels keys = {list(ground_truth_bad_channels.keys())}\"\n        )\n        logging.debug(f\"evaluate_lof_threshold_binary: lof_scores_dict keys = {list(self.lof_scores_dict.keys())}\")\n\n        # Iterate through each animal-day and evaluate channels\n        for animalday, lof_data in self.lof_scores_dict.items():\n            if \"lof_scores\" not in lof_data or \"channel_names\" not in lof_data:\n                raise ValueError(\n                    f\"Invalid LOF data for {animalday}: missing required fields 'lof_scores' or 'channel_names'\"\n                )\n\n            scores = np.array(lof_data[\"lof_scores\"])\n            channel_names = lof_data[\"channel_names\"]\n\n            # Get ground truth bad channels for this animal-day\n            animalday_bad_channels = ground_truth_bad_channels.get(animalday, set())\n\n            # Debug: Log details for this animal-day\n            logging.debug(f\"Processing {animalday}: channel_names = {channel_names}\")\n            logging.debug(f\"Processing {animalday}: animalday_bad_channels = {animalday_bad_channels}\")\n            logging.debug(f\"Processing {animalday}: scores shape = {scores.shape}\")\n\n            # Evaluate each channel in the evaluation subset\n            channels_processed = 0\n            for i, channel in enumerate(channel_names):\n                if (\n                    channel in evaluation_channels\n                    or parse_chname_to_abbrev(channel, strict_matching=False) in evaluation_channels\n                ):\n                    channels_processed += 1\n\n                    # Ground truth: 1 if channel is marked as bad, 0 otherwise\n                    is_bad_channel = (\n                        channel in animalday_bad_channels\n                        or parse_chname_to_abbrev(channel, strict_matching=False) in animalday_bad_channels\n                    )\n                    # if is_bad_channel and channel not in animalday_bad_channels:\n                    #     logging.debug(f\"Mapped full channel '{channel}' -&gt; '{parse_chname_to_abbrev(channel, strict_matching=False)}' found in bad channels\")\n\n                    y_true = 1 if is_bad_channel else 0\n                    # Prediction: 1 if LOF score &gt; threshold, 0 otherwise\n                    y_pred = 1 if scores[i] &gt; threshold else 0\n\n                    y_true_list.append(y_true)\n                    y_pred_list.append(y_pred)\n\n                    logging.debug(\n                        f\"Channel {channel}: y_true={y_true}, y_pred={y_pred} (score={scores[i]:.3f}, threshold={threshold})\"\n                    )\n\n                    # Extra debugging for the alignment issue\n                    if y_true == 1:\n                        logging.info(\n                            f\"TRUE POSITIVE CANDIDATE: {channel} mapped to bad channel in: {animalday_bad_channels}\"\n                        )\n                    if y_pred == 1:\n                        logging.info(f\"LOF PREDICTION: {channel} has score {scores[i]:.3f} &gt; threshold {threshold}\")\n\n            logging.debug(f\"Processed {channels_processed} channels for {animalday}\")\n\n        return y_true_list, y_pred_list\n\n    @classmethod\n    def load_pickle_and_json(cls, folder_path=None, pickle_name=None, json_name=None):\n        \"\"\"Load WindowAnalysisResult from folder\n\n        Args:\n            folder_path (str, optional): Path of folder containing .pkl and .json files. Defaults to None.\n            pickle_name (str, optional): Name of the pickle file. Can be just the filename (e.g. \"war.pkl\")\n                or a path relative to folder_path (e.g. \"subdir/war.pkl\"). If None and folder_path is provided,\n                expects exactly one .pkl file in folder_path. Defaults to None.\n            json_name (str, optional): Name of the JSON file. Can be just the filename (e.g. \"war.json\")\n                or a path relative to folder_path (e.g. \"subdir/war.json\"). If None and folder_path is provided,\n                expects exactly one .json file in folder_path. Defaults to None.\n\n        Raises:\n            ValueError: folder_path does not exist\n            ValueError: Expected exactly one pickle and one json file in folder_path (when pickle_name/json_name not specified)\n            FileNotFoundError: Specified pickle_name or json_name not found\n\n        Returns:\n            result: WindowAnalysisResult object\n        \"\"\"\n        if folder_path is not None:\n            folder_path = Path(folder_path)\n            if not folder_path.exists():\n                raise ValueError(f\"Folder path {folder_path} does not exist\")\n\n            if pickle_name is not None:\n                # Handle pickle_name as either absolute path or relative to folder_path\n                pickle_path = Path(pickle_name)\n                if pickle_path.is_absolute():\n                    df_pickle_path = pickle_path\n                else:\n                    df_pickle_path = folder_path / pickle_name\n\n                if not df_pickle_path.exists():\n                    raise FileNotFoundError(f\"Pickle file not found: {df_pickle_path}\")\n            else:\n                pkl_files = list(folder_path.glob(\"*.pkl\"))\n                if len(pkl_files) != 1:\n                    raise ValueError(f\"Expected exactly one pickle file in {folder_path}, found {len(pkl_files)}\")\n                df_pickle_path = pkl_files[0]\n\n            if json_name is not None:\n                # Handle json_name as either absolute path or relative to folder_path\n                json_path = Path(json_name)\n                if json_path.is_absolute():\n                    json_path = json_path\n                else:\n                    json_path = folder_path / json_name\n\n                if not json_path.exists():\n                    raise FileNotFoundError(f\"JSON file not found: {json_path}\")\n            else:\n                json_files = list(folder_path.glob(\"*.json\"))\n                if len(json_files) != 1:\n                    raise ValueError(f\"Expected exactly one json file in {folder_path}, found {len(json_files)}\")\n                json_path = json_files[0]\n        else:\n            if pickle_name is None or json_name is None:\n                raise ValueError(\n                    \"Either folder_path must be provided, or both pickle_name and json_name must be provided as absolute paths\"\n                )\n\n            df_pickle_path = Path(pickle_name)\n            json_path = Path(json_name)\n\n            if not df_pickle_path.exists():\n                raise FileNotFoundError(f\"Pickle file not found: {df_pickle_path}\")\n            if not json_path.exists():\n                raise FileNotFoundError(f\"JSON file not found: {json_path}\")\n\n        with open(df_pickle_path, \"rb\") as f:\n            data = pd.read_pickle(f)\n        with open(json_path, \"r\") as f:\n            metadata = json.load(f)\n        return cls(data, **metadata)\n\n    def aggregate_time_windows(self, groupby: list[str] | str = [\"animalday\", \"isday\"]) -&gt; None:\n        \"\"\"Aggregate time windows into a single data point per groupby by averaging features. This reduces the number of rows in the result.\n\n        Args:\n            groupby (list[str] | str, optional): Columns to group by. Defaults to ['animalday', 'isday'], which groups by animalday (recording session) and isday (day/night).\n\n        Raises:\n            ValueError: groupby must be from ['animalday', 'isday']\n            ValueError: Columns in groupby not found in result\n            ValueError: Columns in groupby are not constant in groups\n        \"\"\"\n        if isinstance(groupby, str):\n            groupby = [groupby]\n        if not all(col in [\"animalday\", \"isday\"] for col in groupby):\n            raise ValueError(f\"groupby must be from ['animalday', 'isday']. Got {groupby}\")\n        if not all(col in self.result.columns for col in groupby):\n            raise ValueError(f\"Columns {groupby} not found in result. Columns: {self.result.columns.tolist()}\")\n\n        features = [f for f in constants.FEATURES if f in self.result.columns]\n        logging.debug(f\"Aggregating {features}\")\n        result_grouped = self.result.groupby(groupby)\n\n        agg_dict = {}\n\n        if \"animalday\" not in groupby:\n            agg_dict[\"animalday\"] = lambda df: None\n        if \"isday\" not in groupby:\n            agg_dict[\"isday\"] = lambda df: None\n\n        constant_cols = [\"animal\", \"day\", \"genotype\"]\n        for col in constant_cols:\n            if col in self.result.columns:\n                is_constant = result_grouped[col].nunique() == 1\n                if not is_constant.all():\n                    non_constant_groups = is_constant[~is_constant].index.tolist()\n                    raise ValueError(f\"Column {col} is not constant in groups: {non_constant_groups}\")\n                agg_dict[col] = lambda df, col=col: df[col].iloc[0]\n\n        if \"duration\" in self.result.columns:\n            agg_dict[\"duration\"] = lambda df: np.sum(df[\"duration\"])\n\n        if \"endfile\" in self.result.columns:\n            agg_dict[\"endfile\"] = lambda df: df[\"endfile\"].iloc[-1]\n\n        if \"timestamp\" in self.result.columns:\n            agg_dict[\"timestamp\"] = lambda df: df[\"timestamp\"].iloc[0]\n\n        for feat in features:\n            agg_dict[feat] = lambda df, feat=feat: self._average_feature(df, feat, \"duration\")\n\n        aggregated_df = result_grouped.apply(\n            lambda df: pd.Series({col: agg_dict[col](df) for col in self.result.columns if col not in groupby})\n        )\n\n        self.result = aggregated_df.reset_index(drop=False)  # Keep animalday/isday as a column\n\n        self.suppress_short_interval_error = True\n        logging.info(\"Setting suppress_short_interval_error to True\")\n        self.__update_instance_vars()\n\n    def add_unique_hash(self, nbytes: int | None = None):\n        \"\"\"Adds a hex hash to the animal ID to ensure uniqueness. This prevents collisions when, for example, multiple animals in ExperimentPlotter have the same animal ID.\n\n        Args:\n            nbytes (int, optional): Number of bytes to generate. This is passed directly to secrets.token_hex(). Defaults to None, which generates 16 hex characters (8 bytes).\n        \"\"\"\n        import secrets\n\n        hash_suffix = secrets.token_hex(nbytes)\n        new_animal_id = f\"{self.animal_id}_{hash_suffix}\"\n\n        if \"animal\" in self.result.columns:\n            self.result[\"animal\"] = new_animal_id\n        if \"animalday\" in self.result.columns:\n            self.result[\"animalday\"] = self.result[\"animalday\"].str.replace(self.animal_id, new_animal_id)\n        self.animal_id = new_animal_id\n\n        self.__update_instance_vars()\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.WindowAnalysisResult.__init__","title":"<code>__init__(result, animal_id=None, genotype=None, channel_names=None, assume_from_number=False, bad_channels_dict={}, suppress_short_interval_error=False, lof_scores_dict={})</code>","text":"<p>Parameters:</p> Name Type Description Default <code>result</code> <code>DataFrame</code> <p>Result comes from AnimalOrganizer.compute_windowed_analysis()</p> required <code>animal_id</code> <code>str</code> <p>Identifier for the animal where result was computed from. Defaults to None.</p> <code>None</code> <code>genotype</code> <code>str</code> <p>Genotype of animal. Defaults to None.</p> <code>None</code> <code>channel_names</code> <code>list[str]</code> <p>List of channel names. Defaults to None.</p> <code>None</code> <code>assume_channels</code> <code>bool</code> <p>If true, assumes channel names according to AnimalFeatureParser.DEFAULT_CHNUM_TO_NAME. Defaults to False.</p> required <code>bad_channels_dict</code> <code>dict[str, list[str]]</code> <p>Dictionary of channels to reject for each recording session. Defaults to {}.</p> <code>{}</code> <code>suppress_short_interval_error</code> <code>bool</code> <p>If True, suppress ValueError for short intervals between timestamps. Useful for aggregated WARs with large window sizes. Defaults to False.</p> <code>False</code> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def __init__(\n    self,\n    result: pd.DataFrame,\n    animal_id: str = None,\n    genotype: str = None,\n    channel_names: list[str] = None,\n    assume_from_number=False,\n    bad_channels_dict: dict[str, list[str]] = {},\n    suppress_short_interval_error=False,\n    lof_scores_dict: dict[str, dict] = {},\n) -&gt; None:\n    \"\"\"\n    Args:\n        result (pd.DataFrame): Result comes from AnimalOrganizer.compute_windowed_analysis()\n        animal_id (str, optional): Identifier for the animal where result was computed from. Defaults to None.\n        genotype (str, optional): Genotype of animal. Defaults to None.\n        channel_names (list[str], optional): List of channel names. Defaults to None.\n        assume_channels (bool, optional): If true, assumes channel names according to AnimalFeatureParser.DEFAULT_CHNUM_TO_NAME. Defaults to False.\n        bad_channels_dict (dict[str, list[str]], optional): Dictionary of channels to reject for each recording session. Defaults to {}.\n        suppress_short_interval_error (bool, optional): If True, suppress ValueError for short intervals between timestamps. Useful for aggregated WARs with large window sizes. Defaults to False.\n    \"\"\"\n    self.result = result\n    self.animal_id = animal_id\n    self.genotype = genotype\n    self.channel_names = channel_names\n    self.assume_from_number = assume_from_number\n    self.bad_channels_dict = bad_channels_dict.copy()\n    self.suppress_short_interval_error = suppress_short_interval_error\n    self.lof_scores_dict = lof_scores_dict\n\n    self.__update_instance_vars()\n\n    logging.info(f\"Channel names: \\t{self.channel_names}\")\n    logging.info(f\"Channel abbreviations: \\t{self.channel_abbrevs}\")\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.WindowAnalysisResult.__update_instance_vars","title":"<code>__update_instance_vars()</code>","text":"<p>Run after updating self.result, or other init values</p> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def __update_instance_vars(self):\n    \"\"\"Run after updating self.result, or other init values\"\"\"\n    if \"index\" in self.result.columns:\n        warnings.warn(\"Dropping column 'index'\")\n        self.result = self.result.drop(columns=[\"index\"])\n\n    # Check if timestamps are sorted and sort if needed\n    if \"timestamp\" in self.result.columns:\n        if not self.result[\"timestamp\"].is_monotonic_increasing:\n            warnings.warn(\"Timestamps are not sorted. Sorting result DataFrame by timestamp.\")\n            self.result = self.result.sort_values(\"timestamp\")\n\n    # Check for unusually short intervals between timestamps\n    if \"timestamp\" in self.result.columns and \"duration\" in self.result.columns:\n        median_duration = self.result[\"duration\"].median()\n        timestamp_diffs = self.result[\"timestamp\"].diff()\n        short_intervals = timestamp_diffs &lt; pd.Timedelta(seconds=median_duration)\n\n        # Skip first row since diff() produces NaT\n        short_intervals = short_intervals[1:]\n\n        if short_intervals.any():\n            n_short = short_intervals.sum()\n            pct_short = (n_short / len(short_intervals)) * 100\n\n            warning_msg = (\n                f\"Found {n_short} intervals ({pct_short:.1f}%) between timestamps \"\n                f\"that are shorter than the median duration of {median_duration:.1f}s\"\n            )\n\n            if pct_short &gt; 1.0 and not self.suppress_short_interval_error:  # More than 1% of intervals are short\n                raise ValueError(warning_msg)\n            elif not self.suppress_short_interval_error:\n                warnings.warn(warning_msg)\n\n    if \"animal\" in self.result.columns:\n        unique_animals = self.result[\"animal\"].unique()\n        if len(unique_animals) &gt; 1:\n            raise ValueError(f\"Multiple animals found in result: {unique_animals}\")\n        if unique_animals[0] != self.animal_id:\n            raise ValueError(\n                f\"Animal ID mismatch: result has {unique_animals[0]}, but self.animal_id is {self.animal_id}\"\n            )\n\n    self._feature_columns = [x for x in self.result.columns if x in constants.FEATURES]\n    self._nonfeature_columns = [x for x in self.result.columns if x not in constants.FEATURES]\n    self.animaldays = self.result.loc[:, \"animalday\"].unique()\n\n    self.channel_abbrevs = [\n        core.parse_chname_to_abbrev(x, assume_from_number=self.assume_from_number) for x in self.channel_names\n    ]\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.WindowAnalysisResult.add_unique_hash","title":"<code>add_unique_hash(nbytes=None)</code>","text":"<p>Adds a hex hash to the animal ID to ensure uniqueness. This prevents collisions when, for example, multiple animals in ExperimentPlotter have the same animal ID.</p> <p>Parameters:</p> Name Type Description Default <code>nbytes</code> <code>int</code> <p>Number of bytes to generate. This is passed directly to secrets.token_hex(). Defaults to None, which generates 16 hex characters (8 bytes).</p> <code>None</code> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def add_unique_hash(self, nbytes: int | None = None):\n    \"\"\"Adds a hex hash to the animal ID to ensure uniqueness. This prevents collisions when, for example, multiple animals in ExperimentPlotter have the same animal ID.\n\n    Args:\n        nbytes (int, optional): Number of bytes to generate. This is passed directly to secrets.token_hex(). Defaults to None, which generates 16 hex characters (8 bytes).\n    \"\"\"\n    import secrets\n\n    hash_suffix = secrets.token_hex(nbytes)\n    new_animal_id = f\"{self.animal_id}_{hash_suffix}\"\n\n    if \"animal\" in self.result.columns:\n        self.result[\"animal\"] = new_animal_id\n    if \"animalday\" in self.result.columns:\n        self.result[\"animalday\"] = self.result[\"animalday\"].str.replace(self.animal_id, new_animal_id)\n    self.animal_id = new_animal_id\n\n    self.__update_instance_vars()\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.WindowAnalysisResult.aggregate_time_windows","title":"<code>aggregate_time_windows(groupby=['animalday', 'isday'])</code>","text":"<p>Aggregate time windows into a single data point per groupby by averaging features. This reduces the number of rows in the result.</p> <p>Parameters:</p> Name Type Description Default <code>groupby</code> <code>list[str] | str</code> <p>Columns to group by. Defaults to ['animalday', 'isday'], which groups by animalday (recording session) and isday (day/night).</p> <code>['animalday', 'isday']</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>groupby must be from ['animalday', 'isday']</p> <code>ValueError</code> <p>Columns in groupby not found in result</p> <code>ValueError</code> <p>Columns in groupby are not constant in groups</p> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def aggregate_time_windows(self, groupby: list[str] | str = [\"animalday\", \"isday\"]) -&gt; None:\n    \"\"\"Aggregate time windows into a single data point per groupby by averaging features. This reduces the number of rows in the result.\n\n    Args:\n        groupby (list[str] | str, optional): Columns to group by. Defaults to ['animalday', 'isday'], which groups by animalday (recording session) and isday (day/night).\n\n    Raises:\n        ValueError: groupby must be from ['animalday', 'isday']\n        ValueError: Columns in groupby not found in result\n        ValueError: Columns in groupby are not constant in groups\n    \"\"\"\n    if isinstance(groupby, str):\n        groupby = [groupby]\n    if not all(col in [\"animalday\", \"isday\"] for col in groupby):\n        raise ValueError(f\"groupby must be from ['animalday', 'isday']. Got {groupby}\")\n    if not all(col in self.result.columns for col in groupby):\n        raise ValueError(f\"Columns {groupby} not found in result. Columns: {self.result.columns.tolist()}\")\n\n    features = [f for f in constants.FEATURES if f in self.result.columns]\n    logging.debug(f\"Aggregating {features}\")\n    result_grouped = self.result.groupby(groupby)\n\n    agg_dict = {}\n\n    if \"animalday\" not in groupby:\n        agg_dict[\"animalday\"] = lambda df: None\n    if \"isday\" not in groupby:\n        agg_dict[\"isday\"] = lambda df: None\n\n    constant_cols = [\"animal\", \"day\", \"genotype\"]\n    for col in constant_cols:\n        if col in self.result.columns:\n            is_constant = result_grouped[col].nunique() == 1\n            if not is_constant.all():\n                non_constant_groups = is_constant[~is_constant].index.tolist()\n                raise ValueError(f\"Column {col} is not constant in groups: {non_constant_groups}\")\n            agg_dict[col] = lambda df, col=col: df[col].iloc[0]\n\n    if \"duration\" in self.result.columns:\n        agg_dict[\"duration\"] = lambda df: np.sum(df[\"duration\"])\n\n    if \"endfile\" in self.result.columns:\n        agg_dict[\"endfile\"] = lambda df: df[\"endfile\"].iloc[-1]\n\n    if \"timestamp\" in self.result.columns:\n        agg_dict[\"timestamp\"] = lambda df: df[\"timestamp\"].iloc[0]\n\n    for feat in features:\n        agg_dict[feat] = lambda df, feat=feat: self._average_feature(df, feat, \"duration\")\n\n    aggregated_df = result_grouped.apply(\n        lambda df: pd.Series({col: agg_dict[col](df) for col in self.result.columns if col not in groupby})\n    )\n\n    self.result = aggregated_df.reset_index(drop=False)  # Keep animalday/isday as a column\n\n    self.suppress_short_interval_error = True\n    logging.info(\"Setting suppress_short_interval_error to True\")\n    self.__update_instance_vars()\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.WindowAnalysisResult.apply_filters","title":"<code>apply_filters(filter_config=None, min_valid_channels=3, morphological_smoothing_seconds=None)</code>","text":"<p>Apply multiple filters using configuration.</p> <p>Parameters:</p> Name Type Description Default <code>filter_config</code> <code>dict</code> <p>Dictionary of filter names and parameters. Available filters: 'logrms_range', 'high_rms', 'low_rms', 'high_beta', 'reject_channels', 'reject_channels_by_session', 'morphological_smoothing'</p> <code>None</code> <code>min_valid_channels</code> <code>int</code> <p>Minimum valid channels per window. Defaults to 3.</p> <code>3</code> <code>morphological_smoothing_seconds</code> <code>float</code> <p>Temporal smoothing window (deprecated, use config instead)</p> <code>None</code> <p>Returns:</p> Name Type Description <code>WindowAnalysisResult</code> <code>WindowAnalysisResult</code> <p>New filtered instance</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = {\n...     'logrms_range': {'z_range': 3},\n...     'high_rms': {'max_rms': 500},\n...     'reject_channels': {'bad_channels': ['LMot', 'RMot']},\n...     'morphological_smoothing': {'smoothing_seconds': 8.0}\n... }\n&gt;&gt;&gt; filtered_war = war.apply_filters(config)\n</code></pre> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def apply_filters(\n    self, filter_config: dict = None, min_valid_channels: int = 3, morphological_smoothing_seconds: float = None\n) -&gt; \"WindowAnalysisResult\":\n    \"\"\"Apply multiple filters using configuration.\n\n    Args:\n        filter_config (dict, optional): Dictionary of filter names and parameters.\n            Available filters: 'logrms_range', 'high_rms', 'low_rms', 'high_beta',\n            'reject_channels', 'reject_channels_by_session', 'morphological_smoothing'\n        min_valid_channels (int): Minimum valid channels per window. Defaults to 3.\n        morphological_smoothing_seconds (float, optional): Temporal smoothing window (deprecated, use config instead)\n\n    Returns:\n        WindowAnalysisResult: New filtered instance\n\n    Examples:\n        &gt;&gt;&gt; config = {\n        ...     'logrms_range': {'z_range': 3},\n        ...     'high_rms': {'max_rms': 500},\n        ...     'reject_channels': {'bad_channels': ['LMot', 'RMot']},\n        ...     'morphological_smoothing': {'smoothing_seconds': 8.0}\n        ... }\n        &gt;&gt;&gt; filtered_war = war.apply_filters(config)\n    \"\"\"\n    if filter_config is None:\n        filter_config = {\n            \"logrms_range\": {\"z_range\": 3},\n            \"high_rms\": {\"max_rms\": 500},\n            \"low_rms\": {\"min_rms\": 50},\n            \"high_beta\": {\"max_beta_prop\": 0.4},\n            \"reject_channels_by_session\": {},\n        }\n\n    filter_methods = {\n        \"logrms_range\": self.get_filter_logrms_range,\n        \"high_rms\": self.get_filter_high_rms,\n        \"low_rms\": self.get_filter_low_rms,\n        \"high_beta\": self.get_filter_high_beta,\n        \"reject_channels\": self.get_filter_reject_channels,\n        \"reject_channels_by_session\": self.get_filter_reject_channels_by_recording_session,\n    }\n\n    filt_bools = []\n    morphological_params = None\n\n    for filter_name, filter_params in filter_config.items():\n        if filter_name == \"morphological_smoothing\":\n            morphological_params = filter_params\n            continue\n\n        if filter_name not in filter_methods:\n            raise ValueError(\n                f\"Unknown filter: {filter_name}. Available: {list(filter_methods.keys()) + ['morphological_smoothing']}\"\n            )\n\n        filter_func = filter_methods[filter_name]\n        filt_bool = filter_func(**filter_params)\n        filt_bools.append(filt_bool)\n        logging.info(f\"{filter_name}: filtered {filt_bool.size - np.count_nonzero(filt_bool)}/{filt_bool.size}\")\n\n    # Combine all filter masks\n    if filt_bools:\n        filt_bool_all = np.prod(np.stack(filt_bools, axis=-1), axis=-1).astype(bool)\n    else:\n        filt_bool_all = np.ones((len(self.result), len(self.channel_names)), dtype=bool)\n\n    # Apply morphological smoothing if requested (either from config or parameter)\n    if morphological_params or morphological_smoothing_seconds is not None:\n        if morphological_params:\n            smoothing_seconds = morphological_params[\"smoothing_seconds\"]\n        else:\n            smoothing_seconds = morphological_smoothing_seconds\n\n        filt_bool_all = self.get_filter_morphological_smoothing(filt_bool_all, smoothing_seconds)\n        logging.info(f\"Applied morphological smoothing: {smoothing_seconds}s\")\n\n    # Filter windows based on minimum valid channels\n    valid_channels_per_window = np.sum(filt_bool_all, axis=1)\n    window_mask = valid_channels_per_window &gt;= min_valid_channels\n    filt_bool_all = filt_bool_all &amp; window_mask[:, np.newaxis]\n\n    return self._create_filtered_copy(filt_bool_all)\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.WindowAnalysisResult.evaluate_lof_threshold_binary","title":"<code>evaluate_lof_threshold_binary(ground_truth_bad_channels=None, threshold=None, evaluation_channels=None)</code>","text":"<p>Evaluate single threshold against ground truth for binary classification.</p> <p>Parameters:</p> Name Type Description Default <code>ground_truth_bad_channels</code> <code>dict</code> <p>Dict mapping animal-day to bad channel sets.                      If None, uses self.bad_channels_dict as ground truth.</p> <code>None</code> <code>threshold</code> <code>float</code> <p>LOF threshold to test</p> <code>None</code> <code>evaluation_channels</code> <code>list[str]</code> <p>Subset of channels to include in evaluation. If none, uses all channels.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple</code> <p>(y_true_list, y_pred_list) for sklearn.metrics.f1_score    Each element represents one channel from one animal-day</p> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def evaluate_lof_threshold_binary(\n    self, ground_truth_bad_channels: dict = None, threshold: float = None, evaluation_channels: list[str] = None\n) -&gt; tuple:\n    \"\"\"Evaluate single threshold against ground truth for binary classification.\n\n    Args:\n        ground_truth_bad_channels: Dict mapping animal-day to bad channel sets.\n                                 If None, uses self.bad_channels_dict as ground truth.\n        threshold: LOF threshold to test\n        evaluation_channels: Subset of channels to include in evaluation. If none, uses all channels.\n\n    Returns:\n        tuple: (y_true_list, y_pred_list) for sklearn.metrics.f1_score\n               Each element represents one channel from one animal-day\n    \"\"\"\n    if not hasattr(self, \"lof_scores_dict\") or not self.lof_scores_dict:\n        raise ValueError(\"LOF scores not available in this WAR. Run compute_bad_channels() first.\")\n\n    if threshold is None:\n        raise ValueError(\"threshold parameter is required\")\n\n    # Use self.bad_channels_dict as default ground truth\n    if ground_truth_bad_channels is None:\n        if hasattr(self, \"bad_channels_dict\") and self.bad_channels_dict:\n            ground_truth_bad_channels = {}\n\n            # Filter bad_channels_dict to only include keys that exist in lof_scores_dict\n            lof_keys = set(self.lof_scores_dict.keys())\n            bad_channels_keys = set(self.bad_channels_dict.keys())\n\n            missing_keys = bad_channels_keys - lof_keys\n            if missing_keys:\n                raise ValueError(\n                    f\"bad_channels_dict contains keys not found in lof_scores_dict: {missing_keys}. \"\n                    f\"Available LOF keys: {sorted(lof_keys)}\"\n                )\n\n            # Only use bad channel keys that have corresponding LOF data\n            ground_truth_bad_channels = {\n                key: value for key, value in self.bad_channels_dict.items() if key in lof_keys\n            }\n\n            logging.info(\n                f\"Using filtered bad_channels_dict as ground truth with {len(ground_truth_bad_channels)} animal-day sessions\"\n            )\n        else:\n            raise ValueError(\"No ground truth provided and self.bad_channels_dict is empty.\")\n\n    # Get all channels if no subset specified\n    if evaluation_channels is None:\n        evaluation_channels = self.channel_names\n\n    y_true_list = []\n    y_pred_list = []\n\n    # Debug: Log what we're working with\n    logging.debug(f\"evaluate_lof_threshold_binary: evaluation_channels = {evaluation_channels}\")\n    logging.debug(\n        f\"evaluate_lof_threshold_binary: ground_truth_bad_channels keys = {list(ground_truth_bad_channels.keys())}\"\n    )\n    logging.debug(f\"evaluate_lof_threshold_binary: lof_scores_dict keys = {list(self.lof_scores_dict.keys())}\")\n\n    # Iterate through each animal-day and evaluate channels\n    for animalday, lof_data in self.lof_scores_dict.items():\n        if \"lof_scores\" not in lof_data or \"channel_names\" not in lof_data:\n            raise ValueError(\n                f\"Invalid LOF data for {animalday}: missing required fields 'lof_scores' or 'channel_names'\"\n            )\n\n        scores = np.array(lof_data[\"lof_scores\"])\n        channel_names = lof_data[\"channel_names\"]\n\n        # Get ground truth bad channels for this animal-day\n        animalday_bad_channels = ground_truth_bad_channels.get(animalday, set())\n\n        # Debug: Log details for this animal-day\n        logging.debug(f\"Processing {animalday}: channel_names = {channel_names}\")\n        logging.debug(f\"Processing {animalday}: animalday_bad_channels = {animalday_bad_channels}\")\n        logging.debug(f\"Processing {animalday}: scores shape = {scores.shape}\")\n\n        # Evaluate each channel in the evaluation subset\n        channels_processed = 0\n        for i, channel in enumerate(channel_names):\n            if (\n                channel in evaluation_channels\n                or parse_chname_to_abbrev(channel, strict_matching=False) in evaluation_channels\n            ):\n                channels_processed += 1\n\n                # Ground truth: 1 if channel is marked as bad, 0 otherwise\n                is_bad_channel = (\n                    channel in animalday_bad_channels\n                    or parse_chname_to_abbrev(channel, strict_matching=False) in animalday_bad_channels\n                )\n                # if is_bad_channel and channel not in animalday_bad_channels:\n                #     logging.debug(f\"Mapped full channel '{channel}' -&gt; '{parse_chname_to_abbrev(channel, strict_matching=False)}' found in bad channels\")\n\n                y_true = 1 if is_bad_channel else 0\n                # Prediction: 1 if LOF score &gt; threshold, 0 otherwise\n                y_pred = 1 if scores[i] &gt; threshold else 0\n\n                y_true_list.append(y_true)\n                y_pred_list.append(y_pred)\n\n                logging.debug(\n                    f\"Channel {channel}: y_true={y_true}, y_pred={y_pred} (score={scores[i]:.3f}, threshold={threshold})\"\n                )\n\n                # Extra debugging for the alignment issue\n                if y_true == 1:\n                    logging.info(\n                        f\"TRUE POSITIVE CANDIDATE: {channel} mapped to bad channel in: {animalday_bad_channels}\"\n                    )\n                if y_pred == 1:\n                    logging.info(f\"LOF PREDICTION: {channel} has score {scores[i]:.3f} &gt; threshold {threshold}\")\n\n        logging.debug(f\"Processed {channels_processed} channels for {animalday}\")\n\n    return y_true_list, y_pred_list\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.WindowAnalysisResult.filter_all","title":"<code>filter_all(df=None, inplace=True, min_valid_channels=3, filters=None, morphological_smoothing_seconds=None, **kwargs)</code>","text":"<p>Apply a list of filters to the data. Filtering should be performed before aggregation.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>If not None, this function will use this dataframe instead of self.result. Defaults to None.</p> <code>None</code> <code>inplace</code> <code>bool</code> <p>If True, modify the result in place. Defaults to True.</p> <code>True</code> <code>bad_channels</code> <code>list[str]</code> <p>List of channels to reject. Defaults to None.</p> required <code>min_valid_channels</code> <code>int</code> <p>Minimum number of valid channels required per window. Defaults to 3.</p> <code>3</code> <code>filters</code> <code>list[callable]</code> <p>List of filter functions to apply. Each function should return a boolean mask. If None, uses default filters: [get_filter_logrms_range, get_filter_high_rms, get_filter_low_rms, get_filter_high_beta]. Defaults to None.</p> <code>None</code> <code>morphological_smoothing_seconds</code> <code>float</code> <p>If provided, apply morphological opening/closing to smooth the filter mask. This removes isolated false positives/negatives along the time axis for each channel independently. The value specifies the time window in seconds for the morphological operations. Defaults to None.</p> <code>None</code> <code>save_bad_channels</code> <code>Literal['overwrite', 'union', None]</code> <p>How to save bad channels to self.bad_channels_dict. This parameter is passed to the filtering functions. Defaults to \"union\". Note: When using \"overwrite\" mode, the bad_channels parameter and bad_channels_dict parameter may conflict and overwrite each other's bad channel definitions if both are provided.</p> required <code>**kwargs</code> <p>Additional keyword arguments to pass to filter functions.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>WindowAnalysisResult</code> <p>Filtered result</p> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def filter_all(\n    self,\n    df: pd.DataFrame = None,\n    inplace=True,\n    # bad_channels: list[str] = None,\n    min_valid_channels=3,\n    filters: list[callable] = None,\n    morphological_smoothing_seconds: float = None,\n    # save_bad_channels: Literal[\"overwrite\", \"union\", None] = \"union\",\n    **kwargs,\n):\n    \"\"\"Apply a list of filters to the data. Filtering should be performed before aggregation.\n\n    Args:\n        df (pd.DataFrame, optional): If not None, this function will use this dataframe instead of self.result. Defaults to None.\n        inplace (bool, optional): If True, modify the result in place. Defaults to True.\n        bad_channels (list[str], optional): List of channels to reject. Defaults to None.\n        min_valid_channels (int, optional): Minimum number of valid channels required per window. Defaults to 3.\n        filters (list[callable], optional): List of filter functions to apply. Each function should return a boolean mask.\n            If None, uses default filters: [get_filter_logrms_range, get_filter_high_rms, get_filter_low_rms, get_filter_high_beta].\n            Defaults to None.\n        morphological_smoothing_seconds (float, optional): If provided, apply morphological opening/closing to smooth the filter mask.\n            This removes isolated false positives/negatives along the time axis for each channel independently.\n            The value specifies the time window in seconds for the morphological operations. Defaults to None.\n        save_bad_channels (Literal[\"overwrite\", \"union\", None], optional): How to save bad channels to self.bad_channels_dict.\n            This parameter is passed to the filtering functions. Defaults to \"union\".\n            Note: When using \"overwrite\" mode, the bad_channels parameter and bad_channels_dict parameter\n            may conflict and overwrite each other's bad channel definitions if both are provided.\n        **kwargs: Additional keyword arguments to pass to filter functions.\n\n    Returns:\n        WindowAnalysisResult: Filtered result\n    \"\"\"\n    if filters is None:\n        # TODO refactor these into standalone functions, which take in a war as the first parameter, then pass\n        # filt_bool = filt(self, df, **kwargs) as needed\n        filters = [\n            self.get_filter_logrms_range,\n            self.get_filter_high_rms,\n            self.get_filter_low_rms,\n            self.get_filter_high_beta,\n            self.get_filter_reject_channels_by_recording_session,\n            self.get_filter_reject_channels,\n        ]\n\n    filt_bools = []\n    # Apply each filter function\n    for filter_function in filters:\n        filt_bool = filter_function(df, **kwargs)\n        filt_bools.append(filt_bool)\n        logging.info(\n            f\"{filter_function.__name__}:\\tfiltered {filt_bool.size - np.count_nonzero(filt_bool)}/{filt_bool.size}\"\n        )\n\n    # Apply all filters\n    filt_bool_all = np.prod(np.stack(filt_bools, axis=-1), axis=-1).astype(bool)\n    logging.debug(f\"filt_bool_all.shape: {filt_bool_all.shape}\")  # (windows, channels)\n\n    # Apply morphological smoothing if requested\n    if morphological_smoothing_seconds is not None:\n        if \"duration\" not in self.result.columns:\n            raise ValueError(\"Cannot calculate window duration - 'duration' column missing from result dataframe\")\n        window_duration = self.result[\"duration\"].median()\n\n        # Calculate number of windows for the smoothing\n        structure_size = max(1, int(morphological_smoothing_seconds / window_duration))\n\n        if structure_size &gt; 1:\n            logging.info(\n                f\"Applying morphological smoothing with {structure_size} windows ({morphological_smoothing_seconds}s / {window_duration}s per window)\"\n            )\n            # Apply channel-wise temporal smoothing (each channel processed independently)\n            # This avoids spatial assumptions while smoothing temporal artifacts\n            for ch_idx in range(filt_bool_all.shape[1]):\n                channel_mask = filt_bool_all[:, ch_idx]\n                # Opening removes small isolated artifacts\n                channel_mask = binary_opening(channel_mask, structure=np.ones(structure_size))\n                # Closing fills small gaps in valid data\n                channel_mask = binary_closing(channel_mask, structure=np.ones(structure_size))\n                filt_bool_all[:, ch_idx] = channel_mask\n        else:\n            logging.info(\"Skipping morphological smoothing - structure size would be 1 (no effect)\")\n\n    # Filter windows based on number of valid channels\n    valid_channels_per_window = np.sum(filt_bool_all, axis=1)  # axis 1 = channel\n    window_mask = valid_channels_per_window &gt;= min_valid_channels  # True if window has enough valid channels\n    filt_bool_all = filt_bool_all &amp; window_mask[:, np.newaxis]  # Apply window mask to all channels\n\n    filtered_result = self._apply_filter(filt_bool_all)\n    if inplace:\n        del self.result\n        self.result = filtered_result\n    return WindowAnalysisResult(\n        filtered_result,\n        self.animal_id,\n        self.genotype,\n        self.channel_names,\n        self.assume_from_number,\n        self.bad_channels_dict.copy(),\n        self.suppress_short_interval_error,\n        self.lof_scores_dict.copy(),\n    )\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.WindowAnalysisResult.filter_high_beta","title":"<code>filter_high_beta(max_beta_prop=0.4)</code>","text":"<p>Filter out windows with high beta power.</p> <p>Parameters:</p> Name Type Description Default <code>max_beta_prop</code> <code>float</code> <p>Maximum beta power proportion. Defaults to 0.4.</p> <code>0.4</code> <p>Returns:</p> Name Type Description <code>WindowAnalysisResult</code> <code>WindowAnalysisResult</code> <p>New filtered instance</p> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def filter_high_beta(self, max_beta_prop: float = 0.4) -&gt; \"WindowAnalysisResult\":\n    \"\"\"Filter out windows with high beta power.\n\n    Args:\n        max_beta_prop (float): Maximum beta power proportion. Defaults to 0.4.\n\n    Returns:\n        WindowAnalysisResult: New filtered instance\n    \"\"\"\n    mask = self.get_filter_high_beta(max_beta_prop=max_beta_prop)\n    return self._create_filtered_copy(mask)\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.WindowAnalysisResult.filter_high_rms","title":"<code>filter_high_rms(max_rms=500)</code>","text":"<p>Filter out windows with RMS above threshold.</p> <p>Parameters:</p> Name Type Description Default <code>max_rms</code> <code>float</code> <p>Maximum RMS threshold. Defaults to 500.</p> <code>500</code> <p>Returns:</p> Name Type Description <code>WindowAnalysisResult</code> <code>WindowAnalysisResult</code> <p>New filtered instance</p> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def filter_high_rms(self, max_rms: float = 500) -&gt; \"WindowAnalysisResult\":\n    \"\"\"Filter out windows with RMS above threshold.\n\n    Args:\n        max_rms (float): Maximum RMS threshold. Defaults to 500.\n\n    Returns:\n        WindowAnalysisResult: New filtered instance\n    \"\"\"\n    mask = self.get_filter_high_rms(max_rms=max_rms)\n    return self._create_filtered_copy(mask)\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.WindowAnalysisResult.filter_logrms_range","title":"<code>filter_logrms_range(z_range=3)</code>","text":"<p>Filter based on log(rms) z-score range.</p> <p>Parameters:</p> Name Type Description Default <code>z_range</code> <code>float</code> <p>Z-score range threshold. Defaults to 3.</p> <code>3</code> <p>Returns:</p> Name Type Description <code>WindowAnalysisResult</code> <code>WindowAnalysisResult</code> <p>New filtered instance</p> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def filter_logrms_range(self, z_range: float = 3) -&gt; \"WindowAnalysisResult\":\n    \"\"\"Filter based on log(rms) z-score range.\n\n    Args:\n        z_range (float): Z-score range threshold. Defaults to 3.\n\n    Returns:\n        WindowAnalysisResult: New filtered instance\n    \"\"\"\n    mask = self.get_filter_logrms_range(z_range=z_range)\n    return self._create_filtered_copy(mask)\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.WindowAnalysisResult.filter_low_rms","title":"<code>filter_low_rms(min_rms=50)</code>","text":"<p>Filter out windows with RMS below threshold.</p> <p>Parameters:</p> Name Type Description Default <code>min_rms</code> <code>float</code> <p>Minimum RMS threshold. Defaults to 50.</p> <code>50</code> <p>Returns:</p> Name Type Description <code>WindowAnalysisResult</code> <code>WindowAnalysisResult</code> <p>New filtered instance</p> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def filter_low_rms(self, min_rms: float = 50) -&gt; \"WindowAnalysisResult\":\n    \"\"\"Filter out windows with RMS below threshold.\n\n    Args:\n        min_rms (float): Minimum RMS threshold. Defaults to 50.\n\n    Returns:\n        WindowAnalysisResult: New filtered instance\n    \"\"\"\n    mask = self.get_filter_low_rms(min_rms=min_rms)\n    return self._create_filtered_copy(mask)\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.WindowAnalysisResult.filter_morphological_smoothing","title":"<code>filter_morphological_smoothing(smoothing_seconds)</code>","text":"<p>Apply morphological smoothing to all data.</p> <p>Parameters:</p> Name Type Description Default <code>smoothing_seconds</code> <code>float</code> <p>Time window in seconds for morphological operations</p> required <p>Returns:</p> Name Type Description <code>WindowAnalysisResult</code> <code>WindowAnalysisResult</code> <p>New filtered instance</p> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def filter_morphological_smoothing(self, smoothing_seconds: float) -&gt; \"WindowAnalysisResult\":\n    \"\"\"Apply morphological smoothing to all data.\n\n    Args:\n        smoothing_seconds (float): Time window in seconds for morphological operations\n\n    Returns:\n        WindowAnalysisResult: New filtered instance\n    \"\"\"\n    # Start with all-True mask and smooth it\n    base_mask = np.ones((len(self.result), len(self.channel_names)), dtype=bool)\n    smoothed_mask = self.get_filter_morphological_smoothing(base_mask, smoothing_seconds)\n    return self._create_filtered_copy(smoothed_mask)\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.WindowAnalysisResult.filter_reject_channels","title":"<code>filter_reject_channels(bad_channels, use_abbrevs=None)</code>","text":"<p>Filter out specified bad channels.</p> <p>Parameters:</p> Name Type Description Default <code>bad_channels</code> <code>list[str]</code> <p>List of channel names to reject</p> required <code>use_abbrevs</code> <code>bool</code> <p>Whether to use abbreviations. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>WindowAnalysisResult</code> <code>WindowAnalysisResult</code> <p>New filtered instance</p> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def filter_reject_channels(self, bad_channels: list[str], use_abbrevs: bool = None) -&gt; \"WindowAnalysisResult\":\n    \"\"\"Filter out specified bad channels.\n\n    Args:\n        bad_channels (list[str]): List of channel names to reject\n        use_abbrevs (bool, optional): Whether to use abbreviations. Defaults to None.\n\n    Returns:\n        WindowAnalysisResult: New filtered instance\n    \"\"\"\n    mask = self.get_filter_reject_channels(bad_channels=bad_channels, use_abbrevs=use_abbrevs)\n    return self._create_filtered_copy(mask)\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.WindowAnalysisResult.filter_reject_channels_by_session","title":"<code>filter_reject_channels_by_session(bad_channels_dict=None, use_abbrevs=None)</code>","text":"<p>Filter out bad channels by recording session.</p> <p>Parameters:</p> Name Type Description Default <code>bad_channels_dict</code> <code>dict[str, list[str]]</code> <p>Dictionary mapping recording session identifiers to lists of bad channel names to reject. Session identifiers are in the format \"{animal_id} {genotype} {day}\" (e.g., \"A10 WT Apr-01-2023\"). Channel names can be either full names (e.g., \"Left Auditory\") or abbreviations (e.g., \"LAud\"). If None, uses the bad_channels_dict from the constructor. Defaults to None.</p> <code>None</code> <code>use_abbrevs</code> <code>bool</code> <p>Override automatic channel name format detection. If True, channels are assumed to be abbreviations. If False, channels are assumed to be full names. If None, automatically detects format and converts to abbreviations for matching. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>WindowAnalysisResult</code> <code>WindowAnalysisResult</code> <p>New filtered instance with bad channels masked as NaN for their respective recording sessions</p> <p>Examples:</p> <p>Filter specific channels per session using abbreviations:</p> <pre><code>&gt;&gt;&gt; bad_channels = {\n...     \"A10 WT Apr-01-2023\": [\"LAud\", \"RMot\"],  # Session 1: reject left auditory, right motor\n...     \"A10 WT Apr-02-2023\": [\"LVis\"]           # Session 2: reject left visual only\n... }\n&gt;&gt;&gt; filtered_war = war.filter_reject_channels_by_session(bad_channels, use_abbrevs=True)\n</code></pre> <p>Filter using full channel names:</p> <pre><code>&gt;&gt;&gt; bad_channels = {\n...     \"A12 KO May-15-2023\": [\"Left Motor\", \"Right Barrel\"],\n...     \"A12 KO May-16-2023\": [\"Left Auditory\", \"Left Visual\", \"Right Motor\"]\n... }\n&gt;&gt;&gt; filtered_war = war.filter_reject_channels_by_session(bad_channels, use_abbrevs=False)\n</code></pre> <p>Auto-detect channel format (recommended):</p> <pre><code>&gt;&gt;&gt; bad_channels = {\n...     \"A15 WT Jun-10-2023\": [\"LMot\", \"RBar\"],  # Will auto-detect as abbreviations\n...     \"A15 WT Jun-11-2023\": [\"LAud\"]\n... }\n&gt;&gt;&gt; filtered_war = war.filter_reject_channels_by_session(bad_channels)\n</code></pre> Note <ul> <li>Session identifiers must exactly match the \"animalday\" values in the result DataFrame</li> <li>Available channel abbreviations: LAud, RAud, LVis, RVis, LHip, RHip, LBar, RBar, LMot, RMot</li> <li>Channel names are case-insensitive and support various formats (e.g., \"left aud\", \"Left Auditory\")</li> <li>If a session identifier is not found in bad_channels_dict, a warning is logged but processing continues</li> <li>If a channel name is not recognized, a warning is logged but other channels are still processed</li> </ul> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def filter_reject_channels_by_session(\n    self, bad_channels_dict: dict[str, list[str]] = None, use_abbrevs: bool = None\n) -&gt; \"WindowAnalysisResult\":\n    \"\"\"Filter out bad channels by recording session.\n\n    Args:\n        bad_channels_dict (dict[str, list[str]], optional): Dictionary mapping recording session\n            identifiers to lists of bad channel names to reject. Session identifiers are in the\n            format \"{animal_id} {genotype} {day}\" (e.g., \"A10 WT Apr-01-2023\"). Channel names\n            can be either full names (e.g., \"Left Auditory\") or abbreviations (e.g., \"LAud\").\n            If None, uses the bad_channels_dict from the constructor. Defaults to None.\n        use_abbrevs (bool, optional): Override automatic channel name format detection. If True,\n            channels are assumed to be abbreviations. If False, channels are assumed to be full\n            names. If None, automatically detects format and converts to abbreviations for matching.\n            Defaults to None.\n\n    Returns:\n        WindowAnalysisResult: New filtered instance with bad channels masked as NaN for their\n            respective recording sessions\n\n    Examples:\n        Filter specific channels per session using abbreviations:\n        &gt;&gt;&gt; bad_channels = {\n        ...     \"A10 WT Apr-01-2023\": [\"LAud\", \"RMot\"],  # Session 1: reject left auditory, right motor\n        ...     \"A10 WT Apr-02-2023\": [\"LVis\"]           # Session 2: reject left visual only\n        ... }\n        &gt;&gt;&gt; filtered_war = war.filter_reject_channels_by_session(bad_channels, use_abbrevs=True)\n\n        Filter using full channel names:\n        &gt;&gt;&gt; bad_channels = {\n        ...     \"A12 KO May-15-2023\": [\"Left Motor\", \"Right Barrel\"],\n        ...     \"A12 KO May-16-2023\": [\"Left Auditory\", \"Left Visual\", \"Right Motor\"]\n        ... }\n        &gt;&gt;&gt; filtered_war = war.filter_reject_channels_by_session(bad_channels, use_abbrevs=False)\n\n        Auto-detect channel format (recommended):\n        &gt;&gt;&gt; bad_channels = {\n        ...     \"A15 WT Jun-10-2023\": [\"LMot\", \"RBar\"],  # Will auto-detect as abbreviations\n        ...     \"A15 WT Jun-11-2023\": [\"LAud\"]\n        ... }\n        &gt;&gt;&gt; filtered_war = war.filter_reject_channels_by_session(bad_channels)\n\n    Note:\n        - Session identifiers must exactly match the \"animalday\" values in the result DataFrame\n        - Available channel abbreviations: LAud, RAud, LVis, RVis, LHip, RHip, LBar, RBar, LMot, RMot\n        - Channel names are case-insensitive and support various formats (e.g., \"left aud\", \"Left Auditory\")\n        - If a session identifier is not found in bad_channels_dict, a warning is logged but processing continues\n        - If a channel name is not recognized, a warning is logged but other channels are still processed\n    \"\"\"\n    mask = self.get_filter_reject_channels_by_recording_session(\n        bad_channels_dict=bad_channels_dict, use_abbrevs=use_abbrevs\n    )\n    return self._create_filtered_copy(mask)\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.WindowAnalysisResult.get_bad_channels_by_lof_threshold","title":"<code>get_bad_channels_by_lof_threshold(lof_threshold)</code>","text":"<p>Apply LOF threshold directly to stored scores to get bad channels.</p> <p>Parameters:</p> Name Type Description Default <code>lof_threshold</code> <code>float</code> <p>Threshold for determining bad channels.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary mapping animal days to lists of bad channel names.</p> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def get_bad_channels_by_lof_threshold(self, lof_threshold: float) -&gt; dict:\n    \"\"\"Apply LOF threshold directly to stored scores to get bad channels.\n\n    Args:\n        lof_threshold (float): Threshold for determining bad channels.\n\n    Returns:\n        dict: Dictionary mapping animal days to lists of bad channel names.\n    \"\"\"\n    if not hasattr(self, \"lof_scores_dict\") or not self.lof_scores_dict:\n        raise ValueError(\"LOF scores not available in this WAR. Compute LOF scores first.\")\n\n    bad_channels_dict = {}\n    for animalday, lof_data in self.lof_scores_dict.items():\n        if \"lof_scores\" in lof_data and \"channel_names\" in lof_data:\n            scores = np.array(lof_data[\"lof_scores\"])\n            channel_names = lof_data[\"channel_names\"]\n\n            is_inlier = scores &lt; lof_threshold\n            bad_channels = [channel_names[i] for i in np.where(~is_inlier)[0]]\n            bad_channels_dict[animalday] = bad_channels\n        else:\n            raise ValueError(f\"LOF scores not available for {animalday}\")\n\n    return bad_channels_dict\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.WindowAnalysisResult.get_filter_high_beta","title":"<code>get_filter_high_beta(df=None, max_beta_prop=0.4, **kwargs)</code>","text":"<p>Filter windows based on beta power.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>If not None, this function will use this dataframe instead of self.result. Defaults to None.</p> <code>None</code> <code>max_beta_prop</code> <code>float</code> <p>The maximum beta power to filter by. Values above this will be set to NaN. Defaults to 0.4.</p> <code>0.4</code> <p>Returns:</p> Name Type Description <code>out</code> <p>np.ndarray of bool, (M fragments, N channels). True = keep window, False = remove window</p> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def get_filter_high_beta(self, df: pd.DataFrame = None, max_beta_prop=0.4, **kwargs):\n    \"\"\"Filter windows based on beta power.\n\n    Args:\n        df (pd.DataFrame, optional): If not None, this function will use this dataframe instead of self.result. Defaults to None.\n        max_beta_prop (float, optional): The maximum beta power to filter by. Values above this will be set to NaN. Defaults to 0.4.\n\n    Returns:\n        out: np.ndarray of bool, (M fragments, N channels). True = keep window, False = remove window\n    \"\"\"\n    result = df.copy() if df is not None else self.result.copy()\n    if \"psdfrac\" in result.columns:\n        df_psdfrac = pd.DataFrame(result[\"psdfrac\"].tolist())\n        np_prop = np.array(df_psdfrac[\"beta\"].tolist())\n    elif \"psdband\" in result.columns and \"psdtotal\" in result.columns:\n        df_psdband = pd.DataFrame(result[\"psdband\"].tolist())\n        np_beta = np.array(df_psdband[\"beta\"].tolist())\n        np_total = np.array(result[\"psdtotal\"].tolist())\n        np_prop = np_beta / np_total\n    else:\n        raise ValueError(\"psdfrac or psdband+psdtotal required for beta power filtering\")\n\n    out = np.full(np_prop.shape, True)\n    out[np_prop &gt; max_beta_prop] = False\n    out = np.broadcast_to(np.all(out, axis=-1)[:, np.newaxis], out.shape)\n    return out\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.WindowAnalysisResult.get_filter_high_rms","title":"<code>get_filter_high_rms(df=None, max_rms=500, **kwargs)</code>","text":"<p>Filter windows based on rms.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>If not None, this function will use this dataframe instead of self.result. Defaults to None.</p> <code>None</code> <code>max_rms</code> <code>float</code> <p>The maximum rms value to filter by. Values above this will be set to NaN.</p> <code>500</code> <p>Returns:</p> Name Type Description <code>out</code> <p>np.ndarray of bool, (M fragments, N channels). True = keep window, False = remove window</p> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def get_filter_high_rms(self, df: pd.DataFrame = None, max_rms=500, **kwargs):\n    \"\"\"Filter windows based on rms.\n\n    Args:\n        df (pd.DataFrame, optional): If not None, this function will use this dataframe instead of self.result. Defaults to None.\n        max_rms (float, optional): The maximum rms value to filter by. Values above this will be set to NaN.\n\n    Returns:\n        out: np.ndarray of bool, (M fragments, N channels). True = keep window, False = remove window\n    \"\"\"\n    result = df.copy() if df is not None else self.result.copy()\n    np_rms = np.array(result[\"rms\"].tolist())\n    np_rmsnan = np_rms.copy()\n    # Convert to float to allow NaN assignment for integer arrays\n    if np_rmsnan.dtype.kind in (\"i\", \"u\"):  # integer types\n        np_rmsnan = np_rmsnan.astype(float)\n    np_rmsnan[np_rms &gt; max_rms] = np.nan\n    result[\"rms\"] = np_rmsnan.tolist()\n\n    out = np.full(np_rms.shape, True)\n    out[np_rms &gt; max_rms] = False\n    return out\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.WindowAnalysisResult.get_filter_logrms_range","title":"<code>get_filter_logrms_range(df=None, z_range=3, **kwargs)</code>","text":"<p>Filter windows based on log(rms).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>If not None, this function will use this dataframe instead of self.result. Defaults to None.</p> <code>None</code> <code>z_range</code> <code>float</code> <p>The z-score range to filter by. Values outside this range will be set to NaN.</p> <code>3</code> <p>Returns:</p> Name Type Description <code>out</code> <p>np.ndarray of bool, (M fragments, N channels). True = keep window, False = remove window</p> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def get_filter_logrms_range(self, df: pd.DataFrame = None, z_range=3, **kwargs):\n    \"\"\"Filter windows based on log(rms).\n\n    Args:\n        df (pd.DataFrame, optional): If not None, this function will use this dataframe instead of self.result. Defaults to None.\n        z_range (float, optional): The z-score range to filter by. Values outside this range will be set to NaN.\n\n    Returns:\n        out: np.ndarray of bool, (M fragments, N channels). True = keep window, False = remove window\n    \"\"\"\n    result = df.copy() if df is not None else self.result.copy()\n    z_range = abs(z_range)\n    np_rms = np.array(result[\"rms\"].tolist())\n    np_logrms = np.log(np_rms)\n    del np_rms\n    np_logrmsz = zscore(np_logrms, axis=0, nan_policy=\"omit\")\n    np_logrms[(np_logrmsz &gt; z_range) | (np_logrmsz &lt; -z_range)] = np.nan\n\n    out = np.full(np_logrms.shape, True)\n    out[(np_logrmsz &gt; z_range) | (np_logrmsz &lt; -z_range)] = False\n    return out\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.WindowAnalysisResult.get_filter_low_rms","title":"<code>get_filter_low_rms(df=None, min_rms=30, **kwargs)</code>","text":"<p>Filter windows based on rms.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>If not None, this function will use this dataframe instead of self.result. Defaults to None.</p> <code>None</code> <code>min_rms</code> <code>float</code> <p>The minimum rms value to filter by. Values below this will be set to NaN.</p> <code>30</code> <p>Returns:</p> Name Type Description <code>out</code> <p>np.ndarray of bool, (M fragments, N channels). True = keep window, False = remove window</p> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def get_filter_low_rms(self, df: pd.DataFrame = None, min_rms=30, **kwargs):\n    \"\"\"Filter windows based on rms.\n\n    Args:\n        df (pd.DataFrame, optional): If not None, this function will use this dataframe instead of self.result. Defaults to None.\n        min_rms (float, optional): The minimum rms value to filter by. Values below this will be set to NaN.\n\n    Returns:\n        out: np.ndarray of bool, (M fragments, N channels). True = keep window, False = remove window\n    \"\"\"\n    result = df.copy() if df is not None else self.result.copy()\n    np_rms = np.array(result[\"rms\"].tolist())\n    np_rmsnan = np_rms.copy()\n    np_rmsnan[np_rms &lt; min_rms] = np.nan\n    result[\"rms\"] = np_rmsnan.tolist()\n\n    out = np.full(np_rms.shape, True)\n    out[np_rms &lt; min_rms] = False\n    return out\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.WindowAnalysisResult.get_filter_morphological_smoothing","title":"<code>get_filter_morphological_smoothing(filter_mask, smoothing_seconds, **kwargs)</code>","text":"<p>Apply morphological smoothing to a filter mask.</p> <p>Parameters:</p> Name Type Description Default <code>filter_mask</code> <code>ndarray</code> <p>Input boolean mask of shape (n_windows, n_channels)</p> required <code>smoothing_seconds</code> <code>float</code> <p>Time window in seconds for morphological operations</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Smoothed boolean mask</p> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def get_filter_morphological_smoothing(\n    self, filter_mask: np.ndarray, smoothing_seconds: float, **kwargs\n) -&gt; np.ndarray:\n    \"\"\"Apply morphological smoothing to a filter mask.\n\n    Args:\n        filter_mask (np.ndarray): Input boolean mask of shape (n_windows, n_channels)\n        smoothing_seconds (float): Time window in seconds for morphological operations\n\n    Returns:\n        np.ndarray: Smoothed boolean mask\n    \"\"\"\n    if \"duration\" not in self.result.columns:\n        raise ValueError(\"Cannot calculate window duration - 'duration' column missing\")\n\n    window_duration = self.result[\"duration\"].median()\n    structure_size = max(1, int(smoothing_seconds / window_duration))\n\n    if structure_size &lt;= 1:\n        return filter_mask\n\n    smoothed_mask = filter_mask.copy()\n    for ch_idx in range(filter_mask.shape[1]):\n        channel_mask = filter_mask[:, ch_idx]\n        # Opening removes small isolated artifacts\n        channel_mask = binary_opening(channel_mask, structure=np.ones(structure_size))\n        # Closing fills small gaps in valid data\n        channel_mask = binary_closing(channel_mask, structure=np.ones(structure_size))\n        smoothed_mask[:, ch_idx] = channel_mask\n\n    return smoothed_mask\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.WindowAnalysisResult.get_filter_reject_channels","title":"<code>get_filter_reject_channels(df=None, bad_channels=None, use_abbrevs=None, save_bad_channels='union', **kwargs)</code>","text":"<p>Filter channels to reject.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>If not None, this function will use this dataframe instead of self.result. Defaults to None.</p> <code>None</code> <code>bad_channels</code> <code>list[str]</code> <p>List of channels to reject. Can be either full channel names or abbreviations. The method will automatically detect which format is being used. If None, no filtering is performed.</p> <code>None</code> <code>use_abbrevs</code> <code>bool</code> <p>Override automatic detection. If True, channels are assumed to be channel abbreviations. If False, channels are assumed to be channel names. If None, channels are parsed to abbreviations and matched against self.channel_abbrevs.</p> <code>None</code> <code>save_bad_channels</code> <code>Literal['overwrite', 'union', None]</code> <p>How to save bad channels to self.bad_channels_dict. \"overwrite\": Replace self.bad_channels_dict completely with bad channels applied to all sessions. \"union\": Merge bad channels with existing self.bad_channels_dict for all sessions. None: Don't save to self.bad_channels_dict. Defaults to \"union\". Note: When using \"overwrite\" mode, the bad_channels parameter and bad_channels_dict parameter may conflict and overwrite each other's bad channel definitions if both are provided.</p> <code>'union'</code> <p>Returns:</p> Name Type Description <code>out</code> <p>np.ndarray of bool, (M fragments, N channels). True = keep window, False = remove window</p> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def get_filter_reject_channels(\n    self,\n    df: pd.DataFrame = None,\n    bad_channels: list[str] = None,\n    use_abbrevs: bool = None,\n    save_bad_channels: Literal[\"overwrite\", \"union\", None] = \"union\",\n    **kwargs,\n):\n    \"\"\"Filter channels to reject.\n\n    Args:\n        df (pd.DataFrame, optional): If not None, this function will use this dataframe instead of self.result. Defaults to None.\n        bad_channels (list[str]): List of channels to reject. Can be either full channel names or abbreviations.\n            The method will automatically detect which format is being used. If None, no filtering is performed.\n        use_abbrevs (bool, optional): Override automatic detection. If True, channels are assumed to be channel abbreviations. If False, channels are assumed to be channel names.\n            If None, channels are parsed to abbreviations and matched against self.channel_abbrevs.\n        save_bad_channels (Literal[\"overwrite\", \"union\", None], optional): How to save bad channels to self.bad_channels_dict.\n            \"overwrite\": Replace self.bad_channels_dict completely with bad channels applied to all sessions.\n            \"union\": Merge bad channels with existing self.bad_channels_dict for all sessions.\n            None: Don't save to self.bad_channels_dict. Defaults to \"union\".\n            Note: When using \"overwrite\" mode, the bad_channels parameter and bad_channels_dict parameter\n            may conflict and overwrite each other's bad channel definitions if both are provided.\n\n    Returns:\n        out: np.ndarray of bool, (M fragments, N channels). True = keep window, False = remove window\n    \"\"\"\n    n_samples = len(self.result)\n    n_channels = len(self.channel_names)\n    mask = np.ones((n_samples, n_channels), dtype=bool)\n\n    if bad_channels is None:\n        return mask\n\n    channel_targets = (\n        self.channel_abbrevs if use_abbrevs or use_abbrevs is None else self.channel_names\n    )  # Match to appropriate target\n    if use_abbrevs is None:  # Match channels as abbreviations\n        bad_channels = [\n            core.parse_chname_to_abbrev(ch, assume_from_number=self.assume_from_number) for ch in bad_channels\n        ]\n\n    # Match channels to channel_targets\n    for ch in bad_channels:\n        if ch in channel_targets:\n            mask[:, channel_targets.index(ch)] = False\n        else:\n            warnings.warn(f\"Channel {ch} not found in {channel_targets}\")\n\n    # Save bad channels to self.bad_channels_dict if requested\n    if save_bad_channels is not None:\n        # Get all unique animal days from the result\n        animaldays = self.result[\"animalday\"].unique()\n\n        # Convert bad channels to the format used in bad_channels_dict (original channel names)\n        channels_to_save = (\n            bad_channels.copy()\n            if use_abbrevs is False\n            else [\n                core.parse_chname_to_abbrev(ch, assume_from_number=self.assume_from_number) for ch in bad_channels\n            ]\n        )\n\n        if save_bad_channels == \"overwrite\":\n            # Replace entire dict with bad channels applied to all sessions\n            self.bad_channels_dict = {animalday: channels_to_save.copy() for animalday in animaldays}\n        elif save_bad_channels == \"union\":\n            # Merge with existing bad channels for all sessions\n            updated_dict = self.bad_channels_dict.copy()\n            for animalday in animaldays:\n                if animalday in updated_dict:\n                    # Union of existing and new channels\n                    updated_dict[animalday] = list(set(updated_dict[animalday]) | set(channels_to_save))\n                else:\n                    updated_dict[animalday] = channels_to_save.copy()\n            self.bad_channels_dict = updated_dict\n\n    return mask\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.WindowAnalysisResult.get_filter_reject_channels_by_recording_session","title":"<code>get_filter_reject_channels_by_recording_session(df=None, bad_channels_dict=None, use_abbrevs=None, save_bad_channels='union', **kwargs)</code>","text":"<p>Filter channels to reject for each recording session</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>If not None, this function will use this dataframe instead of self.result. Defaults to None.</p> <code>None</code> <code>bad_channels_dict</code> <code>dict[str, list[str]]</code> <p>Dictionary of list of channels to reject for each recording session. Can be either full channel names or abbreviations. The method will automatically detect which format is being used. If None, the method will use the bad_channels_dict passed to the constructor.</p> <code>None</code> <code>use_abbrevs</code> <code>bool</code> <p>Override automatic detection. If True, channels are assumed to be channel abbreviations. If False, channels are assumed to be channel names. If None, channels are parsed to abbreviations and matched against self.channel_abbrevs.</p> <code>None</code> <code>save_bad_channels</code> <code>Literal['overwrite', 'union', None]</code> <p>How to save bad channels to self.bad_channels_dict. \"overwrite\": Replace self.bad_channels_dict completely with bad_channels_dict. \"union\": Merge bad_channels_dict with existing self.bad_channels_dict per session. None: Don't save to self.bad_channels_dict. Defaults to \"union\". Note: When using \"overwrite\" mode, the bad_channels parameter and bad_channels_dict parameter may conflict and overwrite each other's bad channel definitions if both are provided.</p> <code>'union'</code> <p>Returns:</p> Name Type Description <code>out</code> <p>np.ndarray of bool, (M fragments, N channels). True = keep window, False = remove window</p> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def get_filter_reject_channels_by_recording_session(\n    self,\n    df: pd.DataFrame = None,\n    bad_channels_dict: dict[str, list[str]] = None,\n    use_abbrevs: bool = None,\n    save_bad_channels: Literal[\"overwrite\", \"union\", None] = \"union\",\n    **kwargs,\n):\n    \"\"\"Filter channels to reject for each recording session\n\n    Args:\n        df (pd.DataFrame, optional): If not None, this function will use this dataframe instead of self.result. Defaults to None.\n        bad_channels_dict (dict[str, list[str]]): Dictionary of list of channels to reject for each recording session.\n            Can be either full channel names or abbreviations. The method will automatically detect which format is being used.\n            If None, the method will use the bad_channels_dict passed to the constructor.\n        use_abbrevs (bool, optional): Override automatic detection. If True, channels are assumed to be channel abbreviations. If False, channels are assumed to be channel names.\n            If None, channels are parsed to abbreviations and matched against self.channel_abbrevs.\n        save_bad_channels (Literal[\"overwrite\", \"union\", None], optional): How to save bad channels to self.bad_channels_dict.\n            \"overwrite\": Replace self.bad_channels_dict completely with bad_channels_dict.\n            \"union\": Merge bad_channels_dict with existing self.bad_channels_dict per session.\n            None: Don't save to self.bad_channels_dict. Defaults to \"union\".\n            Note: When using \"overwrite\" mode, the bad_channels parameter and bad_channels_dict parameter\n            may conflict and overwrite each other's bad channel definitions if both are provided.\n\n    Returns:\n        out: np.ndarray of bool, (M fragments, N channels). True = keep window, False = remove window\n    \"\"\"\n    if bad_channels_dict is None:\n        bad_channels_dict = self.bad_channels_dict.copy()\n\n    n_samples = len(self.result)\n    n_channels = len(self.channel_names)\n    mask = np.ones((n_samples, n_channels), dtype=bool)\n\n    # Group by animalday to apply filters per recording session\n    for animalday, group in self.result.groupby(\"animalday\"):\n        if bad_channels_dict:\n            if animalday not in bad_channels_dict:\n                raise ValueError(\n                    f\"No bad channels specified for recording session {animalday}. Check that all days are present in bad_channels_dict\"\n                )\n            bad_channels = bad_channels_dict[animalday]\n        else:\n            bad_channels = []\n\n        channel_targets = self.channel_abbrevs if use_abbrevs or use_abbrevs is None else self.channel_names\n        if use_abbrevs is None:\n            bad_channels = [\n                core.parse_chname_to_abbrev(ch, assume_from_number=self.assume_from_number) for ch in bad_channels\n            ]\n\n        # Get indices for this recording session\n        session_indices = group.index\n\n        # Apply channel filtering for this session\n        for ch in bad_channels:\n            if ch in channel_targets:\n                ch_idx = channel_targets.index(ch)\n                mask[session_indices, ch_idx] = False\n            else:\n                logging.warning(f\"Channel {ch} not found in {channel_targets} for session {animalday}\")\n\n    # Save bad channels to self.bad_channels_dict if requested\n    if save_bad_channels is not None and bad_channels_dict is not None:\n        if save_bad_channels == \"overwrite\":\n            self.bad_channels_dict = bad_channels_dict.copy()\n        elif save_bad_channels == \"union\":\n            # Merge with existing bad channels per session\n            updated_dict = self.bad_channels_dict.copy()\n            for animalday, channels in bad_channels_dict.items():\n                if animalday in updated_dict:\n                    # Union of existing and new channels\n                    updated_dict[animalday] = list(set(updated_dict[animalday]) | set(channels))\n                else:\n                    updated_dict[animalday] = channels.copy()\n            self.bad_channels_dict = updated_dict\n\n    return mask\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.WindowAnalysisResult.get_groupavg_result","title":"<code>get_groupavg_result(features, exclude=[], df=None, groupby='animalday')</code>","text":"<p>Group result and average within groups. Preserves data structure and shape for each feature.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>list[str]</code> <p>List of features to get from result</p> required <code>exclude</code> <code>list[str]</code> <p>List of features to exclude from result. Will override the features parameter. Defaults to [].</p> <code>[]</code> <code>df</code> <code>DataFrame</code> <p>If not None, this function will use this dataframe instead of self.result. Defaults to None.</p> <code>None</code> <code>groupby</code> <code>str</code> <p>Feature or list of features to group by before averaging. Passed to the <code>by</code> parameter in pd.DataFrame.groupby(). Defaults to \"animalday\".</p> <code>'animalday'</code> <p>Returns:</p> Name Type Description <code>grouped_result</code> <p>result grouped by <code>groupby</code> and averaged for each group.</p> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def get_groupavg_result(\n    self, features: list[str], exclude: list[str] = [], df: pd.DataFrame = None, groupby=\"animalday\"\n):\n    \"\"\"Group result and average within groups. Preserves data structure and shape for each feature.\n\n    Args:\n        features (list[str]): List of features to get from result\n        exclude (list[str], optional): List of features to exclude from result. Will override the features parameter. Defaults to [].\n        df (pd.DataFrame, optional): If not None, this function will use this dataframe instead of self.result. Defaults to None.\n        groupby (str, optional): Feature or list of features to group by before averaging. Passed to the `by` parameter in pd.DataFrame.groupby(). Defaults to \"animalday\".\n\n    Returns:\n        grouped_result: result grouped by `groupby` and averaged for each group.\n    \"\"\"\n    result_grouped, result_validcols = self.__get_groups(features=features, exclude=exclude, df=df, groupby=groupby)\n    features = _sanitize_feature_request(features, exclude)\n\n    avg_results = []\n    for f in features:\n        if f in result_validcols:\n            avg_result_col = result_grouped.apply(self._average_feature, f, \"duration\", include_groups=False)\n            avg_result_col.name = f\n            avg_results.append(avg_result_col)\n        else:\n            logging.warning(f\"{f} not calculated, skipping\")\n\n    return pd.concat(avg_results, axis=1)\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.WindowAnalysisResult.get_info","title":"<code>get_info()</code>","text":"<p>Returns a formatted string with basic information about the WindowAnalysisResult object</p> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def get_info(self):\n    \"\"\"Returns a formatted string with basic information about the WindowAnalysisResult object\"\"\"\n    info = []\n    info.append(f\"feature names: {', '.join(self._feature_columns)}\")\n    info.append(f\"animaldays: {', '.join(self.result['animalday'].unique())}\")\n    info.append(\n        f\"animal_id: {self.result['animal'].unique()[0] if 'animal' in self.result.columns else self.animal_id}\"\n    )\n    info.append(\n        f\"genotype: {self.result['genotype'].unique()[0] if 'genotype' in self.result.columns else self.genotype}\"\n    )\n    info.append(f\"channel_names: {', '.join(self.channel_names) if self.channel_names else 'None'}\")\n\n    return \"\\n\".join(info)\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.WindowAnalysisResult.get_lof_scores","title":"<code>get_lof_scores()</code>","text":"<p>Get LOF scores from this WAR.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary mapping animal days to LOF score dictionaries.</p> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def get_lof_scores(self) -&gt; dict:\n    \"\"\"Get LOF scores from this WAR.\n\n    Returns:\n        dict: Dictionary mapping animal days to LOF score dictionaries.\n    \"\"\"\n    if not hasattr(self, \"lof_scores_dict\") or not self.lof_scores_dict:\n        raise ValueError(\"LOF scores not available in this WAR. Compute LOF scores first.\")\n\n    result = {}\n    for animalday, lof_data in self.lof_scores_dict.items():\n        if \"lof_scores\" in lof_data and \"channel_names\" in lof_data:\n            scores = lof_data[\"lof_scores\"]\n            channel_names = lof_data[\"channel_names\"]\n            result[animalday] = dict(zip(channel_names, scores))\n        else:\n            raise ValueError(f\"LOF scores not available for {animalday}\")\n\n    return result\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.WindowAnalysisResult.get_result","title":"<code>get_result(features, exclude=[], allow_missing=False)</code>","text":"<p>Get windowed analysis result dataframe, with helpful filters</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>list[str]</code> <p>List of features to get from result</p> required <code>exclude</code> <code>list[str]</code> <p>List of features to exclude from result; will override the features parameter. Defaults to [].</p> <code>[]</code> <code>allow_missing</code> <code>bool</code> <p>If True, will return all requested features as columns regardless if they exist in result. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>result</code> <p>pd.DataFrame object with features in columns and windows in rows</p> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def get_result(self, features: list[str], exclude: list[str] = [], allow_missing=False):\n    \"\"\"Get windowed analysis result dataframe, with helpful filters\n\n    Args:\n        features (list[str]): List of features to get from result\n        exclude (list[str], optional): List of features to exclude from result; will override the features parameter. Defaults to [].\n        allow_missing (bool, optional): If True, will return all requested features as columns regardless if they exist in result. Defaults to False.\n\n    Returns:\n        result: pd.DataFrame object with features in columns and windows in rows\n    \"\"\"\n    features = _sanitize_feature_request(features, exclude)\n    if not allow_missing:\n        return self.result.loc[:, self._nonfeature_columns + features]\n    else:\n        return self.result.reindex(columns=self._nonfeature_columns + features)\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.WindowAnalysisResult.load_pickle_and_json","title":"<code>load_pickle_and_json(folder_path=None, pickle_name=None, json_name=None)</code>  <code>classmethod</code>","text":"<p>Load WindowAnalysisResult from folder</p> <p>Parameters:</p> Name Type Description Default <code>folder_path</code> <code>str</code> <p>Path of folder containing .pkl and .json files. Defaults to None.</p> <code>None</code> <code>pickle_name</code> <code>str</code> <p>Name of the pickle file. Can be just the filename (e.g. \"war.pkl\") or a path relative to folder_path (e.g. \"subdir/war.pkl\"). If None and folder_path is provided, expects exactly one .pkl file in folder_path. Defaults to None.</p> <code>None</code> <code>json_name</code> <code>str</code> <p>Name of the JSON file. Can be just the filename (e.g. \"war.json\") or a path relative to folder_path (e.g. \"subdir/war.json\"). If None and folder_path is provided, expects exactly one .json file in folder_path. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>folder_path does not exist</p> <code>ValueError</code> <p>Expected exactly one pickle and one json file in folder_path (when pickle_name/json_name not specified)</p> <code>FileNotFoundError</code> <p>Specified pickle_name or json_name not found</p> <p>Returns:</p> Name Type Description <code>result</code> <p>WindowAnalysisResult object</p> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>@classmethod\ndef load_pickle_and_json(cls, folder_path=None, pickle_name=None, json_name=None):\n    \"\"\"Load WindowAnalysisResult from folder\n\n    Args:\n        folder_path (str, optional): Path of folder containing .pkl and .json files. Defaults to None.\n        pickle_name (str, optional): Name of the pickle file. Can be just the filename (e.g. \"war.pkl\")\n            or a path relative to folder_path (e.g. \"subdir/war.pkl\"). If None and folder_path is provided,\n            expects exactly one .pkl file in folder_path. Defaults to None.\n        json_name (str, optional): Name of the JSON file. Can be just the filename (e.g. \"war.json\")\n            or a path relative to folder_path (e.g. \"subdir/war.json\"). If None and folder_path is provided,\n            expects exactly one .json file in folder_path. Defaults to None.\n\n    Raises:\n        ValueError: folder_path does not exist\n        ValueError: Expected exactly one pickle and one json file in folder_path (when pickle_name/json_name not specified)\n        FileNotFoundError: Specified pickle_name or json_name not found\n\n    Returns:\n        result: WindowAnalysisResult object\n    \"\"\"\n    if folder_path is not None:\n        folder_path = Path(folder_path)\n        if not folder_path.exists():\n            raise ValueError(f\"Folder path {folder_path} does not exist\")\n\n        if pickle_name is not None:\n            # Handle pickle_name as either absolute path or relative to folder_path\n            pickle_path = Path(pickle_name)\n            if pickle_path.is_absolute():\n                df_pickle_path = pickle_path\n            else:\n                df_pickle_path = folder_path / pickle_name\n\n            if not df_pickle_path.exists():\n                raise FileNotFoundError(f\"Pickle file not found: {df_pickle_path}\")\n        else:\n            pkl_files = list(folder_path.glob(\"*.pkl\"))\n            if len(pkl_files) != 1:\n                raise ValueError(f\"Expected exactly one pickle file in {folder_path}, found {len(pkl_files)}\")\n            df_pickle_path = pkl_files[0]\n\n        if json_name is not None:\n            # Handle json_name as either absolute path or relative to folder_path\n            json_path = Path(json_name)\n            if json_path.is_absolute():\n                json_path = json_path\n            else:\n                json_path = folder_path / json_name\n\n            if not json_path.exists():\n                raise FileNotFoundError(f\"JSON file not found: {json_path}\")\n        else:\n            json_files = list(folder_path.glob(\"*.json\"))\n            if len(json_files) != 1:\n                raise ValueError(f\"Expected exactly one json file in {folder_path}, found {len(json_files)}\")\n            json_path = json_files[0]\n    else:\n        if pickle_name is None or json_name is None:\n            raise ValueError(\n                \"Either folder_path must be provided, or both pickle_name and json_name must be provided as absolute paths\"\n            )\n\n        df_pickle_path = Path(pickle_name)\n        json_path = Path(json_name)\n\n        if not df_pickle_path.exists():\n            raise FileNotFoundError(f\"Pickle file not found: {df_pickle_path}\")\n        if not json_path.exists():\n            raise FileNotFoundError(f\"JSON file not found: {json_path}\")\n\n    with open(df_pickle_path, \"rb\") as f:\n        data = pd.read_pickle(f)\n    with open(json_path, \"r\") as f:\n        metadata = json.load(f)\n    return cls(data, **metadata)\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.WindowAnalysisResult.read_mnes_spikes","title":"<code>read_mnes_spikes(raws, inplace=True)</code>","text":"<p>Extract spike features from MNE RawArray objects with spike annotations.</p> <p>This method extracts spike timing from MNE annotations (where spikes are marked with channel-specific event labels) and bins them into WAR time windows.</p> <p>Parameters:</p> Name Type Description Default <code>raws</code> <code>list[RawArray]</code> <p>List of MNE RawArray objects with spike annotations. One per recording   session (animalday). Each should have annotations with channel names   as event labels (e.g., 'LMot', 'RMot', etc.).</p> required <code>inplace</code> <p>If True, modifies self.result and returns self.     If False, returns a new WindowAnalysisResult.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>WindowAnalysisResult</code> <p>WAR object with added spike features (nspike, lognspike).</p> Notes <ul> <li>Expects MNE annotations with channel names as event descriptions</li> <li>Spike times are extracted from event onsets and binned to WAR windows</li> <li>Channels not found in annotations will have empty spike arrays</li> <li>Delegates to _read_from_spikes_all() for the actual binning logic</li> </ul> Example Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def read_mnes_spikes(self, raws: list[mne.io.RawArray], inplace=True):\n    \"\"\"\n    Extract spike features from MNE RawArray objects with spike annotations.\n\n    This method extracts spike timing from MNE annotations (where spikes are marked\n    with channel-specific event labels) and bins them into WAR time windows.\n\n    Args:\n        raws: List of MNE RawArray objects with spike annotations. One per recording\n              session (animalday). Each should have annotations with channel names\n              as event labels (e.g., 'LMot', 'RMot', etc.).\n        inplace: If True, modifies self.result and returns self.\n                If False, returns a new WindowAnalysisResult.\n\n    Returns:\n        WindowAnalysisResult: WAR object with added spike features (nspike, lognspike).\n\n    Notes:\n        - Expects MNE annotations with channel names as event descriptions\n        - Spike times are extracted from event onsets and binned to WAR windows\n        - Channels not found in annotations will have empty spike arrays\n        - Delegates to _read_from_spikes_all() for the actual binning logic\n\n    Example:\n        &gt;&gt;&gt; # From MNE spike annotations\n        &gt;&gt;&gt; enhanced_war = war.read_mnes_spikes([mne_raw1, mne_raw2], inplace=False)\n    \"\"\"\n    spikes_all = []\n    for raw in raws:\n        # each mne is a contiguous recording session\n        events, event_id = mne.events_from_annotations(raw)\n        event_id = {k.item(): v for k, v in event_id.items()}\n\n        spikes_channel = []\n        for channel in raw.ch_names:\n            if channel not in event_id.keys():\n                logging.warning(f\"Channel {channel} not found in event_id\")\n                spikes_channel.append([])\n                continue\n            event_id_channel = event_id[channel]\n            spike_times = events[events[:, 2] == event_id_channel, 0]\n            spike_times = spike_times / raw.info[\"sfreq\"]\n            spikes_channel.append(spike_times)\n        spikes_all.append(spikes_channel)\n    return self._read_from_spikes_all(spikes_all, inplace=inplace)\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.WindowAnalysisResult.read_mnes_spikes--from-mne-spike-annotations","title":"From MNE spike annotations","text":"<p>enhanced_war = war.read_mnes_spikes([mne_raw1, mne_raw2], inplace=False)</p>"},{"location":"reference/visualization/results/#neurodent.visualization.results.WindowAnalysisResult.read_sars_spikes","title":"<code>read_sars_spikes(sars, read_mode='sa', inplace=True)</code>","text":"<p>Integrate spike analysis results into WAR by adding nspike/lognspike features.</p> <p>This method extracts spike timing information from spike detection results and bins them according to the WAR's time windows, adding spike count features to each row.</p> <p>Parameters:</p> Name Type Description Default <code>sars</code> <code>list[Union[SpikeAnalysisResult, FrequencyDomainSpikeAnalysisResult]]</code> <p>List of SpikeAnalysisResult or FrequencyDomainSpikeAnalysisResult objects.   One result per recording session (animalday).</p> required <code>read_mode</code> <code>Literal['sa', 'mne']</code> <p>Mode for extracting spike data: - \"sa\": Read from SortingAnalyzer objects (result_sas attribute) - \"mne\": Read from MNE RawArray objects (result_mne attribute)</p> <code>'sa'</code> <code>inplace</code> <p>If True, modifies self.result and returns self.     If False, returns a new WindowAnalysisResult.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>WindowAnalysisResult</code> <p>WAR object with added spike features (nspike, lognspike). - If inplace=True: returns self with modified result DataFrame - If inplace=False: returns new WAR object with enhanced result DataFrame</p> Notes <ul> <li>The number of sars must match the number of unique animaldays in self.result</li> <li>Spikes are binned into time windows matching the existing WAR fragments</li> <li>nspike: array of spike counts per channel for each time window</li> <li>lognspike: log-transformed spike counts using core.log_transform()</li> </ul> Example Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def read_sars_spikes(\n    self,\n    sars: list[Union[\"SpikeAnalysisResult\", \"FrequencyDomainSpikeAnalysisResult\"]],\n    read_mode: Literal[\"sa\", \"mne\"] = \"sa\",\n    inplace=True\n):\n    \"\"\"\n    Integrate spike analysis results into WAR by adding nspike/lognspike features.\n\n    This method extracts spike timing information from spike detection results and bins\n    them according to the WAR's time windows, adding spike count features to each row.\n\n    Args:\n        sars: List of SpikeAnalysisResult or FrequencyDomainSpikeAnalysisResult objects.\n              One result per recording session (animalday).\n        read_mode: Mode for extracting spike data:\n            - \"sa\": Read from SortingAnalyzer objects (result_sas attribute)\n            - \"mne\": Read from MNE RawArray objects (result_mne attribute)\n        inplace: If True, modifies self.result and returns self.\n                If False, returns a new WindowAnalysisResult.\n\n    Returns:\n        WindowAnalysisResult: WAR object with added spike features (nspike, lognspike).\n            - If inplace=True: returns self with modified result DataFrame\n            - If inplace=False: returns new WAR object with enhanced result DataFrame\n\n    Notes:\n        - The number of sars must match the number of unique animaldays in self.result\n        - Spikes are binned into time windows matching the existing WAR fragments\n        - nspike: array of spike counts per channel for each time window\n        - lognspike: log-transformed spike counts using core.log_transform()\n\n    Example:\n        &gt;&gt;&gt; # After computing WAR and spike detection\n        &gt;&gt;&gt; enhanced_war = war.read_sars_spikes(fdsar_list, read_mode=\"sa\", inplace=False)\n        &gt;&gt;&gt; enhanced_war.result['nspike']  # Spike counts per channel per window\n    \"\"\"\n    match read_mode:\n        case \"sa\":\n            spikes_all = []\n            for sar in sars:  # for each continuous recording session\n                spikes_channel = []\n                for i, sa in enumerate(sar.result_sas):  # for each channel\n                    spike_times = []\n                    for unit in sa.sorting.get_unit_ids():  # Flatten units\n                        spike_times.extend(sa.sorting.get_unit_spike_train(unit_id=unit).tolist())\n                    spike_times = np.array(spike_times) / sa.sorting.get_sampling_frequency()\n                    spikes_channel.append(spike_times)\n                spikes_all.append(spikes_channel)\n            return self._read_from_spikes_all(spikes_all, inplace=inplace)\n        case \"mne\":\n            raws = [sar.result_mne for sar in sars]\n            return self.read_mnes_spikes(raws, inplace=inplace)\n        case _:\n            raise ValueError(f\"Invalid read_mode: {read_mode}\")\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.WindowAnalysisResult.read_sars_spikes--after-computing-war-and-spike-detection","title":"After computing WAR and spike detection","text":"<p>enhanced_war = war.read_sars_spikes(fdsar_list, read_mode=\"sa\", inplace=False) enhanced_war.result['nspike']  # Spike counts per channel per window</p>"},{"location":"reference/visualization/results/#neurodent.visualization.results.WindowAnalysisResult.reorder_and_pad_channels","title":"<code>reorder_and_pad_channels(target_channels, use_abbrevs=True, inplace=True)</code>","text":"<p>Reorder and pad channels to match a target channel list.</p> <p>This method ensures that the data has a consistent channel order and structure by reordering existing channels and padding missing channels with NaNs.</p> <p>Parameters:</p> Name Type Description Default <code>target_channels</code> <code>list[str]</code> <p>List of target channel names to match</p> required <code>use_abbrevs</code> <code>bool</code> <p>If True, target channel names are read as channel abbreviations instead of channel names. Defaults to True.</p> <code>True</code> <code>inplace</code> <code>bool</code> <p>If True, modify the result in place. Defaults to True.</p> <code>True</code> <p>Returns:     pd.DataFrame: DataFrame with reordered and padded channels</p> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def reorder_and_pad_channels(\n    self, target_channels: list[str], use_abbrevs: bool = True, inplace: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"Reorder and pad channels to match a target channel list.\n\n    This method ensures that the data has a consistent channel order and structure\n    by reordering existing channels and padding missing channels with NaNs.\n\n    Args:\n        target_channels (list[str]): List of target channel names to match\n        use_abbrevs (bool, optional): If True, target channel names are read as channel abbreviations instead of channel names. Defaults to True.\n        inplace (bool, optional): If True, modify the result in place. Defaults to True.\n    Returns:\n        pd.DataFrame: DataFrame with reordered and padded channels\n    \"\"\"\n    duplicates = [ch for ch in target_channels if target_channels.count(ch) &gt; 1]\n    if duplicates:\n        raise ValueError(f\"Target channels must be unique. Found duplicates: {duplicates}\")\n\n    result = self.result.copy()\n\n    channel_map = {ch: i for i, ch in enumerate(target_channels)}\n    channel_names = self.channel_names if not use_abbrevs else self.channel_abbrevs\n\n    valid_channels = [ch for ch in channel_names if ch in channel_map]\n    if not valid_channels:\n        warnings.warn(\n            f\"None of the channel names {channel_names} were found in target channels {target_channels}. Is use_abbrevs correctly set?\"\n        )\n\n    for feature in self._feature_columns:\n        match feature:\n            case _ if feature in constants.LINEAR_FEATURES + constants.BAND_FEATURES:\n                if feature in constants.BAND_FEATURES:\n                    df_bands = pd.DataFrame(result[feature].tolist())\n                    vals = np.array(df_bands.values.tolist())\n                    vals = vals.transpose((0, 2, 1))\n                    keys = df_bands.keys()\n                else:\n                    vals = np.array(result[feature].tolist())\n\n                new_vals = np.full((vals.shape[0], len(target_channels), *vals.shape[2:]), np.nan)  # dubious\n\n                for i, ch in enumerate(channel_names):\n                    if ch in channel_map:\n                        new_vals[:, channel_map[ch]] = vals[:, i]\n\n                if feature in constants.BAND_FEATURES:\n                    new_vals = new_vals.transpose((0, 2, 1))\n                    result[feature] = [dict(zip(keys, vals)) for vals in new_vals]\n                else:\n                    result[feature] = [list(x) for x in new_vals]\n\n            case _ if feature in constants.MATRIX_FEATURES:\n                if feature in [\"cohere\", \"zcohere\", \"imcoh\", \"zimcoh\"]:\n                    df_bands = pd.DataFrame(result[feature].tolist())\n                    vals = np.array(df_bands.values.tolist())\n                    keys = df_bands.keys()\n                else:\n                    vals = np.array(result[feature].tolist())\n\n                logging.debug(f\"vals.shape: {vals.shape}\")\n                new_shape = list(vals.shape[:-2]) + [len(target_channels), len(target_channels)]\n                new_vals = np.full(new_shape, np.nan)\n\n                # Map original channels to target channels\n                for i, ch1 in enumerate(channel_names):\n                    if ch1 in channel_map:\n                        for j, ch2 in enumerate(channel_names):\n                            if ch2 in channel_map:\n                                new_vals[..., channel_map[ch1], channel_map[ch2]] = vals[..., i, j]\n\n                if feature in [\"cohere\", \"zcohere\", \"imcoh\", \"zimcoh\"]:\n                    result[feature] = [dict(zip(keys, vals)) for vals in new_vals]\n                else:\n                    result[feature] = [list(x) for x in new_vals]\n\n            case _ if feature in constants.HIST_FEATURES:\n                coords = np.array([x[0] for x in result[feature].tolist()])\n                vals = np.array([x[1] for x in result[feature].tolist()])\n                new_vals = np.full((*vals.shape[0:-1], len(target_channels)), np.nan)\n\n                for i, ch in enumerate(channel_names):\n                    if ch in channel_map:\n                        new_vals[:, ..., channel_map[ch]] = vals[:, ..., i]\n\n                result[feature] = [(coords[i], new_vals[i]) for i in range(len(coords))]\n\n            case _:\n                raise ValueError(f\"Invalid feature: {feature}\")\n\n    if inplace:\n        self.result = result\n\n        logging.debug(f\"Old channel names: {self.channel_names}\")\n        self.channel_names = target_channels\n        logging.debug(f\"New channel names: {self.channel_names}\")\n\n        logging.debug(f\"Old channel abbreviations: {self.channel_abbrevs}\")\n        self.__update_instance_vars()\n        logging.debug(f\"New channel abbreviations: {self.channel_abbrevs}\")\n\n    return result\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.WindowAnalysisResult.save_pickle_and_json","title":"<code>save_pickle_and_json(folder, make_folder=True, filename=None, slugify_filename=False, save_abbrevs_as_chnames=False)</code>","text":"<p>Archive window analysis result into the folder specified, as a pickle and json file.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str | Path</code> <p>Destination folder to save results to</p> required <code>make_folder</code> <code>bool</code> <p>If True, create the folder if it doesn't exist. Defaults to True.</p> <code>True</code> <code>filename</code> <code>str</code> <p>Name of the file to save. Defaults to \"war\".</p> <code>None</code> <code>slugify_filename</code> <code>bool</code> <p>If True, slugify the filename (replace special characters). Defaults to False.</p> <code>False</code> <code>save_abbrevs_as_chnames</code> <code>bool</code> <p>If True, save the channel abbreviations as the channel names in the json file. Defaults to False.</p> <code>False</code> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def save_pickle_and_json(\n    self,\n    folder: str | Path,\n    make_folder=True,\n    filename: str = None,\n    slugify_filename=False,\n    save_abbrevs_as_chnames=False,\n):\n    \"\"\"Archive window analysis result into the folder specified, as a pickle and json file.\n\n    Args:\n        folder (str | Path): Destination folder to save results to\n        make_folder (bool, optional): If True, create the folder if it doesn't exist. Defaults to True.\n        filename (str, optional): Name of the file to save. Defaults to \"war\".\n        slugify_filename (bool, optional): If True, slugify the filename (replace special characters). Defaults to False.\n        save_abbrevs_as_chnames (bool, optional): If True, save the channel abbreviations as the channel names in the json file. Defaults to False.\n    \"\"\"\n    folder = Path(folder)\n    if make_folder:\n        folder.mkdir(parents=True, exist_ok=True)\n\n    filename = \"war\" if filename is None else filename\n    filename = slugify(filename) if slugify_filename else filename\n\n    filepath = str(folder / filename)\n\n    self.result.to_pickle(filepath + \".pkl\")\n    logging.info(f\"Saved WAR to {filepath + '.pkl'}\")\n\n    json_dict = {\n        \"animal_id\": self.animal_id,\n        \"genotype\": self.genotype,\n        \"channel_names\": self.channel_abbrevs if save_abbrevs_as_chnames else self.channel_names,\n        \"assume_from_number\": False if save_abbrevs_as_chnames else self.assume_from_number,\n        \"bad_channels_dict\": self.bad_channels_dict,\n        \"suppress_short_interval_error\": self.suppress_short_interval_error,\n        \"lof_scores_dict\": self.lof_scores_dict.copy(),\n    }\n\n    with open(filepath + \".json\", \"w\") as f:\n        json.dump(json_dict, f, indent=2)\n        logging.info(f\"Saved WAR to {filepath + '.json'}\")\n</code></pre>"},{"location":"reference/visualization/results/#neurodent.visualization.results.bin_spike_times","title":"<code>bin_spike_times(spike_times, fragment_durations)</code>","text":"<p>Bin spike times into counts based on fragment durations.</p> <p>Parameters:</p> Name Type Description Default <code>spike_times</code> <code>list[float]</code> <p>List of spike timestamps in seconds</p> required <code>fragment_durations</code> <code>list[float]</code> <p>List of fragment durations in seconds</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>list[int]: List of spike counts per fragment</p> Source code in <code>src/neurodent/visualization/results.py</code> <pre><code>def bin_spike_times(spike_times: list[float], fragment_durations: list[float]) -&gt; list[int]:\n    \"\"\"Bin spike times into counts based on fragment durations.\n\n    Args:\n        spike_times (list[float]): List of spike timestamps in seconds\n        fragment_durations (list[float]): List of fragment durations in seconds\n\n    Returns:\n        list[int]: List of spike counts per fragment\n    \"\"\"\n    # Convert fragment durations to bin edges\n    bin_edges = np.cumsum([0] + fragment_durations)\n\n    # Use numpy's histogram function to count spikes in each bin\n    counts, _ = np.histogram(spike_times, bins=bin_edges)\n\n    return counts.tolist()\n</code></pre>"}]}