{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PythonEEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [
     "note"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook pythoneeg.ipynb to script\n",
      "[NbConvertApp] Writing 38544 bytes to pythoneeg/core.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script pythoneeg.ipynb --TagRemovePreprocessor.remove_cell_tags='{\"note\"}' --output-dir ./pythoneeg --output core\n",
    "\n",
    "# # import PyInstaller.__main__\n",
    "# # PyInstaller.__main__.run([\n",
    "# #     r'.\\pys\\pythoneeg.py',\n",
    "# #     r'--additional-hooks-dir',\n",
    "# #     r'.\\extra-hooks',\n",
    "# #     r'--onefile',\n",
    "# #     r'--windowed',\n",
    "# # ])\n",
    "# PyInstaller.__main__.run([\n",
    "#     r'.\\pys\\pythoneeg.py',\n",
    "#     r'options.spec',\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import gzip\n",
    "import tempfile\n",
    "import glob\n",
    "import math\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import statistics\n",
    "import time\n",
    "from typing import Literal\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from scipy import signal, integrate\n",
    "from scipy.integrate import simpson\n",
    "from scipy.signal import welch, decimate\n",
    "from scipy.interpolate import PchipInterpolator, Akima1DInterpolator, CubicSpline\n",
    "from scipy.stats import linregress, pearsonr\n",
    "import pandas as pd\n",
    "from mne.time_frequency import psd_array_multitaper, csd_array_fourier, fwhm\n",
    "from mne import set_config\n",
    "from mne_connectivity import envelope_correlation, spectral_connectivity_time\n",
    "from pactools import raw_to_mask, simulate_pac, Comodulogram, MaskIterator\n",
    "\n",
    "from mountainsort5 import Scheme2SortingParameters, sorting_scheme2\n",
    "from mountainsort5.util import create_cached_recording\n",
    "import spikeinterface.core as si\n",
    "import spikeinterface.extractors as se\n",
    "import spikeinterface.preprocessing as spre\n",
    "# import spikeinterface.sorters as ss\n",
    "# import spikeinterface.postprocessing as spost\n",
    "# import spikeinterface.qualitymetrics as sqm\n",
    "# import spikeinterface.exporters as sexp\n",
    "# import spikeinterface.comparison as scmp\n",
    "# import spikeinterface.curation as scur\n",
    "# import spikeinterface.sortingcomponents as sc\n",
    "import spikeinterface.widgets as sw\n",
    "import probeinterface as pi\n",
    "# from probeinterface.plotting import plot_probe_group, plot_probe\n",
    "\n",
    "# import PyQt5.QtWidgets as qw\n",
    "# import PyQt5.QtCore as qc\n",
    "import PyInstaller.__main__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": [
     "note"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.8\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Metadata Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_units_to_multiplier(current_units, target_units='µV'):\n",
    "    units_to_mult = {'µV' : 1e-6,\n",
    "                     'mV' : 1e-3,\n",
    "                     'V' : 1,\n",
    "                     'nV' : 1e-9}\n",
    "    \n",
    "    assert current_units in units_to_mult.keys(), f\"No valid current unit called '{current_units}' found\"\n",
    "    assert target_units in units_to_mult.keys(), f\"No valid target unit called '{target_units}' found\"\n",
    "\n",
    "    return units_to_mult[current_units] / units_to_mult[target_units]\n",
    "\n",
    "def is_day(dt: datetime, sunrise=6, sunset=18):\n",
    "    return sunrise <= dt.hour < sunset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDFBinaryMetadata:\n",
    "\n",
    "    def __init__(self, metadata_path, verbose=False) -> None:\n",
    "        self.metadata_path = metadata_path\n",
    "        self.metadata_df = pd.read_csv(metadata_path)\n",
    "        self.verbose = verbose\n",
    "        if verbose > 0:\n",
    "            print(self.metadata_df)\n",
    "\n",
    "        self.n_channels = len(self.metadata_df.index)\n",
    "        self.f_s = self.__getsinglecolval(\"SampleRate\")\n",
    "        self.V_units = self.__getsinglecolval(\"Units\")\n",
    "        self.mult_to_uV = convert_units_to_multiplier(self.V_units)\n",
    "        self.precision = self.__getsinglecolval(\"Precision\")\n",
    "        self.dt_end: datetime\n",
    "        self.dt_start: datetime\n",
    "        if \"LastEdit\" in self.metadata_df.keys():\n",
    "            self.dt_end = datetime.fromisoformat(self.__getsinglecolval(\"LastEdit\"))\n",
    "        else:\n",
    "            self.dt_end = None\n",
    "            warnings.warn(\"No LastEdit column provided in metadata. dt_end set to None\")\n",
    "\n",
    "        self.channel_to_info = self.metadata_df.loc[:, [\"BinColumn\", \"ProbeInfo\"]].set_index('BinColumn').T.to_dict('list')\n",
    "        self.channel_to_info = {k:v[0] for k,v in self.channel_to_info.items()}\n",
    "        self.id_to_info = {k-1:v for k,v in self.channel_to_info.items()}\n",
    "        self.entity_to_info = self.metadata_df.loc[:, [\"Entity\", \"ProbeInfo\"]].set_index('Entity').T.to_dict('list')\n",
    "        self.entity_to_info = {k:v[0] for k,v in self.entity_to_info.items()}\n",
    "        self.channel_names = list(self.channel_to_info.values())\n",
    "\n",
    "        # TODO read probe geometry information, may be user-defined\n",
    "\n",
    "    def __getsinglecolval(self, colname):\n",
    "        vals = self.metadata_df.loc[:, colname]\n",
    "        if len(np.unique(vals)) > 1:\n",
    "            warnings.warn(f\"Not all {colname}s are equal!\")\n",
    "        if vals.size == 0:\n",
    "            return None\n",
    "        return vals.iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": [
     "note"
    ]
   },
   "outputs": [],
   "source": [
    "%%script true\n",
    "tmpmet = DDFBinaryMetadata(r\"Z:\\PythonEEG Data Bins\\A5 WT 12_12-2023\\Cage 1 A5 -1_Meta.csv\")\n",
    "# tmpmet.dt_end.ctime()\n",
    "tmpmet.channel_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Analysis Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ddfcolbin_to_ddfrowbin(rowdir_path, colbin_path, metadata, save_gzip=True):\n",
    "    assert isinstance(metadata, DDFBinaryMetadata), \"Metadata needs to be of type DDFBinaryMetadata\"\n",
    "    \n",
    "    tempbin = np.fromfile(colbin_path, dtype=metadata.precision)\n",
    "    tempbin = np.reshape(tempbin, (-1, metadata.n_channels), order='F')\n",
    "\n",
    "    # rowbin_path = Path(colbin_path).parent / f'{Path(colbin_path).stem.replace(\"ColMajor\", \"RowMajor\")}'\n",
    "    rowbin_path = convert_colpath_to_rowpath(rowdir_path, colbin_path, gzip=save_gzip)\n",
    "\n",
    "    if save_gzip:\n",
    "        # rowbin_path = str(rowbin_path) + \".npy.gz\"\n",
    "        with gzip.GzipFile(rowbin_path, \"w\") as fcomp:\n",
    "            np.save(file=fcomp, arr=tempbin)\n",
    "    else:\n",
    "        # rowbin_path = str(rowbin_path) + \".bin\"\n",
    "        tempbin.tofile(rowbin_path)\n",
    "    \n",
    "    return rowbin_path\n",
    "\n",
    "def convert_colpath_to_rowpath(rowdir_path, col_path, gzip=True, aspath=True):\n",
    "    out = Path(rowdir_path) / f'{Path(col_path).stem.replace(\"ColMajor\", \"RowMajor\")}'\n",
    "    if gzip:\n",
    "        out = str(out) + \".npy.gz\"\n",
    "    else:\n",
    "        out = str(out) + \".bin\"\n",
    "    return Path(out) if aspath else out\n",
    "\n",
    "def convert_ddfrowbin_to_si(bin_rowmajor_path, metadata):\n",
    "    # 1-file .MAT containing entire recording trace\n",
    "    # Returns as SpikeInterface Recording structure\n",
    "    assert isinstance(metadata, DDFBinaryMetadata), \"Metadata needs to be of type DDFBinaryMetadata\"\n",
    "\n",
    "    bin_rowmajor_path = Path(bin_rowmajor_path)\n",
    "    params = {\"sampling_frequency\" : metadata.f_s,\n",
    "              \"dtype\" : metadata.precision,\n",
    "              \"num_channels\" : metadata.n_channels,\n",
    "              \"gain_to_uV\" : metadata.mult_to_uV,\n",
    "              \"time_axis\" : 0,\n",
    "              \"is_filtered\" : False}\n",
    "\n",
    "    # Read either .npy.gz files or .bin files into the recording object\n",
    "    if \".npy.gz\" in str(bin_rowmajor_path):\n",
    "        temppath = os.path.join(tempfile.gettempdir(), os.urandom(24).hex())\n",
    "        print(f\"Opening tempfile {temppath}\")\n",
    "        with open(temppath, \"wb\") as tmp:\n",
    "            fcomp = gzip.GzipFile(bin_rowmajor_path, \"r\")\n",
    "            bin_rowmajor_decomp = np.load(fcomp)\n",
    "            bin_rowmajor_decomp.tofile(tmp)\n",
    "\n",
    "            rec = se.read_binary(tmp.name, **params)\n",
    "    else:\n",
    "        rec = se.read_binary(bin_rowmajor_path, **params)\n",
    "        temppath = None\n",
    "\n",
    "    return rec, temppath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filepath_to_index(filepath) -> int:\n",
    "    fpath = str(filepath)\n",
    "    for suffix in ['_RowMajor', '_ColMajor', '_Meta']:\n",
    "        fpath = fpath.replace(suffix, '')\n",
    "    fpath = fpath.removesuffix(''.join(Path(fpath).suffixes))\n",
    "    fname = Path(fpath).name\n",
    "    fname = re.split(r'\\D+', fname)\n",
    "    fname = list(filter(None, fname))\n",
    "    return int(fname[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenPrints:\n",
    "    def __init__(self, silence=True) -> None:\n",
    "        self.silence = silence\n",
    "\n",
    "    def __enter__(self):\n",
    "        if self.silence:\n",
    "            self._original_stdout = sys.stdout\n",
    "            sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        if self.silence:\n",
    "            sys.stdout.close()\n",
    "            sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LongRecordingOrganizer:\n",
    "    def __init__(self, base_folder_path, \n",
    "                 colbin_folder_path=None,\n",
    "                 rowbin_folder_path=None,\n",
    "                 metadata_path=None,\n",
    "                 truncate=False) -> None:\n",
    "        \n",
    "        if type(truncate) is int:\n",
    "            self.truncate = True\n",
    "            self.n_truncate = truncate\n",
    "        elif type(truncate) is bool:\n",
    "            self.truncate = truncate\n",
    "            self.n_truncate = 10\n",
    "        else:\n",
    "            self.truncate = False\n",
    "            warnings.warn(\"Invalid truncate parameter, setting truncate = False\")\n",
    "        if self.truncate:\n",
    "            warnings.warn(f\"truncate = True. Only the first {self.n_truncate} files of each animal will be used\")\n",
    "\n",
    "        self.base_folder_path = Path(base_folder_path)\n",
    "\n",
    "        self.colbin_folder_path = self.base_folder_path if colbin_folder_path is None else Path(colbin_folder_path)\n",
    "        os.makedirs(self.colbin_folder_path, exist_ok=True)\n",
    "        self.rowbin_folder_path = self.colbin_folder_path if rowbin_folder_path is None else Path(rowbin_folder_path)\n",
    "        os.makedirs(self.rowbin_folder_path, exist_ok=True)\n",
    "\n",
    "        self.__update_colbins_rowbins_metas()\n",
    "\n",
    "        if metadata_path is not None:\n",
    "            self.meta = DDFBinaryMetadata(metadata_path)\n",
    "            self.metadata_objects = [self.meta]\n",
    "        else:\n",
    "            self.meta = DDFBinaryMetadata(self.metas[0])\n",
    "            self.metadata_objects = [DDFBinaryMetadata(x) for x in self.metas]\n",
    "            self._validate_metadata_consistency(self.metadata_objects)\n",
    "        self.channel_names = self.meta.channel_names\n",
    "\n",
    "        dt_ends = [x.dt_end for x in self.metadata_objects]\n",
    "        if all(x is None for x in dt_ends):\n",
    "            raise ValueError(\"No dates found in any metadata object!\")\n",
    "        \n",
    "        self._median_datetime = statistics.median_low(pd.Series(dt_ends).dropna())\n",
    "        self._idx_median_datetime = dt_ends.index(self._median_datetime)\n",
    "\n",
    "    def __truncate_lists(self, colbins, rowbins, metas):\n",
    "        if len(colbins) > self.n_truncate:\n",
    "            out_colbins = colbins[:self.n_truncate]\n",
    "        else:\n",
    "            out_colbins = colbins\n",
    "\n",
    "        out_rowbins = []\n",
    "        out_metas = []\n",
    "        for i, e in enumerate(rowbins):\n",
    "            tempcolname = Path(e).name.replace(\"RowMajor.npy.gz\", \"ColMajor.bin\")\n",
    "            if str(self.colbin_folder_path / tempcolname) in out_colbins:\n",
    "                out_rowbins.append(e)\n",
    "        for i, e in enumerate(metas):\n",
    "            tempcolname = Path(e).name.replace(\"Meta.csv\", \"ColMajor.bin\")\n",
    "            if str(self.colbin_folder_path / tempcolname) in out_colbins:\n",
    "                out_metas.append(e)\n",
    "\n",
    "        return out_colbins, out_rowbins, out_metas\n",
    "\n",
    "    def __update_colbins_rowbins_metas(self):\n",
    "        self.colbins = glob.glob(str(self.colbin_folder_path / \"*_ColMajor.bin\"))\n",
    "        self.rowbins = glob.glob(str(self.rowbin_folder_path / \"*_RowMajor.npy.gz\"))\n",
    "        self.metas = glob.glob(str(self.colbin_folder_path / \"*_Meta.csv\"))\n",
    "\n",
    "        self.colbins.sort(key=filepath_to_index)\n",
    "        self.rowbins.sort(key=filepath_to_index)\n",
    "        self.metas.sort(key=filepath_to_index)\n",
    "\n",
    "        metadatas = [DDFBinaryMetadata(x) for x in self.metas]\n",
    "        for meta in metadatas:\n",
    "            if meta.metadata_df.empty:\n",
    "                searchstr = Path(meta.metadata_path).name.replace(\"_Meta\", \"\")\n",
    "                self.colbins = [x for x in self.colbins if searchstr not in x]\n",
    "                self.rowbins = [x for x in self.rowbins if searchstr not in x]\n",
    "                self.metas = [x for x in self.metas if searchstr not in x]\n",
    "\n",
    "        if self.truncate:\n",
    "            self.colbins, self.rowbins, self.metas = self.__truncate_lists(self.colbins, self.rowbins, self.metas)\n",
    "\n",
    "    def _validate_metadata_consistency(self, metadatas:list[DDFBinaryMetadata]):\n",
    "        meta0 = metadatas[0]\n",
    "        attributes = ['f_s', 'n_channels', 'precision', 'V_units', 'channel_names']\n",
    "        for attr in attributes:\n",
    "            if not all([getattr(meta0, attr) == getattr(x, attr) for x in metadatas]):\n",
    "                raise ValueError(f\"Metadata files inconsistent at attribute {attr}\")\n",
    "        return\n",
    "\n",
    "\n",
    "    def convert_colbins_to_rowbins(self, overwrite=True):\n",
    "        if not overwrite and self.rowbins:\n",
    "            warnings.warn(\"Row-major binary files already exist! Skipping existing files\")\n",
    "            # else:\n",
    "            #     raise FileExistsError(\"Row-major binary files already exist! overwrite=False\")\n",
    "        for i, e in enumerate(self.colbins):\n",
    "            if convert_colpath_to_rowpath(self.rowbin_folder_path, e, aspath=False) not in self.rowbins or overwrite:\n",
    "                print(f\"Converting {e}\")\n",
    "                convert_ddfcolbin_to_ddfrowbin(self.rowbin_folder_path, e, self.meta)\n",
    "        self.__update_colbins_rowbins_metas()\n",
    "\n",
    "    def convert_rowbins_to_rec(self):\n",
    "        recs = []\n",
    "        self.end_relative = []\n",
    "        t_to_median = 0\n",
    "        t_cumulative = 0\n",
    "        self.temppaths = []\n",
    "        for i, e in enumerate(self.rowbins):\n",
    "            print(f\"Reading {e}\")\n",
    "            rec, temppath = convert_ddfrowbin_to_si(e, self.meta)\n",
    "            recs.append(rec)\n",
    "            self.temppaths.append(temppath)\n",
    "\n",
    "            if i <= self._idx_median_datetime:\n",
    "                t_to_median += rec.get_duration()\n",
    "            t_cumulative += rec.get_duration()\n",
    "            self.end_relative.append(t_cumulative)\n",
    "\n",
    "        self.LongRecording = si.concatenate_recordings(recs)\n",
    "        self.start_datetime = self._median_datetime - timedelta(seconds=t_to_median)\n",
    "\n",
    "    def cleanup_rec(self):\n",
    "        try:\n",
    "            del self.LongRecording\n",
    "        except AttributeError:\n",
    "            warnings.warn(\"LongRecording does not exist, probably deleted already\")\n",
    "        for tpath in self.temppaths:\n",
    "            Path.unlink(tpath)\n",
    "\n",
    "    def get_num_fragments(self, fragment_len_s):\n",
    "        frag_len_idx = self.__time_to_idx(fragment_len_s)\n",
    "        duration_idx = self.LongRecording.get_num_frames()\n",
    "        return math.ceil(duration_idx / frag_len_idx)\n",
    "\n",
    "    def __time_to_idx(self, time_s):\n",
    "        return self.LongRecording.time_to_sample_index(time_s)\n",
    "    \n",
    "    def __idx_to_time(self, idx):\n",
    "        return self.LongRecording.sample_index_to_time(idx)\n",
    "\n",
    "    def get_fragment(self, fragment_len_s, fragment_idx):\n",
    "        startidx, endidx = self.__fragidx_to_startendind(fragment_len_s, fragment_idx)\n",
    "        return self.LongRecording.frame_slice(startidx, endidx)\n",
    "    \n",
    "    def get_dur_fragment(self, fragment_len_s, fragment_idx):\n",
    "        startidx, endidx = self.__fragidx_to_startendind(fragment_len_s, fragment_idx)\n",
    "        return self.__idx_to_time(endidx) - self.__idx_to_time(startidx)\n",
    "    \n",
    "    def get_datetime_fragment(self, fragment_len_s, fragment_idx):\n",
    "        idx, _ = self.__fragidx_to_startendind(fragment_len_s, fragment_idx)\n",
    "        return self.start_datetime + timedelta(seconds=self.__idx_to_time(idx))\n",
    "        \n",
    "    def __fragidx_to_startendind(self, fragment_len_s, fragment_idx):\n",
    "        frag_len_idx = self.__time_to_idx(fragment_len_s)\n",
    "        startidx = frag_len_idx * fragment_idx\n",
    "        endidx = min(frag_len_idx * (fragment_idx + 1), self.LongRecording.get_num_frames())\n",
    "        return startidx, endidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MountainSortOrganizer:\n",
    "    def __init__(self, recording, plot_probe=False, verbose=False, n_jobs: None | int = None) -> None:\n",
    "        assert isinstance(recording, si.BaseRecording)\n",
    "        self.recording = recording\n",
    "        # self.notch_filter = 60 # Hz\n",
    "        self.n_channels = recording.get_num_channels()\n",
    "        self.verbose = verbose\n",
    "        if n_jobs is not None:\n",
    "            si.set_global_job_kwargs(n_jobs=n_jobs) # n_jobs = -1 => use all cores (may be slower)\n",
    "        \n",
    "        # Create a dummy probe\n",
    "        linprobe = pi.generate_linear_probe(self.n_channels, ypitch=40)\n",
    "        linprobe.set_device_channel_indices(self.recording.get_channel_ids())\n",
    "        linprobe.set_contact_ids(self.recording.get_channel_ids())\n",
    "        self.recording = self.recording.set_probe(linprobe)\n",
    "\n",
    "        # Visualize\n",
    "        if plot_probe:\n",
    "            _, ax2 = plt.subplots(1, 1)\n",
    "            plot_probe(linprobe, ax=ax2, with_device_index=True, with_contact_id=True)\n",
    "            plt.show()\n",
    "\n",
    "    def preprocess_recording(self, freq_min=100):\n",
    "        rec_prep = spre.common_reference(self.recording)\n",
    "        rec_prep = spre.scale(rec_prep, gain=10) # Scaling for whitening to work properly\n",
    "        rec_prep = spre.whiten(rec_prep)\n",
    "        rec_prep = spre.highpass_filter(rec_prep, freq_min=freq_min, ftype='bessel')\n",
    "        # rec_prep = spre.bandpass_filter(rec_prep, freq_min=freq_min, freq_max=freq_max, ftype='bessel')\n",
    "        # rec_preps = []\n",
    "        # for i in range(self.n_channels):\n",
    "        #     rec_preps.append(rec_prep.remove_channels(np.delete(np.arange(self.n_channels), i)))\n",
    "        # rec_prep = rec_prep.remove_channels(np.arange(1, 8)) # Experimental, remove all channels except one\n",
    "        # self.prep_recordings = rec_preps\n",
    "        self.prep_recording = rec_prep\n",
    "    \n",
    "    def extract_spikes(self, snippet_T=0.1):\n",
    "        snippet_samples = round(self.recording.sampling_frequency * snippet_T)\n",
    "        temp_dir = Path(tempfile.gettempdir()) / os.urandom(24).hex()\n",
    "        os.makedirs(temp_dir)\n",
    "        sort_params = Scheme2SortingParameters(\n",
    "            phase1_detect_channel_radius=1, \n",
    "            detect_channel_radius=1, \n",
    "            snippet_T1=snippet_samples, \n",
    "            snippet_T2=snippet_samples,\n",
    "            )\n",
    "        \n",
    "        recording_cached = create_cached_recording(self.prep_recording, folder=temp_dir)\n",
    "\n",
    "        with HiddenPrints(silence=not self.verbose):\n",
    "            # Sort over all channels\n",
    "            self.sorting = sorting_scheme2(\n",
    "                recording=recording_cached,\n",
    "                sorting_parameters=sort_params\n",
    "            )\n",
    "            # Sort on individual channels\n",
    "            self.sortings = []\n",
    "            for i in range(self.n_channels):\n",
    "                sorting = sorting_scheme2(\n",
    "                            recording=recording_cached.remove_channels(np.delete(np.arange(self.n_channels), i)),\n",
    "                            sorting_parameters=sort_params\n",
    "                        )\n",
    "                self.sortings.append(sorting)\n",
    "\n",
    "    def preprocess_final_recording(self, notch_freq=60):\n",
    "        rec_prep = spre.notch_filter(self.recording, freq=notch_freq) # Get rid of mains hum\n",
    "        # rec_prep = spre.highpass_filter(rec_prep, freq_min=60, ftype='bessel')\n",
    "\n",
    "        rec_preps = []\n",
    "        for i in range(self.n_channels):\n",
    "            rec_preps.append(rec_prep.remove_channels(np.delete(np.arange(self.n_channels), i)))\n",
    "        self.prep_final_recording = rec_prep\n",
    "        self.prep_final_recordings = rec_preps\n",
    "\n",
    "    def get_final_analyzer(self, folder=None):\n",
    "        if folder is None:\n",
    "            folder = Path(tempfile.gettempdir()) / os.urandom(24).hex()\n",
    "            os.makedirs(folder)\n",
    "        # self.get_final_sorting()\n",
    "        sorting_analyzers = []\n",
    "        for i,e in enumerate(self.sortings):\n",
    "            sorting_analyzers.append(si.create_sorting_analyzer(e, self.prep_final_recordings[i],\n",
    "                                                                # folder=folder,\n",
    "                                                                sparse=False,\n",
    "                                                                overwrite=True))\n",
    "        sorting_analyzer = si.create_sorting_analyzer(self.sorting, self.prep_final_recording, \n",
    "                                                    #   folder=folder, \n",
    "                                                      sparse=False, \n",
    "                                                      overwrite=True)\n",
    "        \n",
    "        self.sorting_analyzer = sorting_analyzer\n",
    "        self.sorting_analyzers = sorting_analyzers\n",
    "        return sorting_analyzer, sorting_analyzers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LongRecordingAnalyzer:\n",
    "    \n",
    "    FEATURES = ['rms', 'ampvar', 'psd', 'psdtotal', 'psdband', 'psdslope', 'cohere', 'pcorr', 'nspike', 'wavetemp']\n",
    "    GLOBAL_FEATURES = ['templates']\n",
    "    FREQ_BANDS = {'delta' : (0.1, 4),\n",
    "                'theta' : (4, 8),\n",
    "                'alpha' : (8, 13),\n",
    "                'beta'  : (13, 25),\n",
    "                'gamma' : (25, 50)}\n",
    "    FREQ_BAND_TOTAL = (0.1, 50)\n",
    "    FREQ_MINS = [v[0] for k,v in FREQ_BANDS.items()]\n",
    "    FREQ_MAXS = [v[1] for k,v in FREQ_BANDS.items()]\n",
    "    FREQ_BAND_NAMES = list(FREQ_BANDS.keys())\n",
    "\n",
    "    def __init__(self, longrecording, fragment_len_s=10, notch_freq=60) -> None:\n",
    "\n",
    "        assert isinstance(longrecording, LongRecordingOrganizer)\n",
    "\n",
    "        self.LongRecording = longrecording\n",
    "        self.fragment_len_s = fragment_len_s\n",
    "        self.n_fragments = longrecording.get_num_fragments(fragment_len_s)\n",
    "        self.channel_to_info = longrecording.meta.channel_to_info\n",
    "        self.channel_names = longrecording.channel_names\n",
    "        self.n_channels = longrecording.meta.n_channels\n",
    "        self.V_units = longrecording.meta.V_units\n",
    "        self.mult_to_uV = longrecording.meta.mult_to_uV\n",
    "        self.f_s = int(longrecording.meta.f_s)\n",
    "        self.notch_freq = notch_freq\n",
    "        \n",
    "\n",
    "    def get_fragment_rec(self, index) -> si.BaseRecording:\n",
    "        \"\"\"Get window at index as a spikeinterface recording object\n",
    "\n",
    "        Args:\n",
    "            index (int): Index of time window\n",
    "\n",
    "        Returns:\n",
    "            si.BaseRecording: spikeinterface recording object\n",
    "        \"\"\"\n",
    "        return self.LongRecording.get_fragment(self.fragment_len_s, index)\n",
    "\n",
    "    def get_fragment_np(self, index, recobj=None) -> np.ndarray:\n",
    "        \"\"\"Get window at index as a numpy array object\n",
    "\n",
    "        Args:\n",
    "            index (int): Index of time window\n",
    "            recobj (si.BaseRecording, optional): If not None, uses this recording object to get the numpy array. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Numpy array with dimensions (N, M), N = number of samples, M = number of channels. Values in uV\n",
    "        \"\"\"\n",
    "        assert isinstance(recobj, si.BaseRecording) or recobj is None\n",
    "        if recobj is None:\n",
    "            return self.get_fragment_rec(index).get_traces(return_scaled=True) # (num_samples, num_channels), in units uV\n",
    "        else:\n",
    "            return recobj.get_traces(return_scaled=True)\n",
    "\n",
    "    def get_fragment_mne(self, index, recobj=None) -> np.ndarray:\n",
    "        \"\"\"Get window at index as a numpy array object, formatted for ease of use with MNE functions\n",
    "\n",
    "        Args:\n",
    "            index (int): Index of time window\n",
    "            recobj (si.BaseRecording, optional): If not None, uses this recording object to get the numpy array. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Numpy array with dimensions (1, M, N), M = number of channels, N = number of samples. 1st dimension corresponds\n",
    "             to number of epochs, which there is only 1 in a window. Values in uV\n",
    "        \"\"\"\n",
    "        rec = self.get_fragment_np(index, recobj=recobj)[..., np.newaxis]\n",
    "        return np.transpose(rec, (2, 1, 0)) # (1 epoch, num_channels, num_samples)\n",
    "\n",
    "    def compute_rms(self, index, **kwargs):\n",
    "        \"\"\"Compute root mean square amplitude\n",
    "\n",
    "        Args:\n",
    "            index (int): Index of time window\n",
    "\n",
    "        Returns:\n",
    "            rms (np.ndarray): Array with shape (M, ), M = number of channels\n",
    "        \"\"\"\n",
    "        rec = self.get_fragment_np(index)\n",
    "        return np.sqrt((rec ** 2).sum(axis=0) / rec.shape[0])\n",
    "    \n",
    "    def compute_ampvar(self, index, **kwargs):\n",
    "        \"\"\"Compute amplitude variance\n",
    "\n",
    "        Args:\n",
    "            index (int): Index of time window\n",
    "\n",
    "        Returns:\n",
    "            ampvar (np.ndarray): Array with shape (M, ), M = number of channels\n",
    "        \"\"\"\n",
    "        rec = self.get_fragment_np(index)\n",
    "        return np.std(rec, axis=0) ** 2\n",
    "    \n",
    "    def compute_psd(self, index, welch_bin_t=1, notch_filter=True, multitaper=False, n_jobs=None, **kwargs):\n",
    "        \"\"\"Compute PSD (power spectral density)\n",
    "\n",
    "        Args:\n",
    "            index (int): Index of time window\n",
    "            welch_bin_t (int, optional): Length of time bins to use in Welch's method, in seconds. Defaults to 1.\n",
    "            notch_filter (bool, optional): If True, inserts a notch filter at the line frequency specified in self.notch_freq. Defaults to True.\n",
    "            multitaper (bool, optional): If True, uses the multitaper method in MNE instead of Welch's method to compute the PSD. Defaults to False.\n",
    "            n_jobs (int, optional): Number of jobs to use in multitaper computation. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            f (np.ndarray): Array of sample frequencies with shape (X, ), X = number of sample frequencies.\n",
    "            psd (np.ndarray): Array of PSD values at sample frequencies with shape (X, M), M = number of channels.\n",
    "            If sample window length is too short, PSD is interpolated\n",
    "        \"\"\"\n",
    "        rec = self.get_fragment_rec(index)\n",
    "        if notch_filter:\n",
    "            rec = spre.notch_filter(rec, freq=self.notch_freq, q=100)\n",
    "        rec_np = rec.get_traces(return_scaled=True)\n",
    "\n",
    "        if not multitaper:\n",
    "            f, psd = welch(rec_np, fs=self.f_s, nperseg=round(welch_bin_t * self.f_s), axis=0)\n",
    "            \n",
    "            if index == self.n_fragments - 1 and self.n_fragments > 1:\n",
    "                f_prev, _ = self.compute_psd(index - 1, welch_bin_t, notch_filter, multitaper)\n",
    "                psd = Akima1DInterpolator(f, psd, axis=0, extrapolate=True)(f_prev)\n",
    "                f = f_prev\n",
    "        else:\n",
    "            psd, f = psd_array_multitaper(rec_np.transpose(), self.f_s, fmax=self.FREQ_BAND_TOTAL[1],\n",
    "                                            adaptive=True, n_jobs=n_jobs, normalization='full', low_bias=False, verbose=0)\n",
    "            psd = psd.transpose()\n",
    "        return f, psd\n",
    "    \n",
    "    def compute_psdband(self, index, welch_bin_t=1, notch_filter=True, bands=None, multitaper=False, f_psd=None, **kwargs):\n",
    "        \"\"\"Compute power of each band in PSD\n",
    "\n",
    "        Args:\n",
    "            index (int): Index of time window\n",
    "            welch_bin_t (int, optional): Length of time bins to use in Welch's method, in seconds. Defaults to 1.\n",
    "            notch_filter (bool, optional): If True, inserts a notch filter at the line frequency specified in self.notch_freq. Defaults to True.\n",
    "            bands (dict[str, tuple], optional): Frequency bands to calculate over; see FREQ_BANDS for formatting. keys = band name, values = (lowcutoff, highcutoff). Defaults to None.\n",
    "            multitaper (bool, optional): If True, uses the multitaper method in MNE instead of Welch's method to compute the PSD. Defaults to False.\n",
    "            f_psd (optional): Output of compute_psd. If not None, will be used to calculate statistic, otherwise interally calls compute_psd. Useful to avoid re-computing the PSD. Defaults to None.\n",
    "            \n",
    "        Returns:\n",
    "            psdband (dict[str, np.ndarray]): Dictionary with keys = band name (delta, theta, ...) and values = array of shape (M, ), M = number of channels\n",
    "        \"\"\"\n",
    "\n",
    "        fbands = self.FREQ_BANDS if bands is None else bands\n",
    "        if f_psd is not None:\n",
    "            f, psd = f_psd\n",
    "        else:\n",
    "            f, psd = self.compute_psd(index, welch_bin_t, notch_filter, multitaper)\n",
    "        deltaf = np.diff(f).mean()\n",
    "\n",
    "        out = {}\n",
    "        for k,v in fbands.items():\n",
    "            out_v = simpson(psd[np.logical_and(f >= v[0], f <= v[1]), :], dx=deltaf, axis=0)\n",
    "            out[k] = out_v\n",
    "        return out\n",
    "    \n",
    "    def compute_psdtotal(self, index, welch_bin_t=1, notch_filter=True, band: list[int]=None, multitaper=False, f_psd=None, **kwargs):\n",
    "        \"\"\"Compute total power over PSD (power spectral density) plot within a specified frequency band\n",
    "\n",
    "        Args:\n",
    "            index (int): Index of time window\n",
    "            welch_bin_t (int, optional): Length of time bins to use in Welch's method, in seconds. Defaults to 1.\n",
    "            notch_filter (bool, optional): If True, inserts a notch filter at the line frequency specified in self.notch_freq. Defaults to True.\n",
    "            band (list[int], optional): Frequency band to calculate over. band[0] is the lowest frequency, band[1] is the highest. If None, uses self.FREQ_BAND_TOTAL. Defaults to None.\n",
    "            multitaper (bool, optional): If True, uses the multitaper method in MNE instead of Welch's method to compute the PSD. Defaults to False.\n",
    "            f_psd (optional): Output of compute_psd. If not None, will be used to calculate statistic, otherwise interally calls compute_psd. Useful to avoid re-computing the PSD. Defaults to None.\n",
    "            n_jobs (int, optional): Number of jobs to use in multitaper computation. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            psdtotal (np.ndarray): Array with shape (M, ), M = number of channels\n",
    "        \"\"\"\n",
    "\n",
    "        fband = self.FREQ_BAND_TOTAL if band is None else band\n",
    "        if f_psd is not None:\n",
    "            f, psd = f_psd\n",
    "        else:\n",
    "            f, psd = self.compute_psd(index, welch_bin_t, notch_filter, multitaper)\n",
    "        deltaf = np.diff(f).mean()\n",
    "\n",
    "        return simpson(psd[np.logical_and(f >= fband[0], f <= fband[1]), :], dx=deltaf, axis=0)\n",
    "    \n",
    "    def compute_psdslope(self, index, welch_bin_t=1, notch_filter=True, band=None, multitaper=False, f_psd=None, **kwargs):\n",
    "        \"\"\"Compute slope of PSD. Performs linear regression on a log10-log10 plot.\n",
    "\n",
    "        Args:\n",
    "            index (int): Index of time window\n",
    "            welch_bin_t (int, optional): Length of time bins to use in Welch's method, in seconds. Defaults to 1.\n",
    "            notch_filter (bool, optional): If True, inserts a notch filter at the line frequency specified in self.notch_freq. Defaults to True.\n",
    "            band (list[int], optional): Frequency band to calculate over. band[0] is the lowest frequency, band[1] is the highest. If None, uses self.FREQ_BAND_TOTAL. Defaults to None.\n",
    "            multitaper (bool, optional): If True, uses the multitaper method in MNE instead of Welch's method to compute the PSD. Defaults to False.\n",
    "            f_psd (optional): Output of compute_psd. If not None, will be used to calculate statistic, otherwise interally calls compute_psd. Useful to avoid re-computing the PSD. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            psdslope (list[tuple]): List of length M containing tuples of (slope, intercept) on log10-log10 plot, M = number of channels\n",
    "        \"\"\"\n",
    "\n",
    "        fband = self.FREQ_BAND_TOTAL if band is None else band\n",
    "        if f_psd is not None:\n",
    "            f, psd = f_psd\n",
    "        else:\n",
    "            f, psd = self.compute_psd(index, welch_bin_t, notch_filter, multitaper)\n",
    "        \n",
    "        frange = np.logical_and(f >= fband[0], f <= fband[1])\n",
    "        logf = np.log10(f[frange])\n",
    "        logpsd = np.log10(psd[frange, :])\n",
    "\n",
    "        out = []\n",
    "        for i in range(logpsd.shape[1]):\n",
    "            result = linregress(logf, logpsd[:, i], 'less')\n",
    "            out.append((result.slope, result.intercept))\n",
    "        return out\n",
    "\n",
    "    # Needs work; will need to accept a geometry file to effectively find multielectrode events\n",
    "    def compute_spikes(self, verbose=False, n_jobs_si: None | int = None, **kwargs):\n",
    "        mso = MountainSortOrganizer(self.LongRecording.LongRecording, verbose=verbose, n_jobs=n_jobs_si)\n",
    "        mso.preprocess_recording()\n",
    "        mso.extract_spikes()\n",
    "        mso.preprocess_final_recording()\n",
    "        self.sorting_analyzer, self.sorting_analyzers = mso.get_final_analyzer()\n",
    "        return self.sorting_analyzer, self.sorting_analyzers\n",
    "    \n",
    "    def compute_nspike(self, index, sa_sas=None, **kwargs):\n",
    "        if sa_sas is None:\n",
    "            if not hasattr(self, \"sorting_analyzer\") or not hasattr(self, \"sorting_analyzers\"):\n",
    "                self.compute_spikes(**kwargs)\n",
    "            sa_sas = (self.sorting_analyzer, self.sorting_analyzers)\n",
    "        sa, sas = sa_sas\n",
    "        assert isinstance(sa, si.SortingAnalyzer)\n",
    "        for e in sas:\n",
    "            assert isinstance(e, si.SortingAnalyzer)\n",
    "        \n",
    "        tbound = self.__frag_idx_to_timebound(index)\n",
    "        nspike_unit = []\n",
    "        if sa.get_num_units() > 0:\n",
    "            for unit_id in sa.sorting.unit_ids:\n",
    "                t_spike = sa.sorting.get_unit_spike_train(unit_id=unit_id) / self.f_s\n",
    "                nspike_unit.append(((tbound[0] <= t_spike) & (t_spike < tbound[1])).sum())\n",
    "        else:\n",
    "            print(\"No units across all channels, skipping..\")\n",
    "\n",
    "        nspikes_unit = []\n",
    "        for i,e in enumerate(sas):\n",
    "            nspikes_unit.append([])\n",
    "            if e.get_num_units() == 0:\n",
    "                # print(f\"No units in channel {i}, skipping..\")\n",
    "                continue\n",
    "            for unit_id in e.sorting.unit_ids:\n",
    "                t_spike = e.sorting.get_unit_spike_train(unit_id=unit_id) / self.f_s\n",
    "                nspikes_unit[-1].append(((tbound[0] <= t_spike) & (t_spike < tbound[1])).sum())\n",
    "\n",
    "        return nspikes_unit, nspikes_unit\n",
    "        \n",
    "    def __frag_idx_to_timebound(self, index):\n",
    "        frag_len_idx = round(self.fragment_len_s * self.f_s)\n",
    "        startidx = frag_len_idx * index\n",
    "        endidx = min(frag_len_idx * (index + 1), self.LongRecording.LongRecording.get_num_frames())\n",
    "        return (startidx / self.f_s, endidx / self.f_s)\n",
    "\n",
    "    def compute_wavetemp(self, index, sa_sas=None, ms_before=200, ms_after=200, **kwargs):\n",
    "        if sa_sas is None:\n",
    "            if not hasattr(self, \"sorting_analyzer\") or not hasattr(self, \"sorting_analyzers\"):\n",
    "                self.compute_spikes()\n",
    "            sa_sas = (self.sorting_analyzer, self.sorting_analyzers)\n",
    "        sa, sas = sa_sas\n",
    "        assert isinstance(sa, si.SortingAnalyzer)\n",
    "        for e in sas:\n",
    "            assert isinstance(e, si.SortingAnalyzer)\n",
    "\n",
    "        if hasattr(self, 'computed_sorting_analyzer') and hasattr(self, 'computed_sorting_analyzers'):\n",
    "            return self.computed_sorting_analyzer, self.computed_sorting_analyzers\n",
    "        else:\n",
    "            if sa.get_num_units() > 0:\n",
    "                sa.compute(\"random_spikes\", max_spikes_per_unit=1000)\n",
    "                sa.compute(\"waveforms\", ms_before=ms_before, ms_after=ms_after)\n",
    "                sa.compute(\"templates\", operators=[\"average\", \"median\", \"std\"])\n",
    "            else:\n",
    "                print(\"No units across all channels, skipping..\")\n",
    "            for i,e in enumerate(sas):\n",
    "                if e.get_num_units() == 0:\n",
    "                    # print(f\"No units in channel {i}, skipping..\")\n",
    "                    continue\n",
    "                e.compute(\"random_spikes\", max_spikes_per_unit=1000)\n",
    "                e.compute(\"waveforms\", ms_before=ms_before, ms_after=ms_after)\n",
    "                e.compute(\"templates\", operators=[\"average\", \"median\", \"std\"])\n",
    "\n",
    "            self.computed_sorting_analyzer = sa\n",
    "            self.computed_sorting_analyzers = sas\n",
    "            return sa, sas\n",
    "\n",
    "    def __get_freqs_cycles(self, index, freq_res, n_cycles_max, geomspace, mode:str, epsilon):\n",
    "        if geomspace:\n",
    "            freqs = np.geomspace(self.FREQ_BAND_TOTAL[0], self.FREQ_BAND_TOTAL[1], round((np.diff(self.FREQ_BAND_TOTAL) / freq_res).item()))\n",
    "        else:\n",
    "            freqs = np.arange(self.FREQ_BAND_TOTAL[0], self.FREQ_BAND_TOTAL[1], freq_res)\n",
    "\n",
    "        frag_len_s = self.LongRecording.get_dur_fragment(self.fragment_len_s, index)\n",
    "        match mode:\n",
    "            case 'cwt_morlet':\n",
    "                maximum_cyc = (frag_len_s * self.f_s + 1) * np.pi / 5 * freqs / self.f_s\n",
    "                # print(fwhm(freqs, n_cycles_max))\n",
    "            case 'multitaper':\n",
    "                maximum_cyc = frag_len_s * freqs\n",
    "            case _:\n",
    "                raise ValueError(f\"Invalid mode {mode}, pick 'cwt_morlet' or 'multitaper'\")\n",
    "        maximum_cyc = maximum_cyc - epsilon # Shave off a bit to avoid indexing errors\n",
    "        n_cycles = np.minimum(np.full(maximum_cyc.shape, n_cycles_max), maximum_cyc)\n",
    "        return freqs, n_cycles\n",
    "\n",
    "    def compute_cohere(self, index, freq_res=1, n_cycles_max=7.0, geomspace=True, mode:Literal['cwt_morlet', 'multitaper']='cwt_morlet', downsamp_q=4, epsilon=1e-2, n_jobs_coh:int=None, **kwargs):\n",
    "        \"\"\"Compute pairwise channel coherence over frequency bands\n",
    "\n",
    "        Args:\n",
    "            index (int): Index of time window\n",
    "            freq_res (float, optional): Resolution of frequency bins. If geomspace=True, respaces frequencies over a geometric series (linear in log frequency). Defaults to 1.\n",
    "            n_cycles_max (float, optional): Maximum number of signal cycles to include in analysis. Defaults to 7.0.\n",
    "            geomspace (bool, optional): See freq_res. Defaults to True.\n",
    "            mode (Literal[&#39;cwt_morlet&#39;, &#39;multitaper&#39;], optional, optional): Spectrum estimation mode. Defaults to 'cwt_morlet'.\n",
    "            downsamp_q (int, optional): Downsampling factor before calculation. Useful to greatly speed up analysis. Defaults to 4.\n",
    "            epsilon (float, optional): Number of cycles to shave off n_cycles_max, to avoid indexing errors. Defaults to 1e-2.\n",
    "            n_jobs_coh (int, optional): Number of jobs, useful for parallel processing. Defaults to None.\n",
    "\n",
    "        Raises:\n",
    "            MemoryError: If freq_res is too small and program runs out of memory, this is raised\n",
    "\n",
    "        Returns:\n",
    "            cohere (dict[str, np.ndarray]): Dictionary with keys = band names, values = (M, M) lower triangular array with value at (i, j) = coherence between channel i and j at that band\n",
    "        \"\"\"\n",
    "        \n",
    "        rec = self.get_fragment_mne(index)\n",
    "        rec = decimate(rec, q=downsamp_q, axis=-1)\n",
    "        freqs, n_cycles = self.__get_freqs_cycles(index=index, freq_res=freq_res, n_cycles_max=n_cycles_max, geomspace=geomspace, mode=mode, epsilon=epsilon)\n",
    "        try:\n",
    "            con = spectral_connectivity_time(rec,\n",
    "                                            freqs=freqs,\n",
    "                                            method='coh',\n",
    "                                            average=True,\n",
    "                                            faverage=True,\n",
    "                                            mode=mode,\n",
    "                                            fmin=self.FREQ_MINS,\n",
    "                                            fmax=self.FREQ_MAXS,\n",
    "                                            sfreq=self.f_s / downsamp_q,\n",
    "                                            n_cycles=n_cycles,\n",
    "                                            n_jobs=n_jobs_coh,\n",
    "                                            verbose=False)\n",
    "        except MemoryError as e:\n",
    "            raise MemoryError(\"Out of memory, use a larger freq_res parameter\") from e\n",
    "        data = con.get_data()\n",
    "        out = {}\n",
    "        for i in range(data.shape[1]):\n",
    "            out[self.FREQ_BAND_NAMES[i]] = data[:, i].reshape((self.n_channels, self.n_channels))\n",
    "        return out\n",
    "    \n",
    "    def compute_cacoh(self, index, freq_res=1, n_cycles_max=7.0, geomspace=True, mode:str='cwt_morlet', downsamp_q=4, epsilon=1e-2, mag_phase=True, indices=None, **kwargs):\n",
    "        rec = self.get_fragment_mne(index)\n",
    "        rec = decimate(rec, q=downsamp_q, axis=-1)\n",
    "        freqs, n_cycles = self.__get_freqs_cycles(index=index, freq_res=freq_res, n_cycles_max=n_cycles_max, geomspace=geomspace, mode=mode, epsilon=epsilon)\n",
    "        try:\n",
    "            con = spectral_connectivity_time(rec,\n",
    "                                            freqs=freqs,\n",
    "                                            method='cacoh',\n",
    "                                            average=True,\n",
    "                                            mode=mode,\n",
    "                                            fmin=self.FREQ_BAND_TOTAL[0],\n",
    "                                            fmax=self.FREQ_BAND_TOTAL[1],\n",
    "                                            sfreq=self.f_s / downsamp_q,\n",
    "                                            n_cycles=n_cycles,\n",
    "                                            indices=indices, # TODO implement L/R hemisphere coherence metrics\n",
    "                                            verbose=False)\n",
    "        except MemoryError as e:\n",
    "            raise MemoryError(\"Out of memory, use a larger freq_res parameter\") from e\n",
    "        \n",
    "        data:np.ndarray = con.get_data().squeeze()\n",
    "        if mag_phase:\n",
    "            return np.abs(data), np.angle(data, deg=True), con.freqs\n",
    "        else:\n",
    "            return data, con.freqs\n",
    "\n",
    "    def compute_pcorr(self, index, lower_triag=True, **kwargs) -> np.ndarray:\n",
    "        rec = spre.bandpass_filter(self.get_fragment_rec(index),\n",
    "                                    freq_min=self.FREQ_BAND_TOTAL[0],\n",
    "                                    freq_max=self.FREQ_BAND_TOTAL[1])\n",
    "        rec = self.get_fragment_np(index, rec).transpose()\n",
    "        result = pearsonr(rec[:, np.newaxis, :], rec, axis=-1)\n",
    "        if lower_triag:\n",
    "            return np.tril(result.correlation, k=-1)\n",
    "        else:\n",
    "            return result.correlation\n",
    "    \n",
    "    def compute_csd(self, index, magnitude=True, n_jobs=None, **kwargs) -> np.ndarray:\n",
    "        rec = self.get_fragment_mne(index)\n",
    "        csd = csd_array_fourier(rec, self.f_s, \n",
    "                                fmin=self.FREQ_BAND_TOTAL[0], \n",
    "                                fmax=self.FREQ_BAND_TOTAL[1], \n",
    "                                ch_names=self.channel_names,\n",
    "                                n_jobs=n_jobs,\n",
    "                                verbose=False)\n",
    "        out = {}\n",
    "        for k,v in self.FREQ_BANDS.items():\n",
    "            try:\n",
    "                csd_band = csd.mean(fmin=v[0], fmax=v[1]) # Breaks if slice is too short\n",
    "            except (IndexError, UnboundLocalError):\n",
    "                timebound = self.__frag_idx_to_timebound(index)\n",
    "                warnings.warn(f\"compute_csd failed for window {index}, {round(timebound[1]-timebound[0], 5)} s. Likely too short\")\n",
    "                data = self.compute_csd(index - 1, magnitude)[k]\n",
    "            else:\n",
    "                data = csd_band.get_data()\n",
    "            finally:\n",
    "                if magnitude:\n",
    "                    out[k] = np.abs(data)\n",
    "                else:\n",
    "                    out[k] = data\n",
    "        return out\n",
    "\n",
    "    def compute_envcorr(self, index, **kwargs) -> np.ndarray:\n",
    "        rec = spre.bandpass_filter(self.get_fragment_rec(index),\n",
    "                                    freq_min=self.FREQ_BAND_TOTAL[0],\n",
    "                                    freq_max=self.FREQ_BAND_TOTAL[1])\n",
    "        rec = self.get_fragment_mne(index, rec)\n",
    "        envcor = envelope_correlation(rec, self.channel_names)\n",
    "        return envcor.get_data().reshape((self.n_channels, self.n_channels))\n",
    "    \n",
    "    def compute_pac(self, index):\n",
    "        ... # TODO implement CFC measures\n",
    "        \n",
    "\n",
    "    def get_file_end(self, index, **kwargs):\n",
    "        tstart, tend = self.__frag_idx_to_timebound(index)\n",
    "        for tfile in self.LongRecording.end_relative:\n",
    "           if tstart <= tfile < tend:\n",
    "               return tfile - tstart\n",
    "        return None \n",
    "\n",
    "    def setup_njobs(self):\n",
    "        set_config('MNE_MEMMAP_MIN_SIZE', '30M')\n",
    "        set_config('MNE_CACHE_DIR', Path(tempfile.gettempdir()) / os.urandom(24).hex())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Analysis Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": [
     "note"
    ]
   },
   "outputs": [],
   "source": [
    "%%script true\n",
    "# lrec = LongRecordingOrganizer(base_folder_path=\"/mnt/isilon/marsh_single_unit/PythonEEG\",\n",
    "#                             colbin_folder_path=\"/mnt/isilon/marsh_single_unit/PythonEEG/bins-mat\",\n",
    "#                             rowbin_folder_path=\"/mnt/isilon/marsh_single_unit/PythonEEG/bins-py\",\n",
    "#                             metadata_path=None)\n",
    "# lrec = LongRecordingOrganizer(base_folder_path=r\"Z:\\PythonEEG\",\n",
    "#                             colbin_folder_path=r\"Z:\\PythonEEG\\bins\",\n",
    "#                             rowbin_folder_path=r\"Z:\\PythonEEG\\bins\",\n",
    "#                             metadata_path=None)\n",
    "lrec = LongRecordingOrganizer(base_folder_path=r\"Z:\\PythonEEG_testdir\", # Change to your directories!\n",
    "                            colbin_folder_path=r\"Z:\\PythonEEG_testdir\\bins-mat\",\n",
    "                            rowbin_folder_path=r\"Z:\\PythonEEG_testdir\\bins-py\",\n",
    "                            metadata_path=None,\n",
    "                            truncate=5)\n",
    "# lrec = LongRecordingOrganizer(base_folder_path=r\"Z:\\PythonEEG Data Bins\\N24 Cage 3 KO\",\n",
    "#                             metadata_path=None)\n",
    "lrec.convert_colbins_to_rowbins(overwrite=False)\n",
    "lrec.convert_rowbins_to_rec()\n",
    "\n",
    "# sw.plot_traces(lrec.get_fragment(5, 50), channel_ids=[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": [
     "note"
    ]
   },
   "outputs": [],
   "source": [
    "%%script true\n",
    "lan = LongRecordingAnalyzer(lrec, 10)\n",
    "print(lan.n_fragments, lan.n_channels)\n",
    "\n",
    "lan.setup_njobs()\n",
    "mags = []\n",
    "phases = []\n",
    "\n",
    "for i in range(lan.n_fragments):\n",
    "# for i in range(lan.n_fragments - 5, lan.n_fragments):\n",
    "    if i == 20:\n",
    "        break\n",
    "    t = time.process_time()\n",
    "\n",
    "    # lan.compute_nspike(i)\n",
    "    # csd = lan.compute_csd(i)\n",
    "    # plt.imshow(csd['alpha'])\n",
    "\n",
    "    coh = lan.compute_cohere(i)\n",
    "    print(coh)\n",
    "    plt.imshow(coh['gamma'])\n",
    "\n",
    "    # pcorr = lan.compute_pcorr(i)\n",
    "    # plt.imshow(pcorr)\n",
    "\n",
    "    # cacoh = lan.compute_cacoh(i, indices=([[1]], [[6]]))\n",
    "    # plt.plot(cacoh[2], cacoh[0])\n",
    "    # plt.plot(cacoh[2], cacoh[1])\n",
    "\n",
    "    # cacoh = lan.compute_cacoh(i, freq_res=1, indices=([[0,1,2,3]], [[4,5,6,7]]))\n",
    "    # mags.append(cacoh[0])\n",
    "    # phases.append(cacoh[1])\n",
    "\n",
    "    # lan.compute_rms(i)\n",
    "    # lan.compute_ampvar(i)\n",
    "    # f, psd = lan.compute_psd(i)\n",
    "    # lan.compute_psdband(i, f_psd=(f, psd))\n",
    "    # lan.compute_psdtotal(i, f_psd=(f, psd))\n",
    "    # mb = lan.compute_psdslope(i, f_psd=(f, psd))\n",
    "    # lan.compute_cohere(i)\n",
    "    # corr = lan.compute_envcorr(i)\n",
    "\n",
    "    # fig, ax = plt.subplots(1, 2, figsize=(6, 3))\n",
    "    # ax[0].imshow(corr)\n",
    "    # ax[1].imshow(pcorr)\n",
    "    # plt.show()\n",
    "\n",
    "    print(time.process_time() - t)\n",
    "    \n",
    "# _, ax = plt.subplots(1, 2, figsize=(8, 2), sharex=True)\n",
    "# mags = np.stack(mags, axis=-1).mean(axis=-1)\n",
    "# phases = np.stack(phases, axis=-1).mean(axis=-1)\n",
    "# ax[0].semilogx(cacoh[2], mags.transpose())\n",
    "# ax[1].semilogx(cacoh[2], phases.transpose(), c='C1')\n",
    "# ax[0].set_title(\"Magnitude\")\n",
    "# ax[1].set_title(\"Phase\")\n",
    "# plt.show()\n",
    "\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(8, 3))\n",
    "# ax.loglog(f, psd)\n",
    "# for i, (m,b) in enumerate(mb):\n",
    "#     ax.plot(f, 10**(b + m * np.log10(f)), c=f'C{i}')\n",
    "# ax.set_ylabel(\"PSD (uV^2/Hz)\")\n",
    "# ax.set_xlabel(\"Frequency (Hz)\")\n",
    "# ax.axvline(60, c='black', ls='--', alpha=0.25)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": [
     "note"
    ]
   },
   "outputs": [],
   "source": [
    "%%script true\n",
    "# Computationally heavy tests\n",
    "# lan.compute_spikes() \n",
    "# lan.compute_wavetemp()\n",
    "lan.compute_nspike(10) # Will autotrigger compute_spikes\n",
    "wt, wts = lan.get_temps_from_wavetemp()\n",
    "\n",
    "for wtch in wts:\n",
    "    if wtch is not None:\n",
    "        print(wtch.shape)\n",
    "    else:\n",
    "        continue\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 1.5))\n",
    "    for i in range(wtch.shape[0]):\n",
    "        ax.plot(wtch[i, :, :])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test PygWalker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": [
     "note"
    ]
   },
   "outputs": [],
   "source": [
    "%%script true\n",
    "class MainWindow(qw.QMainWindow): # TODO write GUI for application\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.setWindowTitle(\"Python EEG\")\n",
    "\n",
    "        self.label = qw.QLabel()\n",
    "        self.input = qw.QLineEdit()\n",
    "        self.input.textChanged.connect(self.label.setText)\n",
    "\n",
    "\n",
    "        self.mat_button = qw.QPushButton(\"Open .MAT Folder\")\n",
    "        self.mat_pathdialog = qw.QFileDialog()\n",
    "        self.mat_pathlabel = qw.QLabel(\"(No directory selected)\")\n",
    "        self.mat_directory = None\n",
    "        \n",
    "        self.mat_button.clicked.connect(self.select_mat_directory)\n",
    "\n",
    "\n",
    "        # self.mat_button.clicked.connect(self.mat_pathdialog.getExistingDirectory)\n",
    "        # self.mat_pathdialog.urlSelected.connect(self.set_mat_directory)\n",
    "        # self.mat_pathdialog.\n",
    "        # self.mat_pathdialog.directoryUrlEntered.connect(self.mat_pathlabel.setText)\n",
    "        # self.mat_pathdialog.currentUrlChanged.connect(self.mat_pathlabel.setText)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        # self.button = qw.QPushButton(\"Button to open html...\")\n",
    "        # self.button.clicked.connect(self.open_html_window)\n",
    "        \n",
    "        # self.browser = qweb.QWebEngineView()\n",
    "        # self.html = teststring\n",
    "        # url = qc.QUrl.fromLocalFile(r\"/mnt/isilon/marsh_single_unit/PythonEEG/html_pyg_walker.html\")\n",
    "        # self.browser.setUrl(url)\n",
    "        # self.browser.loadProgress.connect(self.log_loading_progress)\n",
    "        # self.browser.loadFinished.connect(self.log_loading_finished)\n",
    "\n",
    "\n",
    "        layout = qw.QVBoxLayout()\n",
    "        layout.addWidget(self.input)\n",
    "        layout.addWidget(self.label)\n",
    "        layout.addWidget(self.mat_button)\n",
    "        layout.addWidget(self.mat_pathlabel)\n",
    "\n",
    "        container = qw.QWidget()\n",
    "        container.setLayout(layout)\n",
    "\n",
    "        self.setCentralWidget(container)\n",
    "    \n",
    "    # def set_mat_directory(self, e):\n",
    "    #     self.mat_directory = e\n",
    "    #     print(e)\n",
    "        # self.mat_pathlabel.setText(e)\n",
    "    \n",
    "    def select_mat_directory(self):\n",
    "        filedialog = qw.QFileDialog()\n",
    "        self.mat_directory = filedialog.getExistingDirectory(self, \"Open Directory\")\n",
    "        self.mat_pathlabel.setText(self.mat_directory)\n",
    "\n",
    "\n",
    "\n",
    "    def log_loading_progress(self, e):\n",
    "        print(e)\n",
    "\n",
    "    def log_loading_finished(self, e):\n",
    "        print(e)\n",
    "\n",
    "    # def set_matlab_path(self):\n",
    "    #     print(\"Yes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": [
     "note"
    ]
   },
   "outputs": [],
   "source": [
    "%%script true\n",
    "app = qc.QCoreApplication.instance()\n",
    "if app is None:\n",
    "    app = qw.QApplication(sys.argv)\n",
    "\n",
    "window = MainWindow()\n",
    "window.show()\n",
    "\n",
    "app.exec()\n",
    "\n",
    "\n",
    "# export XDG_RUNTIME_DIR=/run/user/1028484\n",
    "\n",
    "# export XDG_RUNTIME_DIR=/run/user/$UID \n",
    "# source ~/.bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-win",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
