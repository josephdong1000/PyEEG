{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Testing Notebook - Yong copy\n",
    "\n",
    "This notebook is meant to test external-facing functions to ensure they are working as expected.\n",
    "\n",
    "A dedicated test_all_functions.py with unit testing might be better, but this is a good benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "import tempfile\n",
    "import logging\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import mne\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ohy2/Documents/GitHubRepo/PyEEG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ohy2/Documents/GitHubRepo/PyEEG/.venv_mouseEEG/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DDFBinaryMetadata', 'LongRecordingOrganizer', 'convert_ddfcolbin_to_ddfrowbin', 'convert_ddfrowbin_to_si', 'convert_units_to_multiplier', 'convert_colpath_to_rowpath', 'filepath_to_index', 'is_day', 'set_temp_directory', 'get_temp_directory', 'parse_path_to_animalday', 'parse_path_to_genotype', 'parse_path_to_animal', 'parse_path_to_day', 'parse_chname_to_abbrev', 'nanaverage', 'LongRecordingAnalyzer', 'MountainSortAnalyzer', 'FragmentAnalyzer']\n"
     ]
    }
   ],
   "source": [
    "# packageroot = Path('../../').resolve()\n",
    "packageroot = Path('/Users/ohy2/Documents/GitHubRepo/PyEEG').resolve()\n",
    "print(packageroot)\n",
    "sys.path.append(str(packageroot))\n",
    "\n",
    "from pythoneeg import core\n",
    "from pythoneeg import visualization\n",
    "# from pythoneeg import constants\n",
    "\n",
    "print(core.__all__)\n",
    "# print(visualization.__all__)\n",
    "# print(dir(constants))\n",
    "\n",
    "core.set_temp_directory('/Users/ohy2/Documents/GitHubRepo/PyEEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import logging\n",
    "from typing import Literal, Optional, List, Dict, Any, Union, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "import mne\n",
    "from mne.time_frequency import tfr_morlet\n",
    "\n",
    "class MNEExperimentPlotter:\n",
    "    \"\"\"\n",
    "    A class for creating various plots from a list of MNE objects with grouping capabilities.\n",
    "    \n",
    "    This class provides methods for creating different types of plots (evoked, TFR, etc.)\n",
    "    from MNE data with consistent data processing, grouping, and contrasting.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mne_objects: Union[mne.io.Raw, List[mne.io.Raw]]):\n",
    "        \"\"\"\n",
    "        Initialize plotter with MNE Raw object(s).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        mne_objects : mne.io.Raw or list[mne.io.Raw]\n",
    "            Single MNE Raw object or list of MNE Raw objects\n",
    "        \"\"\"\n",
    "        # Convert single MNE object to list\n",
    "        if not isinstance(mne_objects, list):\n",
    "            mne_objects = [mne_objects]\n",
    "            \n",
    "        self.mne_objects = mne_objects\n",
    "        \n",
    "        # Extract metadata from all objects\n",
    "        self.metadata = []\n",
    "        for obj in mne_objects:\n",
    "            # Ensure temp dict exists\n",
    "            if 'temp' not in obj.info:\n",
    "                obj.info['temp'] = {}\n",
    "            \n",
    "            # Extract metadata from the object\n",
    "            meta = {\n",
    "                'animal_id': obj.info['temp'].get('animal_id', 'Unknown'),\n",
    "                'file_path': obj.info['temp'].get('file_path', 'Unknown'),\n",
    "                'sfreq': obj.info['sfreq'],\n",
    "                'ch_names': obj.ch_names,\n",
    "                'mne_obj': obj  # Store reference to the object itself\n",
    "            }\n",
    "            self.metadata.append(meta)\n",
    "        \n",
    "        # Collect all unique channel names\n",
    "        self.all_channel_names = sorted(list(set([ch for obj in mne_objects for ch in obj.ch_names])))\n",
    "        logging.info(f'all_channel_names: {self.all_channel_names}')\n",
    "\n",
    "\n",
    "    def random_subsample_epochs(epochs: mne.Epochs, max_epochs=1000):\n",
    "        \"\"\"\n",
    "        Randomly subsample epochs if there are more than max_epochs.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        epochs : mne.Epochs\n",
    "            The epochs object to potentially subsample\n",
    "        max_epochs : int\n",
    "            Maximum number of epochs to keep\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        mne.Epochs\n",
    "            The original epochs object if len(epochs) <= max_epochs,\n",
    "            otherwise a randomly subsampled version\n",
    "        \"\"\"\n",
    "        if len(epochs) > max_epochs:\n",
    "            # Get random indices for subsampling\n",
    "            indices = np.random.choice(len(epochs), size=max_epochs, replace=False)\n",
    "            epochs = epochs[indices]\n",
    "        return epochs\n",
    "\n",
    "    def load_mne_objects(animal_ids, base_folder='./test-mnes', rename_channels=True, \n",
    "                    rename_events=True, fmin=1, fmax=50, downsample=125):\n",
    "        \"\"\"\n",
    "        Load MNE objects from SpikeAnalysisResult files for specified animal IDs.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        animal_ids : list of str\n",
    "            List of animal IDs to load data for\n",
    "        base_folder : str or Path\n",
    "            Base folder containing the data files\n",
    "        rename_channels : bool\n",
    "            Whether to rename channels using parse_chname_to_abbrev function\n",
    "        rename_events : bool\n",
    "            Whether to rename event IDs using parse_chname_to_abbrev function\n",
    "        fmin : float\n",
    "            Lower frequency bound for bandpass filter (Hz)\n",
    "        fmax : float\n",
    "            Upper frequency bound for bandpass filter (Hz)\n",
    "        downsample : float or None\n",
    "            Target sampling frequency for downsampling (Hz).\n",
    "            If None, no downsampling is performed.\n",
    "            If specified and lower than the original sampling rate, the data is resampled.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        list of mne.io.Raw\n",
    "            List of loaded MNE Raw objects with metadata\n",
    "        \"\"\"\n",
    "        import glob\n",
    "        from pathlib import Path\n",
    "        from pythoneeg.core.utils import parse_chname_to_abbrev, clean_channel_name\n",
    "        \n",
    "        # Create a list to store all mne objects\n",
    "        mne_objects = []\n",
    "        \n",
    "        # Get all files for each animal ID\n",
    "        for animal_id in animal_ids:\n",
    "            # Find all files matching the pattern for this animal\n",
    "            file_pattern = f'{base_folder}/{animal_id}*'\n",
    "            animal_files = glob.glob(file_pattern)\n",
    "            \n",
    "            print(f\"Found {len(animal_files)} files for {animal_id}:\")\n",
    "            for file in animal_files:\n",
    "                print(f\"  - {file}\")\n",
    "                \n",
    "                # Load each file and add to our list\n",
    "                try:\n",
    "                    reconstruct_sas = visualization.SpikeAnalysisResult.load_fif_and_json(file)\n",
    "                    mne_obj = reconstruct_sas.result_mne\n",
    "\n",
    "                    # Downsample if requested and necessary\n",
    "                    if downsample is not None:\n",
    "                        current_sfreq = mne_obj.info['sfreq']\n",
    "                        if downsample < current_sfreq:\n",
    "                            mne_obj.resample(downsample)\n",
    "                            print(f\"  ✓ Downsampled from {current_sfreq} Hz to {downsample} Hz\")\n",
    "                        else:\n",
    "                            print(f\"  ⚠ Requested downsample rate ({downsample} Hz) is >= current rate ({current_sfreq} Hz). Skipping.\")\n",
    "                    \n",
    "                    # Apply bandpass filter\n",
    "                    mne_obj.filter(l_freq=fmin, h_freq=fmax, verbose=True)\n",
    "                    print(f\"  ✓ Applied bandpass filter: {fmin}-{fmax} Hz\")\n",
    "                    \n",
    "                    # Rename channels if requested\n",
    "                    if rename_channels:\n",
    "                        ch_name_mapping = {}\n",
    "                        for ch_name in mne_obj.ch_names:\n",
    "                            try:\n",
    "                                # Try to get abbreviation using parse_chname_to_abbrev\n",
    "                                abbrev = parse_chname_to_abbrev(ch_name, assume_from_number=True)\n",
    "                                ch_name_mapping[ch_name] = abbrev\n",
    "                            except Exception:\n",
    "                                # Fall back to clean_channel_name if parse_chname_to_abbrev fails\n",
    "                                try:\n",
    "                                    cleaned = clean_channel_name(ch_name)\n",
    "                                    ch_name_mapping[ch_name] = cleaned\n",
    "                                except Exception:\n",
    "                                    # Keep original name if both methods fail\n",
    "                                    ch_name_mapping[ch_name] = ch_name\n",
    "                        \n",
    "                        # Apply the channel renaming\n",
    "                        mne_obj.rename_channels(ch_name_mapping)\n",
    "                        print(f\"  ✓ Renamed channels: {list(ch_name_mapping.values())}\")\n",
    "                    \n",
    "                    # Rename event IDs if requested\n",
    "                    if rename_events:\n",
    "                        # Get current events and event_id\n",
    "                        events, event_dict = mne.events_from_annotations(mne_obj)\n",
    "                        \n",
    "                        # Create mapping from original event names to abbreviated names\n",
    "                        event_mapping = {}\n",
    "                        for event_name in event_dict.keys():\n",
    "                            try:\n",
    "                                # Try to get abbreviation using parse_chname_to_abbrev\n",
    "                                abbrev = parse_chname_to_abbrev(event_name, assume_from_number=True)\n",
    "                                event_mapping[event_name] = abbrev\n",
    "                            except Exception:\n",
    "                                # Fall back to clean_channel_name if parse_chname_to_abbrev fails\n",
    "                                try:\n",
    "                                    cleaned = clean_channel_name(event_name)\n",
    "                                    event_mapping[event_name] = cleaned\n",
    "                                except Exception:\n",
    "                                    # Keep original name if both methods fail\n",
    "                                    event_mapping[event_name] = event_name\n",
    "                        \n",
    "                        # Create new annotations with renamed events\n",
    "                        old_annot = mne_obj.annotations\n",
    "                        new_descriptions = []\n",
    "                        for desc in old_annot.description:\n",
    "                            new_desc = event_mapping.get(desc, desc)  # Use mapping or keep original\n",
    "                            new_descriptions.append(new_desc)\n",
    "                        \n",
    "                        # Create new annotations\n",
    "                        new_annot = mne.Annotations(\n",
    "                            onset=old_annot.onset, \n",
    "                            duration=old_annot.duration,\n",
    "                            description=new_descriptions,\n",
    "                            orig_time=old_annot.orig_time\n",
    "                        )\n",
    "                        \n",
    "                        # Apply new annotations to the raw object\n",
    "                        mne_obj.set_annotations(new_annot)\n",
    "                        print(f\"  ✓ Renamed events: {list(set(new_descriptions))}\")\n",
    "                    \n",
    "                    # Store metadata in the 'temp' dictionary\n",
    "                    if 'temp' not in mne_obj.info:\n",
    "                        mne_obj.info['temp'] = {}\n",
    "                    mne_obj.info['temp']['animal_id'] = animal_id\n",
    "                    mne_obj.info['temp']['file_path'] = file\n",
    "                    \n",
    "                    # Add to our list\n",
    "                    mne_objects.append(mne_obj)\n",
    "                    print(f\"  ✓ Successfully loaded\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  ✗ Error loading: {str(e)}\")\n",
    "        \n",
    "        print(f\"\\nTotal MNE objects loaded: {len(mne_objects)}\")\n",
    "        return mne_objects\n",
    "\n",
    "    def get_objects_by_group(self, group_key: str) -> Dict[str, List[mne.io.Raw]]:\n",
    "        \"\"\"\n",
    "        Group MNE objects by a metadata key\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        group_key : str\n",
    "            Metadata key to group by (e.g. 'animal_id')\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, List[mne.io.Raw]]\n",
    "            Dictionary mapping group values to lists of MNE objects\n",
    "        \"\"\"\n",
    "        groups = {}\n",
    "        for meta in self.metadata:\n",
    "            value = meta.get(group_key, 'Unknown')\n",
    "            if value not in groups:\n",
    "                groups[value] = []\n",
    "            groups[value].append(meta['mne_obj'])\n",
    "        return groups\n",
    "\n",
    "    def create_epochs(self, tmin: float = -10, tmax: float = 5, tcrop=(-3, 3),\n",
    "                  baseline: Tuple[Optional[float], Optional[float]] = (None, -5),\n",
    "                  event_id: Optional[Dict[str, int]] = None) ->  Dict[str, List[mne.Epochs]]:\n",
    "        \"\"\"\n",
    "        Create epochs for all MNE objects\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        tmin : float\n",
    "            Start time before event\n",
    "        tmax : float\n",
    "            End time after event\n",
    "        baseline : tuple\n",
    "            Baseline correction period\n",
    "        event_id : dict\n",
    "            Event IDs to include\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, List[mne.Epochs]]\n",
    "            Dictionary mapping animal_ids to lists of Epochs objects\n",
    "        \"\"\"\n",
    "        grouped_epochs = {}\n",
    "        \n",
    "        for meta in self.metadata:\n",
    "            animal_id = meta['animal_id']\n",
    "            raw = meta['mne_obj']\n",
    "            \n",
    "            try:\n",
    "                # Extract events from annotations\n",
    "                events, event_dict = mne.events_from_annotations(raw)\n",
    "                \n",
    "                # Use provided event_id if given, otherwise use all events\n",
    "                use_event_id = event_id if event_id else event_dict\n",
    "                \n",
    "                # Create epochs\n",
    "                epochs = mne.Epochs(\n",
    "                    raw, \n",
    "                    events, \n",
    "                    event_id=use_event_id,\n",
    "                    tmin=tmin, \n",
    "                    tmax=tmax, \n",
    "                    baseline=baseline,\n",
    "                    preload=True\n",
    "                )\n",
    "\n",
    "                epochs = self.random_subsample_epochs(epochs).load_data()\n",
    "                epochs.crop(tmin=tcrop[0], tmax=tcrop[1])\n",
    "                \n",
    "                if animal_id not in grouped_epochs:\n",
    "                    grouped_epochs[animal_id] = []\n",
    "                \n",
    "                grouped_epochs[animal_id].append(epochs)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Error creating epochs for {animal_id}: {str(e)}\")\n",
    "        \n",
    "        return grouped_epochs\n",
    "    \n",
    "    def display_event_counts(self):\n",
    "        \"\"\"\n",
    "        Display the number of each type of event in each MNE object.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame containing event counts for each object\n",
    "        \"\"\"\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import mne\n",
    "        \n",
    "        # Create a list to store results\n",
    "        results = []\n",
    "        \n",
    "        # Process each MNE object - use self.mne_objects instead of mne_objects\n",
    "        for i, raw in enumerate(self.mne_objects):\n",
    "            try:\n",
    "                # Get metadata\n",
    "                animal_id = raw.info['temp'].get('animal_id', 'Unknown')\n",
    "                file_path = raw.info['temp'].get('file_path', 'Unknown')\n",
    "                \n",
    "                # Extract events from annotations\n",
    "                events, event_id = mne.events_from_annotations(raw)\n",
    "                \n",
    "                # Count occurrences of each event type\n",
    "                event_types = events[:, 2]\n",
    "                unique_events, counts = np.unique(event_types, return_counts=True)\n",
    "                \n",
    "                # Create a dictionary with basic info\n",
    "                obj_info = {\n",
    "                    'Object Index': i,\n",
    "                    'Animal ID': animal_id,\n",
    "                    'File': file_path,\n",
    "                    'Duration (s)': raw.times[-1],\n",
    "                    'Total Events': len(events)\n",
    "                }\n",
    "                \n",
    "                # Add event counts to the dictionary\n",
    "                for event_code, count in zip(unique_events, counts):\n",
    "                    # Get event name\n",
    "                    event_name = \"Unknown\"\n",
    "                    for name, code in event_id.items():\n",
    "                        if code == event_code:\n",
    "                            event_name = name\n",
    "                            break\n",
    "                    \n",
    "                    obj_info[f'{event_name} ({event_code})'] = count\n",
    "                \n",
    "                results.append(obj_info)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing object {i}: {str(e)}\")\n",
    "        \n",
    "        # Create DataFrame from results\n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        # Display summary\n",
    "        print(f\"Event summary for {len(self.mne_objects)} MNE objects:\")\n",
    "        print(f\"Total unique event types: {len(df.columns) - 5}\")  # Subtract the 5 basic info columns\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def plot_evoked_by_group(self, group_key: str = 'animal_id', \n",
    "                        tmin: float = -0.2, tmax: float = 0.5,\n",
    "                        baseline: Tuple[Optional[float], Optional[float]] = (None, 0),\n",
    "                        event_id: Optional[Dict[str, int]] = None,\n",
    "                        picks: Optional[List[str]] = None,\n",
    "                        combine: str = 'mean',\n",
    "                        event_repeated: str = 'drop',\n",
    "                        title: Optional[str] = None,\n",
    "                        figsize: Optional[Tuple[float, float]] = None,\n",
    "                        match_channels_to_events: bool = True) -> Tuple[plt.Figure, List[plt.Axes]]:\n",
    "        \"\"\"\n",
    "        Plot evoked responses grouped by a metadata key\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        group_key : str\n",
    "            Metadata key to group by (e.g. 'animal_id')\n",
    "        tmin : float\n",
    "            Start time before event\n",
    "        tmax : float\n",
    "            End time after event\n",
    "        baseline : tuple\n",
    "            Baseline correction period\n",
    "        event_id : dict\n",
    "            Event IDs to include\n",
    "        picks : list\n",
    "            Channels to include\n",
    "        combine : str\n",
    "            How to combine channels ('mean', 'median', etc.)\n",
    "        event_repeated : str\n",
    "            How to handle repeated events: 'error', 'drop', or 'merge'\n",
    "        title : str\n",
    "            Figure title\n",
    "        figsize : tuple\n",
    "            Figure size\n",
    "        match_channels_to_events : bool\n",
    "            If True, match channels to events with the same name and plot each channel's\n",
    "            response to its corresponding event across groups\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[plt.Figure, List[plt.Axes]]\n",
    "            Figure and axes objects\n",
    "        \"\"\"\n",
    "        # Get objects grouped by the specified key\n",
    "        grouped_objects = self.get_objects_by_group(group_key)\n",
    "        \n",
    "        if match_channels_to_events:\n",
    "            # First, collect all channel names across all objects\n",
    "            all_channel_names = set()\n",
    "            for objects in grouped_objects.values():\n",
    "                for raw in objects:\n",
    "                    all_channel_names.update(raw.ch_names)\n",
    "            \n",
    "            # Filter to only include channels in picks if specified\n",
    "            if picks:\n",
    "                all_channel_names = [ch for ch in all_channel_names if ch in picks]\n",
    "            else:\n",
    "                all_channel_names = list(all_channel_names)\n",
    "            \n",
    "            # Sort channel names for consistent display\n",
    "            all_channel_names.sort()\n",
    "            \n",
    "            # Create figure with one subplot per channel\n",
    "            if figsize is None:\n",
    "                figsize = (12, 3 * len(all_channel_names))\n",
    "            fig, axes = plt.subplots(len(all_channel_names), 1, figsize=figsize, sharex=True)\n",
    "            \n",
    "            # Ensure axes is always a list for consistent indexing\n",
    "            if len(all_channel_names) == 1:\n",
    "                axes = [axes]\n",
    "            \n",
    "            # Process each channel\n",
    "            for ch_idx, channel_name in enumerate(all_channel_names):\n",
    "                ax = axes[ch_idx]\n",
    "                \n",
    "                # Set channel as title for the subplot\n",
    "                ax.set_title(f\"Channel: {channel_name}\")\n",
    "                \n",
    "                # Dictionary to store evoked objects by group\n",
    "                evoked_by_group = {}\n",
    "                \n",
    "                # Process each group\n",
    "                for group_value, objects in grouped_objects.items():\n",
    "                    # List to store evoked objects for this group and channel\n",
    "                    group_evoked_list = []\n",
    "                    \n",
    "                    # Process each object in the group\n",
    "                    for raw in objects:\n",
    "                        try:\n",
    "                            # Skip if channel is not in this raw object\n",
    "                            if channel_name not in raw.ch_names:\n",
    "                                continue\n",
    "                                \n",
    "                            # Extract events from annotations\n",
    "                            events, event_dict = mne.events_from_annotations(raw)\n",
    "                            \n",
    "                            # Find the event ID that matches the channel name\n",
    "                            matching_event_id = {}\n",
    "                            for event_name, event_code in event_dict.items():\n",
    "                                if channel_name in event_name:\n",
    "                                    matching_event_id = {event_name: event_code}\n",
    "                                    break\n",
    "                            \n",
    "                            # Skip if no matching event found\n",
    "                            if not matching_event_id:\n",
    "                                continue\n",
    "                            \n",
    "                            # Create epochs with only the matching event and channel\n",
    "                            epochs = mne.Epochs(\n",
    "                                raw, \n",
    "                                events, \n",
    "                                event_id=matching_event_id,\n",
    "                                tmin=tmin, \n",
    "                                tmax=tmax, \n",
    "                                baseline=baseline,\n",
    "                                preload=True,\n",
    "                                picks=[channel_name],  # Only select the matching channel\n",
    "                                event_repeated=event_repeated\n",
    "                            )\n",
    "                            \n",
    "                            # Skip if no valid epochs\n",
    "                            if len(epochs) == 0:\n",
    "                                continue\n",
    "                                \n",
    "                            # Compute evoked response\n",
    "                            evoked = epochs.average()\n",
    "                            group_evoked_list.append(evoked)\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            logging.warning(f\"Error processing {group_value}, channel {channel_name}: {str(e)}\")\n",
    "                    \n",
    "                    # Average evoked responses for this group and channel\n",
    "                    if group_evoked_list:\n",
    "                        avg_evoked = mne.combine_evoked(group_evoked_list, weights='equal')\n",
    "                        evoked_by_group[group_value] = avg_evoked\n",
    "                \n",
    "                # Plot evoked responses for each group on this channel's subplot\n",
    "                if evoked_by_group:\n",
    "                    colors = plt.cm.tab10.colors  # Use a colormap for different groups\n",
    "                    for i, (group_value, evoked) in enumerate(evoked_by_group.items()):\n",
    "                        color = colors[i % len(colors)]\n",
    "                        times = evoked.times\n",
    "                        data = evoked.get_data(picks=0).flatten()  # Get data for the single channel\n",
    "                        ax.plot(times, data, label=f\"{group_key}: {group_value}\", color=color)\n",
    "                    \n",
    "                    ax.axvline(x=0, color='k', linestyle='--', alpha=0.5)  # Mark time=0\n",
    "                    ax.set_ylabel('Amplitude')\n",
    "                    ax.grid(True)\n",
    "                    ax.legend()\n",
    "                else:\n",
    "                    ax.text(0.5, 0.5, f\"No data for channel {channel_name}\", \n",
    "                        ha='center', va='center', transform=ax.transAxes)\n",
    "            \n",
    "            # Set overall title and x-label\n",
    "            if title:\n",
    "                fig.suptitle(title, fontsize=16)\n",
    "            axes[-1].set_xlabel('Time (s)')\n",
    "            \n",
    "        else:\n",
    "            # Original implementation for non-matched case\n",
    "            if figsize is None:\n",
    "                figsize = (10, 8 * len(grouped_objects))\n",
    "            fig, axes = plt.subplots(len(grouped_objects), 1, figsize=figsize, sharex=True)\n",
    "            if len(grouped_objects) == 1:\n",
    "                axes = [axes]\n",
    "            \n",
    "            # Process each group\n",
    "            for ax_idx, (group_value, objects) in enumerate(grouped_objects.items()):\n",
    "                ax = axes[ax_idx]\n",
    "                \n",
    "                # List to store evoked objects\n",
    "                evoked_list = []\n",
    "                \n",
    "                # Process each object in the group\n",
    "                for raw in objects:\n",
    "                    try:\n",
    "                        # Extract events\n",
    "                        events, event_dict = mne.events_from_annotations(raw)\n",
    "                        \n",
    "                        # Use provided event_id if given, otherwise use all events\n",
    "                        use_event_id = event_id if event_id else event_dict\n",
    "                        \n",
    "                        # Create epochs with channel selection\n",
    "                        epochs = mne.Epochs(\n",
    "                            raw, \n",
    "                            events, \n",
    "                            event_id=use_event_id,\n",
    "                            tmin=tmin, \n",
    "                            tmax=tmax, \n",
    "                            baseline=baseline,\n",
    "                            preload=True,\n",
    "                            picks=picks,\n",
    "                            event_repeated=event_repeated\n",
    "                        )\n",
    "                        \n",
    "                        # Get evoked response\n",
    "                        if len(epochs) > 0:\n",
    "                            evoked = epochs.average()\n",
    "                            evoked_list.append(evoked)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logging.warning(f\"Error processing {group_value}: {str(e)}\")\n",
    "                \n",
    "                # Plot evoked responses\n",
    "                if evoked_list:\n",
    "                    # Average across all evoked objects in this group\n",
    "                    all_evoked = mne.combine_evoked(evoked_list, weights='equal')\n",
    "                    \n",
    "                    # Plot the data as regular time series\n",
    "                    all_evoked.plot(axes=ax, show=False, titles=None, \n",
    "                                time_unit='s', gfp=True, \n",
    "                                spatial_colors=False)\n",
    "                    \n",
    "                    # Add group title\n",
    "                    ax.set_title(f\"{group_key}: {group_value}\")\n",
    "                else:\n",
    "                    ax.text(0.5, 0.5, f\"No valid data for {group_value}\", \n",
    "                        ha='center', va='center', transform=ax.transAxes)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig, axes\n",
    "\n",
    "    def compute_tfr(self, epochs: mne.Epochs, freqs: np.ndarray, \n",
    "                   n_cycles: Union[float, List[float], np.ndarray] = None,\n",
    "                   picks: Optional[List[str]] = None,\n",
    "                   return_itc: bool = False) -> mne.time_frequency.AverageTFR:\n",
    "        \"\"\"\n",
    "        Compute TFR for epochs using Morlet wavelets\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        epochs : mne.Epochs\n",
    "            Epochs object\n",
    "        freqs : array\n",
    "            Frequencies of interest\n",
    "        n_cycles : float or array\n",
    "            Number of cycles in Morlet wavelet\n",
    "        picks : list\n",
    "            Channels to include\n",
    "        return_itc : bool\n",
    "            Whether to return inter-trial coherence\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        mne.time_frequency.AverageTFR\n",
    "            TFR object\n",
    "        \"\"\"\n",
    "        # Set default n_cycles if not provided\n",
    "        if n_cycles is None:\n",
    "            n_cycles = freqs / 2\n",
    "        \n",
    "        # Compute TFR\n",
    "        power, itc = tfr_morlet(epochs, freqs=freqs, n_cycles=n_cycles, \n",
    "                               use_fft=True, return_itc=True, picks=picks)\n",
    "        \n",
    "        return itc if return_itc else power\n",
    "\n",
    "    def plot_tfr_contrast(self, group_key: str = 'animal_id',\n",
    "                        tmin: float = -0.5, tmax: float = 1.0,\n",
    "                        baseline: Tuple[Optional[float], Optional[float]] = (None, 0),\n",
    "                        event_id: Optional[Dict[str, int]] = None,\n",
    "                        freqs: np.ndarray = np.linspace(4, 40, 20),\n",
    "                        n_cycles: Optional[Union[float, List[float], np.ndarray]] = None,\n",
    "                        picks: Optional[List[str]] = None,\n",
    "                        event_repeated: str = 'drop',\n",
    "                        mode: str = 'zscore',\n",
    "                        title: Optional[str] = None,\n",
    "                        figsize: Optional[Tuple[float, float]] = None) -> Tuple[plt.Figure, List[plt.Axes]]:\n",
    "        \"\"\"\n",
    "        Plot TFR contrast between groups\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        group_key : str\n",
    "            Metadata key to group by (e.g. 'animal_id')\n",
    "        tmin : float\n",
    "            Start time before event\n",
    "        tmax : float\n",
    "            End time after event\n",
    "        baseline : tuple\n",
    "            Baseline correction period (not used for 'natlog' or 'log10' modes)\n",
    "        event_id : dict\n",
    "            Event IDs to include\n",
    "        freqs : array\n",
    "            Frequencies of interest\n",
    "        n_cycles : float or array\n",
    "            Number of cycles in Morlet wavelet\n",
    "        picks : list\n",
    "            Channels to include\n",
    "        event_repeated : str\n",
    "            How to handle repeated events: 'error', 'drop', or 'merge'\n",
    "        mode : str\n",
    "            Mode for data transformation:\n",
    "            - 'zscore', 'mean', 'ratio', 'logratio', 'percent': Standard baseline correction modes\n",
    "            - 'natlog': Apply natural logarithm to data without baseline correction\n",
    "            - 'log10': Apply base-10 logarithm to data without baseline correction\n",
    "        title : str\n",
    "            Figure title\n",
    "        figsize : tuple\n",
    "            Figure size\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[plt.Figure, List[plt.Axes]]\n",
    "            Figure and axes objects\n",
    "        \"\"\"\n",
    "        # Get objects grouped by the specified key\n",
    "        grouped_objects = self.get_objects_by_group(group_key)\n",
    "        groups = list(grouped_objects.keys())\n",
    "        \n",
    "        # We need at least 2 groups for contrast\n",
    "        if len(groups) < 2:\n",
    "            raise ValueError(f\"Need at least 2 groups for contrast, but only found: {groups}\")\n",
    "        \n",
    "        # Get channel list to use - intersection of channels in all objects\n",
    "        all_channels = [obj.ch_names for group in grouped_objects.values() for obj in group]\n",
    "        common_channels = set(all_channels[0])\n",
    "        for ch_list in all_channels[1:]:\n",
    "            common_channels &= set(ch_list)\n",
    "        common_channels = sorted(list(common_channels))\n",
    "        \n",
    "        if picks:\n",
    "            common_channels = [ch for ch in common_channels if ch in picks]\n",
    "        \n",
    "        if not common_channels:\n",
    "            raise ValueError(\"No common channels found across all objects\")\n",
    "        \n",
    "        logging.info(f\"Using common channels: {common_channels}\")\n",
    "        \n",
    "        # Dictionary to store TFR results for each group\n",
    "        tfr_by_group = {}\n",
    "        # Dictionary to store event counts for each group\n",
    "        event_counts = {}\n",
    "        \n",
    "        # Process each group\n",
    "        for group, objects in grouped_objects.items():\n",
    "            # List to store TFR objects\n",
    "            tfr_list = []\n",
    "            # Counter for total events in this group\n",
    "            total_events = 0\n",
    "            \n",
    "            # Process each object in the group\n",
    "            for raw in objects:\n",
    "                try:\n",
    "                    # Extract events\n",
    "                    events, event_dict = mne.events_from_annotations(raw)\n",
    "                    \n",
    "                    # Use provided event_id if given, otherwise use all events\n",
    "                    use_event_id = event_id if event_id else event_dict\n",
    "                    \n",
    "                    # Create epochs\n",
    "                    epochs = mne.Epochs(\n",
    "                        raw, \n",
    "                        events, \n",
    "                        event_id=use_event_id,\n",
    "                        tmin=tmin, \n",
    "                        tmax=tmax, \n",
    "                        baseline=None,  # Always use None here, we'll apply baseline later if needed\n",
    "                        preload=True,\n",
    "                        picks=common_channels,\n",
    "                        event_repeated=event_repeated\n",
    "                    )\n",
    "                    \n",
    "                    # Count events in this epoch\n",
    "                    if len(epochs) > 0:\n",
    "                        # Add to the total event count for this group\n",
    "                        total_events += len(epochs)\n",
    "                        \n",
    "                        # Compute TFR\n",
    "                        tfr = self.compute_tfr(epochs, freqs, n_cycles, picks=common_channels)\n",
    "                        tfr_list.append(tfr)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Error processing {group}: {str(e)}\")\n",
    "            \n",
    "            # Store the total event count for this group\n",
    "            event_counts[group] = total_events\n",
    "            \n",
    "            # Average TFRs for this group\n",
    "            if tfr_list:\n",
    "                # Create a grand average TFR\n",
    "                tfr_avg = tfr_list[0].copy()\n",
    "                if len(tfr_list) > 1:\n",
    "                    for tfr in tfr_list[1:]:\n",
    "                        tfr_avg._data += tfr._data\n",
    "                    tfr_avg._data /= len(tfr_list)\n",
    "                \n",
    "                # Apply transformation based on mode\n",
    "                if mode == 'natlog':\n",
    "                    # Apply natural logarithm\n",
    "                    # Add a small constant to avoid log(0)\n",
    "                    tfr_avg._data = np.log(tfr_avg._data + 1e-10)\n",
    "                    logging.info(f\"Applied natural logarithm (natlog) to {group} data\")\n",
    "                elif mode == 'log10':\n",
    "                    # Apply base-10 logarithm\n",
    "                    # Add a small constant to avoid log10(0)\n",
    "                    tfr_avg._data = np.log10(tfr_avg._data + 1e-10)\n",
    "                    logging.info(f\"Applied base-10 logarithm (log10) to {group} data\")\n",
    "                else:\n",
    "                    # Apply standard baseline correction\n",
    "                    tfr_avg.apply_baseline(baseline=baseline, mode=mode)\n",
    "                    logging.info(f\"Applied baseline correction with mode={mode} to {group} data\")\n",
    "                \n",
    "                # Store result\n",
    "                tfr_by_group[group] = tfr_avg\n",
    "        \n",
    "        # Special case: If exactly 2 groups, show each group's TFR and then the contrast\n",
    "        if len(groups) == 2 and groups[0] in tfr_by_group and groups[1] in tfr_by_group:\n",
    "            # Determine subplot layout\n",
    "            n_channels = len(common_channels)\n",
    "            n_columns = 3  # Two groups + contrast\n",
    "            \n",
    "            # Create figure\n",
    "            if figsize is None:\n",
    "                figsize = (4 * n_columns, 3 * n_channels)\n",
    "            \n",
    "            fig, axes = plt.subplots(n_channels, n_columns, figsize=figsize)\n",
    "            \n",
    "            # Handle case of single channel\n",
    "            if n_channels == 1:\n",
    "                axes = axes.reshape(1, -1)\n",
    "            \n",
    "            # Get max value across all TFRs for consistent colormap scaling\n",
    "            max_val = 0\n",
    "            for group in groups:\n",
    "                curr_max = np.max(np.abs(tfr_by_group[group].data))\n",
    "                if curr_max > max_val:\n",
    "                    max_val = curr_max\n",
    "            \n",
    "            # Plot each group's TFR first\n",
    "            for col_idx, group in enumerate(groups):\n",
    "                tfr = tfr_by_group[group]\n",
    "                \n",
    "                for ch_idx, ch_name in enumerate(common_channels):\n",
    "                    ax = axes[ch_idx, col_idx]\n",
    "                    \n",
    "                    # Extract data for this channel\n",
    "                    ch_idx_in_tfr = tfr.ch_names.index(ch_name)\n",
    "                    data = tfr.data[ch_idx_in_tfr]\n",
    "                    \n",
    "                    # Plot TFR with consistent scale\n",
    "                    im = ax.pcolormesh(tfr.times, tfr.freqs, data,\n",
    "                                    cmap='RdBu_r', vmin=-max_val, vmax=max_val)\n",
    "                    \n",
    "                    # Set labels\n",
    "                    if ch_idx == 0:  # Top row\n",
    "                        events_count = event_counts.get(group, 0)\n",
    "                        transform_str = f\" ({mode})\" if mode in ['natlog', 'log10'] else \"\"\n",
    "                        ax.set_title(f\"{group} (n={events_count}){transform_str}\")\n",
    "                    \n",
    "                    if col_idx == 0:  # Leftmost column\n",
    "                        ax.set_ylabel(f\"{ch_name}\\nFreq (Hz)\")\n",
    "                    \n",
    "                    if ch_idx == n_channels - 1:  # Bottom row\n",
    "                        ax.set_xlabel(\"Time (s)\")\n",
    "            \n",
    "            # Now plot the contrast in the last column\n",
    "            group1, group2 = groups\n",
    "            contrast = tfr_by_group[group1].copy()\n",
    "            contrast._data -= tfr_by_group[group2]._data\n",
    "            \n",
    "            # Get max value for contrast\n",
    "            contrast_max = np.max(np.abs(contrast.data))\n",
    "            \n",
    "            for ch_idx, ch_name in enumerate(common_channels):\n",
    "                ax = axes[ch_idx, 2]  # Last column\n",
    "                \n",
    "                # Extract data for this channel\n",
    "                ch_idx_in_tfr = contrast.ch_names.index(ch_name)\n",
    "                data = contrast.data[ch_idx_in_tfr]\n",
    "                \n",
    "                # Plot TFR\n",
    "                im = ax.pcolormesh(contrast.times, contrast.freqs, data,\n",
    "                                cmap='RdBu_r', vmin=-contrast_max, vmax=contrast_max)\n",
    "                \n",
    "                # Add colorbar for the last column\n",
    "                plt.colorbar(im, ax=ax)\n",
    "                \n",
    "                # Set labels\n",
    "                if ch_idx == 0:  # Top row\n",
    "                    events1 = event_counts.get(group1, 0)\n",
    "                    events2 = event_counts.get(group2, 0)\n",
    "                    transform_str = f\" ({mode})\" if mode in ['natlog', 'log10'] else \"\"\n",
    "                    ax.set_title(f\"{group1} (n={events1}) - {group2} (n={events2}){transform_str}\")\n",
    "                \n",
    "                if ch_idx == n_channels - 1:  # Bottom row\n",
    "                    ax.set_xlabel(\"Time (s)\")\n",
    "        \n",
    "        else:\n",
    "            # Original implementation for more than 2 groups\n",
    "            # Create pairwise contrasts\n",
    "            contrasts = []\n",
    "            group_pairs = []\n",
    "            for i, group1 in enumerate(groups):\n",
    "                for group2 in groups[i+1:]:\n",
    "                    if group1 in tfr_by_group and group2 in tfr_by_group:\n",
    "                        # Create a contrast TFR\n",
    "                        contrast = tfr_by_group[group1].copy()\n",
    "                        contrast._data -= tfr_by_group[group2]._data\n",
    "                        \n",
    "                        contrasts.append(contrast)\n",
    "                        group_pairs.append((group1, group2))\n",
    "            \n",
    "            # Determine subplot layout\n",
    "            n_channels = len(common_channels)\n",
    "            n_contrasts = len(contrasts)\n",
    "            \n",
    "            # Create figure\n",
    "            if figsize is None:\n",
    "                figsize = (4 * n_contrasts, 3 * n_channels)\n",
    "            \n",
    "            fig, axes = plt.subplots(n_channels, n_contrasts, figsize=figsize)\n",
    "            \n",
    "            # Handle case of single channel or contrast\n",
    "            if n_channels == 1 and n_contrasts == 1:\n",
    "                axes = np.array([[axes]])\n",
    "            elif n_channels == 1:\n",
    "                axes = axes.reshape(1, -1)\n",
    "            elif n_contrasts == 1:\n",
    "                axes = axes.reshape(-1, 1)\n",
    "            \n",
    "            # Plot each contrast for each channel\n",
    "            for contrast_idx, (contrast, group_pair) in enumerate(zip(contrasts, group_pairs)):\n",
    "                for ch_idx, ch_name in enumerate(common_channels):\n",
    "                    ax = axes[ch_idx, contrast_idx]\n",
    "                    \n",
    "                    # Extract data for this channel\n",
    "                    ch_idx_in_tfr = contrast.ch_names.index(ch_name)\n",
    "                    data = contrast.data[ch_idx_in_tfr]\n",
    "                    \n",
    "                    # Plot TFR\n",
    "                    im = ax.pcolormesh(contrast.times, contrast.freqs, data,\n",
    "                                    cmap='RdBu_r', vmin=-np.max(np.abs(data)), vmax=np.max(np.abs(data)))\n",
    "                    \n",
    "                    # Add colorbar for rightmost column\n",
    "                    if contrast_idx == n_contrasts - 1:\n",
    "                        plt.colorbar(im, ax=ax)\n",
    "                    \n",
    "                    # Set labels\n",
    "                    if ch_idx == 0:  # Top row\n",
    "                        # Include event counts in the title\n",
    "                        group1, group2 = group_pair\n",
    "                        events1 = event_counts.get(group1, 0)\n",
    "                        events2 = event_counts.get(group2, 0)\n",
    "                        transform_str = f\" ({mode})\" if mode in ['natlog', 'log10'] else \"\"\n",
    "                        ax.set_title(f\"{group1} (n={events1}) - {group2} (n={events2}){transform_str}\")\n",
    "                    \n",
    "                    if contrast_idx == 0:  # Leftmost column\n",
    "                        ax.set_ylabel(f\"{ch_name}\\nFreq (Hz)\")\n",
    "                    \n",
    "                    if ch_idx == n_channels - 1:  # Bottom row\n",
    "                        ax.set_xlabel(\"Time (s)\")\n",
    "        \n",
    "        # Set overall title\n",
    "        if title:\n",
    "            fig.suptitle(title, fontsize=16)\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 files for A5:\n",
      "  - ./test-mnes/A5 WT Dec-12-2023\n",
      "Opening raw data file test-mnes/A5 WT Dec-12-2023/A5-WT-A5 WT Dec-12-2023-raw.fif...\n",
      "    Reading extended channel information\n",
      "Isotrak not found\n",
      "    Range : 0 ... 40067758 =      0.000 ... 40067.758 secs\n",
      "Ready.\n",
      "  ✓ Downsampled from 1000.0 Hz to 125 Hz\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 50 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 50.00 Hz\n",
      "- Upper transition bandwidth: 12.50 Hz (-6 dB cutoff frequency: 56.25 Hz)\n",
      "- Filter length: 413 samples (3.304 s)\n",
      "\n",
      "  ✓ Applied bandpass filter: 1-50 Hz\n",
      "  ✓ Renamed channels: ['LAud', 'LVis', 'LHip', 'LBar', 'LMot', 'RMot', 'RBar', 'RHip', 'RVis', 'RAud']\n",
      "Used Annotations descriptions: ['Intan Input (1)/PortB L Aud Ctx', 'Intan Input (1)/PortB L Barrel', 'Intan Input (1)/PortB L Hipp', 'Intan Input (1)/PortB L Motor', 'Intan Input (1)/PortB L Vis Ctx', 'Intan Input (1)/PortB R Aud Ctx', 'Intan Input (1)/PortB R Barrel', 'Intan Input (1)/PortB R Hipp', 'Intan Input (1)/PortB R Motor', 'Intan Input (1)/PortB R Vis Ctx']\n",
      "  ✓ Renamed events: ['RMot', 'LHip', 'LBar', 'LAud', 'RBar', 'RAud', 'RVis', 'LMot', 'LVis', 'RHip']\n",
      "  ✓ Successfully loaded\n",
      "  - ./test-mnes/A5 WT Dec-14-2023\n",
      "Opening raw data file test-mnes/A5 WT Dec-14-2023/A5-WT-A5 WT Dec-14-2023-raw.fif...\n",
      "    Reading extended channel information\n",
      "Isotrak not found\n",
      "    Range : 0 ... 38893319 =      0.000 ... 38893.319 secs\n",
      "Ready.\n",
      "  ✓ Downsampled from 1000.0 Hz to 125 Hz\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 50 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 50.00 Hz\n",
      "- Upper transition bandwidth: 12.50 Hz (-6 dB cutoff frequency: 56.25 Hz)\n",
      "- Filter length: 413 samples (3.304 s)\n",
      "\n",
      "  ✓ Applied bandpass filter: 1-50 Hz\n",
      "  ✓ Renamed channels: ['LAud', 'LVis', 'LHip', 'LBar', 'LMot', 'RMot', 'RBar', 'RHip', 'RVis', 'RAud']\n",
      "Used Annotations descriptions: ['Intan Input (1)/PortB L Aud Ctx', 'Intan Input (1)/PortB L Barrel', 'Intan Input (1)/PortB L Hipp', 'Intan Input (1)/PortB L Motor', 'Intan Input (1)/PortB L Vis Ctx', 'Intan Input (1)/PortB R Aud Ctx', 'Intan Input (1)/PortB R Barrel', 'Intan Input (1)/PortB R Hipp', 'Intan Input (1)/PortB R Motor', 'Intan Input (1)/PortB R Vis Ctx']\n",
      "  ✓ Renamed events: ['RMot', 'LHip', 'RHip', 'LAud', 'RBar', 'RAud', 'RVis', 'LMot', 'LVis', 'LBar']\n",
      "  ✓ Successfully loaded\n",
      "Found 2 files for A10:\n",
      "  - ./test-mnes/A10 KO Dec-14-2023\n",
      "Opening raw data file test-mnes/A10 KO Dec-14-2023/A10-KO-A10 KO Dec-14-2023-raw-1.fif...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ohy2/Documents/GitHubRepo/PyEEG/pythoneeg/visualization/results.py:854: RuntimeWarning: This filename (test-mnes/A10 KO Dec-14-2023/A10-KO-A10 KO Dec-14-2023-raw-1.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, _meg.fif, _eeg.fif, _ieeg.fif, raw.fif.gz, raw_sss.fif.gz, raw_tsss.fif.gz, _meg.fif.gz, _eeg.fif.gz or _ieeg.fif.gz\n",
      "  data['result_mne'] = mne.io.read_raw_fif(fif_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Reading extended channel information\n",
      "Isotrak not found\n",
      "    Range : 53537000 ... 83896319 =  53537.000 ... 83896.319 secs\n",
      "Ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Intan Input (1)/PortC C-009 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-010 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-012 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-014 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-015 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-016 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-017 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-019 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-021 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-022 does not match name aliases. Assuming alias from number in channel name.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Downsampled from 1000.0 Hz to 125 Hz\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 50 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 50.00 Hz\n",
      "- Upper transition bandwidth: 12.50 Hz (-6 dB cutoff frequency: 56.25 Hz)\n",
      "- Filter length: 413 samples (3.304 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Intan Input (1)/PortC C-009 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-010 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-012 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-014 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-015 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-016 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-017 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-019 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-021 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-022 does not match name aliases. Assuming alias from number in channel name.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Applied bandpass filter: 1-50 Hz\n",
      "  ✓ Renamed channels: ['LAud', 'LVis', 'LHip', 'LBar', 'LMot', 'RMot', 'RBar', 'RHip', 'RVis', 'RAud']\n",
      "Used Annotations descriptions: ['Intan Input (1)/PortC C-009', 'Intan Input (1)/PortC C-010', 'Intan Input (1)/PortC C-012', 'Intan Input (1)/PortC C-014', 'Intan Input (1)/PortC C-015', 'Intan Input (1)/PortC C-016', 'Intan Input (1)/PortC C-017', 'Intan Input (1)/PortC C-019', 'Intan Input (1)/PortC C-021', 'Intan Input (1)/PortC C-022']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Intan Input (1)/PortC C-009 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-010 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-012 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-014 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-015 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-016 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-017 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-019 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-021 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-022 does not match name aliases. Assuming alias from number in channel name.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Renamed events: ['LHip', 'RMot', 'LBar', 'LAud', 'RBar', 'RAud', 'RVis', 'LMot', 'LVis', 'RHip']\n",
      "  ✓ Successfully loaded\n",
      "  - ./test-mnes/A10 KO Dec-13-2023\n",
      "Opening raw data file test-mnes/A10 KO Dec-13-2023/A10-KO-A10 KO Dec-13-2023-raw.fif...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gz/c4v7tdb972v1l3clm5lpwwyszxhlcz/T/ipykernel_76537/3895829044.py:202: RuntimeWarning: Omitted 40726 annotation(s) that were outside data range.\n",
      "  mne_obj.set_annotations(new_annot)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Reading extended channel information\n",
      "Isotrak not found\n",
      "    Range : 0 ... 120359 =      0.000 ...   120.359 secs\n",
      "Ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Intan Input (1)/PortC C-009 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-010 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-012 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-014 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-015 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-016 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-017 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-019 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-021 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-022 does not match name aliases. Assuming alias from number in channel name.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Downsampled from 1000.0 Hz to 125 Hz\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 50 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 50.00 Hz\n",
      "- Upper transition bandwidth: 12.50 Hz (-6 dB cutoff frequency: 56.25 Hz)\n",
      "- Filter length: 413 samples (3.304 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Intan Input (1)/PortC C-009 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-010 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-012 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-014 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-015 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-016 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-017 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-019 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-021 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-022 does not match name aliases. Assuming alias from number in channel name.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Applied bandpass filter: 1-50 Hz\n",
      "  ✓ Renamed channels: ['LAud', 'LVis', 'LHip', 'LBar', 'LMot', 'RMot', 'RBar', 'RHip', 'RVis', 'RAud']\n",
      "Used Annotations descriptions: ['Intan Input (1)/PortC C-009', 'Intan Input (1)/PortC C-010', 'Intan Input (1)/PortC C-014', 'Intan Input (1)/PortC C-016', 'Intan Input (1)/PortC C-017']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Intan Input (1)/PortC C-009 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-010 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-014 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-016 does not match name aliases. Assuming alias from number in channel name.\n",
      "WARNING:root:Intan Input (1)/PortC C-017 does not match name aliases. Assuming alias from number in channel name.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Renamed events: ['RMot', 'LAud', 'RBar', 'LVis', 'LBar']\n",
      "  ✓ Successfully loaded\n",
      "\n",
      "Total MNE objects loaded: 4\n",
      "Used Annotations descriptions: ['LAud', 'LBar', 'LHip', 'LMot', 'LVis', 'RAud', 'RBar', 'RHip', 'RMot', 'RVis']\n",
      "Used Annotations descriptions: ['LAud', 'LBar', 'LHip', 'LMot', 'LVis', 'RAud', 'RBar', 'RHip', 'RMot', 'RVis']\n",
      "Used Annotations descriptions: ['LAud', 'LBar', 'LVis', 'RBar', 'RMot']\n",
      "Event summary for 4 MNE objects:\n",
      "Total unique event types: 13\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Object Index</th>\n",
       "      <th>Animal ID</th>\n",
       "      <th>File</th>\n",
       "      <th>Duration (s)</th>\n",
       "      <th>Total Events</th>\n",
       "      <th>LAud (1)</th>\n",
       "      <th>LBar (2)</th>\n",
       "      <th>LHip (3)</th>\n",
       "      <th>LMot (4)</th>\n",
       "      <th>LVis (5)</th>\n",
       "      <th>RAud (6)</th>\n",
       "      <th>RBar (7)</th>\n",
       "      <th>RHip (8)</th>\n",
       "      <th>RMot (9)</th>\n",
       "      <th>RVis (10)</th>\n",
       "      <th>LVis (3)</th>\n",
       "      <th>RBar (4)</th>\n",
       "      <th>RMot (5)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A5</td>\n",
       "      <td>./test-mnes/A5 WT Dec-12-2023</td>\n",
       "      <td>40067.752</td>\n",
       "      <td>22399</td>\n",
       "      <td>1791.0</td>\n",
       "      <td>2487.0</td>\n",
       "      <td>1521.0</td>\n",
       "      <td>1812.0</td>\n",
       "      <td>2122.0</td>\n",
       "      <td>1755.0</td>\n",
       "      <td>2438.0</td>\n",
       "      <td>2136.0</td>\n",
       "      <td>3032.0</td>\n",
       "      <td>3305.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A5</td>\n",
       "      <td>./test-mnes/A5 WT Dec-14-2023</td>\n",
       "      <td>38893.312</td>\n",
       "      <td>1526</td>\n",
       "      <td>489.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>375.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A10</td>\n",
       "      <td>./test-mnes/A10 KO Dec-14-2023</td>\n",
       "      <td>30359.312</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A10</td>\n",
       "      <td>./test-mnes/A10 KO Dec-13-2023</td>\n",
       "      <td>120.352</td>\n",
       "      <td>28</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Object Index Animal ID                            File  Duration (s)  \\\n",
       "0             0        A5   ./test-mnes/A5 WT Dec-12-2023     40067.752   \n",
       "1             1        A5   ./test-mnes/A5 WT Dec-14-2023     38893.312   \n",
       "2             2       A10  ./test-mnes/A10 KO Dec-14-2023     30359.312   \n",
       "3             3       A10  ./test-mnes/A10 KO Dec-13-2023       120.352   \n",
       "\n",
       "   Total Events  LAud (1)  LBar (2)  LHip (3)  LMot (4)  LVis (5)  RAud (6)  \\\n",
       "0         22399    1791.0    2487.0    1521.0    1812.0    2122.0    1755.0   \n",
       "1          1526     489.0     182.0      24.0      17.0      92.0      64.0   \n",
       "2             0       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "3            28       6.0       1.0       NaN       NaN       NaN       NaN   \n",
       "\n",
       "   RBar (7)  RHip (8)  RMot (9)  RVis (10)  LVis (3)  RBar (4)  RMot (5)  \n",
       "0    2438.0    2136.0    3032.0     3305.0       NaN       NaN       NaN  \n",
       "1     185.0      47.0      51.0      375.0       NaN       NaN       NaN  \n",
       "2       NaN       NaN       NaN        NaN       NaN       NaN       NaN  \n",
       "3       NaN       NaN       NaN        NaN       6.0       1.0      14.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load MNE objects using the static method\n",
    "animal_ids = ['A5', 'A10']\n",
    "mne_objects = MNEExperimentPlotter.load_mne_objects(animal_ids, base_folder='./test-mnes')\n",
    "\n",
    "# # After loading your MNE objects as shown in your existing code\n",
    "plotter = MNEExperimentPlotter(mne_objects)\n",
    "\n",
    "event_df = plotter.display_event_counts()\n",
    "display(event_df)\n",
    "\n",
    "# # To plot evoked responses grouped by animal ID\n",
    "# fig, axes = plotter.plot_evoked_by_group(group_key='animal_id')\n",
    "\n",
    "# # To plot TFR contrast between animals\n",
    "# fig, axes = plotter.plot_tfr_contrast(\n",
    "#     group_key='animal_id',\n",
    "#     tmin=-1, \n",
    "#     tmax=1.0,\n",
    "#     freqs=np.linspace(4, 40, 20),\n",
    "#     mode='log10'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = mne_objects[0]\n",
    "\n",
    "raw.crop(tmin = 0, tmax= 20)\n",
    "\n",
    "raw.plot(duration=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script true\n",
    "# animal_ids = ['A5', 'A10', 'F22', 'G25']\n",
    "# animal_ids = ['A5']\n",
    "animal_ids = ['A5', 'A10']\n",
    "# animal_ids = ['F22']\n",
    "# animal_ids = ['G25']\n",
    "base_folder = Path('./test-data').resolve()\n",
    "# base_folder = Path('/mnt/isilon/marsh_single_unit/PythonEEG Data Bins').resolve()\n",
    "for animal_id in animal_ids:\n",
    "    ao = visualization.AnimalOrganizer(base_folder, animal_id, mode=\"concat\", assume_from_number=True, truncate=False)\n",
    "    ao.convert_colbins_to_rowbins(overwrite=False)\n",
    "    ao.convert_rowbins_to_rec() # paralleization breaks if not enough memory\n",
    "\n",
    "    # with Client(cluster_window) as client:\n",
    "    #     client.upload_file(str(packageroot / 'pythoneeg.zip'))\n",
    "    #     display(client)\n",
    "    #     print(\"running war\")\n",
    "    #     war = ao.compute_windowed_analysis(['all'], exclude=['nspike', 'wavetemp'], multiprocess_mode='dask')\n",
    "    #     war.to_pickle_and_json(Path(f'./test-wars/{animal_id}').resolve())\n",
    "\n",
    "    # with Client(cluster_spike) as client:\n",
    "    #     client.upload_file(str(packageroot / 'pythoneeg.zip'))\n",
    "    #     display(client)\n",
    "    #     print(\"running sar\")\n",
    "    #     sar = ao.compute_spike_analysis(multiprocess_mode='dask')\n",
    "\n",
    "    # with Client(LocalCluster()) as client:\n",
    "    #     client.upload_file(str(packageroot / 'pythoneeg.zip'))\n",
    "    #     display(client)\n",
    "    #     print(\"running sar\")\n",
    "    sar = ao.compute_spike_analysis(multiprocess_mode='serial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in sar:\n",
    "    e.convert_to_mne(chunk_len=1440)\n",
    "    e.save_fif_and_json(Path(f'./test-mnes/{e.animal_day}').resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruct_sas = visualization.SpikeAnalysisResult.load_fif_and_json('./test-mnes/A5 WT Dec-12-2023')\n",
    "mne_obj = reconstruct_sas.result_mne\n",
    "\n",
    "\n",
    "\n",
    "print(mne_obj.info)\n",
    "\n",
    "# Apply the custom function to your MNE object\n",
    "#mne_obj.rename_channels(core.utils.clean_channel_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store all mne objects\n",
    "mne_objects = []\n",
    "\n",
    "animal_ids = ['A5', 'A10']\n",
    "\n",
    "# Get all files for each animal ID\n",
    "for animal_id in animal_ids:  # ['A5', 'A10']\n",
    "    # Find all files matching the pattern for this animal\n",
    "    file_pattern = f'./test-mnes/{animal_id}*'\n",
    "    animal_files = glob.glob(file_pattern)\n",
    "    \n",
    "    print(f\"Found {len(animal_files)} files for {animal_id}:\")\n",
    "    for file in animal_files:\n",
    "        print(f\"  - {file}\")\n",
    "        \n",
    "        # Load each file and add to our list\n",
    "        try:\n",
    "            reconstruct_sas = visualization.SpikeAnalysisResult.load_fif_and_json(file)\n",
    "            mne_obj = reconstruct_sas.result_mne\n",
    "            \n",
    "            # Store metadata in the 'temp' dictionary\n",
    "            if 'temp' not in mne_obj.info:\n",
    "                mne_obj.info['temp'] = {}\n",
    "            mne_obj.info['temp']['animal_id'] = animal_id\n",
    "            mne_obj.info['temp']['file_path'] = file\n",
    "            \n",
    "            # Add to our list\n",
    "            mne_objects.append(mne_obj)\n",
    "            print(f\"  ✓ Successfully loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error loading: {str(e)}\")\n",
    "\n",
    "print(f\"\\nTotal MNE objects loaded: {len(mne_objects)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can access the metadata like this:\n",
    "for i, obj in enumerate(mne_objects):\n",
    "    print(f\"Object {i}: Animal {obj.info['temp']['animal_id']}, from {obj.info['temp']['file_path']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events, event_id = mne.events_from_annotations(raw=mne_obj)\n",
    "#fig = mne.viz.plot_events(events, sfreq=mne_obj.info['sfreq'], first_samp=mne_obj.first_samp, event_id=event_id)\n",
    "\n",
    "unique_events, counts = np.unique(events[:, 2], return_counts=True)\n",
    "\n",
    "print(\"Event counts:\")\n",
    "for event_code, count in zip(unique_events, counts):\n",
    "    for name, code in event_id.items():\n",
    "        if code == event_code:\n",
    "            print(f\"{name}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reject_epochs_by_rms(epochs, threshold_sd=2.5, return_scores=True):\n",
    "    \"\"\"\n",
    "    Reject epochs based on RMS amplitude exceeding a threshold in standard deviations,\n",
    "    computing thresholds separately for each channel.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    epochs : mne.Epochs\n",
    "        The epochs object to clean\n",
    "    threshold_sd : float\n",
    "        Threshold in standard deviations from mean RMS\n",
    "    return_scores : bool\n",
    "        Whether to return the RMS scores for all epochs and channels\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    epochs_clean : mne.Epochs\n",
    "        The cleaned epochs object\n",
    "    rejection_info : dict\n",
    "        Dictionary with rejection statistics\n",
    "    rms_scores : ndarray (optional)\n",
    "        RMS amplitude for each epoch and channel if return_scores=True\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Make sure data is loaded\n",
    "    epochs.load_data()\n",
    "    \n",
    "    # Get the data\n",
    "    data = epochs.get_data()\n",
    "    n_epochs, n_channels, n_times = data.shape\n",
    "    \n",
    "    # Compute RMS for each epoch and each channel\n",
    "    # Formula: RMS = sqrt(mean(x²))\n",
    "    rms_per_epoch_channel = np.sqrt(np.mean(np.square(data), axis=2))  # shape: (n_epochs, n_channels)\n",
    "    \n",
    "    # Compute mean and standard deviation of RMS values for each channel\n",
    "    rms_mean_per_channel = np.mean(rms_per_epoch_channel, axis=0)\n",
    "    rms_std_per_channel = np.std(rms_per_epoch_channel, axis=0)\n",
    "    \n",
    "    # Set upper and lower thresholds for each channel\n",
    "    upper_thresh_per_channel = rms_mean_per_channel + threshold_sd * rms_std_per_channel\n",
    "    lower_thresh_per_channel = rms_mean_per_channel - threshold_sd * rms_std_per_channel\n",
    "    \n",
    "    # Find bad epochs per channel\n",
    "    bad_epochs_mask_per_channel = (rms_per_epoch_channel > upper_thresh_per_channel) | \\\n",
    "                                 (rms_per_epoch_channel < lower_thresh_per_channel)\n",
    "    \n",
    "    # An epoch is considered bad if it's bad in any channel\n",
    "    bad_epochs_mask = np.any(bad_epochs_mask_per_channel, axis=1)\n",
    "    \n",
    "    # Get indices of epochs to drop\n",
    "    bad_indices = np.where(bad_epochs_mask)[0]\n",
    "    \n",
    "    # Create a clean copy with the bad epochs dropped\n",
    "    epochs_clean = epochs.copy().drop(bad_indices)\n",
    "    \n",
    "    # Count bad epochs per channel\n",
    "    bad_epochs_per_channel = np.sum(bad_epochs_mask_per_channel, axis=0)\n",
    "    percent_bad_per_channel = (bad_epochs_per_channel / n_epochs) * 100\n",
    "    \n",
    "    # Overall rejection statistics\n",
    "    n_epochs_rejected = len(bad_indices)\n",
    "    percent_rejected = (n_epochs_rejected / n_epochs) * 100\n",
    "    \n",
    "    # Create info dictionary\n",
    "    channel_info = {\n",
    "        ch_name: {\n",
    "            'bad_epochs': bad_epochs_per_channel[i],\n",
    "            'percent_bad': percent_bad_per_channel[i],\n",
    "            'rms_mean': rms_mean_per_channel[i],\n",
    "            'rms_std': rms_std_per_channel[i],\n",
    "            'upper_threshold': upper_thresh_per_channel[i],\n",
    "            'lower_threshold': lower_thresh_per_channel[i]\n",
    "        }\n",
    "        for i, ch_name in enumerate(epochs.ch_names)\n",
    "    }\n",
    "    \n",
    "    rejection_info = {\n",
    "        'n_epochs_original': n_epochs,\n",
    "        'n_epochs_rejected': n_epochs_rejected,\n",
    "        'n_epochs_kept': n_epochs - n_epochs_rejected,\n",
    "        'percent_rejected': percent_rejected,\n",
    "        'threshold_sd': threshold_sd,\n",
    "        'bad_indices': bad_indices,\n",
    "        'channels': channel_info\n",
    "    }\n",
    "    \n",
    "    # Print rejection statistics\n",
    "    print(f\"Epochs rejection summary:\")\n",
    "    print(f\"  Original epochs: {n_epochs}\")\n",
    "    print(f\"  Rejected epochs: {n_epochs_rejected} ({percent_rejected:.2f}%)\")\n",
    "    print(f\"  Remaining epochs: {n_epochs - n_epochs_rejected}\")\n",
    "    print(f\"  Rejection by channel:\")\n",
    "    \n",
    "    # Sort channels by percent bad\n",
    "    sorted_channels = sorted(\n",
    "        epochs.ch_names, \n",
    "        key=lambda ch: channel_info[ch]['percent_bad'],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    for ch_name in sorted_channels:\n",
    "        info = channel_info[ch_name]\n",
    "        print(f\"    {ch_name}: {info['bad_epochs']} epochs ({info['percent_bad']:.2f}%)\")\n",
    "    \n",
    "    if return_scores:\n",
    "        return epochs_clean, rejection_info, rms_per_epoch_channel\n",
    "    else:\n",
    "        return epochs_clean, rejection_info\n",
    "\n",
    "\n",
    "def epoch_peri_spike(mne_obj: mne.io.Raw, tmin=-10, tmax=5, baseline=(None, -5), l_freq=1, h_freq=50):\n",
    "\n",
    "    raw = mne_obj.copy()\n",
    "    \n",
    "    # with joblib.parallel_backend('dask', scheduler_host=cluster_general.scheduler_address):\n",
    "    # raw.filter(l_freq=l_freq, h_freq=h_freq)\n",
    "\n",
    "    # logging.debug(f\"Events: {events}\")\n",
    "    # logging.debug(f\"Event dict: {event_dict}\")\n",
    "\n",
    "    # Process each spike type separately\n",
    "        # logging.debug(f\"Processing channel: {channel_name}\")\n",
    "        \n",
    "    # Create spike-centered epochs\n",
    "    epochs = mne.Epochs(\n",
    "        raw,\n",
    "        tmin=tmin,\n",
    "        tmax=tmax,\n",
    "        baseline=baseline,\n",
    "        event_repeated='drop',\n",
    "        preload=False)\n",
    "    epochs.drop_bad()\n",
    "    #epochs = random_subsample_epochs(epochs).load_data()\n",
    "    \n",
    "    # with joblib.parallel_backend('dask', scheduler_host=cluster_general.scheduler_address):\n",
    "    epochs.filter(l_freq=l_freq, h_freq=h_freq)\n",
    "    \n",
    "    return epochs\n",
    "\n",
    "def plot_peri_spike_evoked(epochs: mne.epochs, tcrop=(-3, 3)):\n",
    "    \n",
    "    # Create a figure with subplots for each channel\n",
    "    n_channels = len(epochs.ch_names)\n",
    "    fig, axes = plt.subplots(n_channels, 1, figsize=(10, 2*n_channels), sharex=True)\n",
    "\n",
    "    for ch_idx, ch_name in enumerate(epochs.ch_names):\n",
    "        evoked = epochs[ch_name].average()\n",
    "        evoked.crop(tmin=tcrop[0], tmax=tcrop[1])\n",
    "        evoked.plot(picks=[ch_idx], axes=axes[ch_idx], show=False, spatial_colors=False)\n",
    "        axes[ch_idx].set_title(ch_name)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_peri_spike_tfr(epochs, l_freq=1, h_freq=50, baseline=(-10, -5), tcrop=(-3, 3), \n",
    "                        rescale_method='zscore', fmin=None, fmax=None):\n",
    "    \"\"\"\n",
    "    Plot time-frequency representation around spikes with multiple rescaling options.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    epochs : mne.Epochs\n",
    "        The epoch data around spikes\n",
    "    l_freq : float\n",
    "        Lower frequency bound\n",
    "    h_freq : float\n",
    "        Upper frequency bound\n",
    "    baseline : tuple\n",
    "        Time range to use for baseline (for normalization)\n",
    "    tcrop : tuple\n",
    "        Time range to display in plots\n",
    "    rescale_method : str\n",
    "        Method for rescaling data:\n",
    "        - 'log': Apply log10 transform\n",
    "        - 'db': Convert to decibels relative to baseline (10*log10(data/baseline))\n",
    "        - 'zscore': Z-score relative to baseline (default)\n",
    "        - 'percent': Percent change from baseline\n",
    "        - 'ratio': Ratio relative to baseline (data/baseline)\n",
    "        - 'none': No rescaling\n",
    "    fmin : float | None\n",
    "        Minimum frequency to display (if None, use l_freq)\n",
    "    fmax : float | None\n",
    "        Maximum frequency to display (if None, use h_freq)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    fig : matplotlib.Figure\n",
    "        The figure containing the plots\n",
    "    \"\"\"\n",
    "    # Set frequency range for display\n",
    "    fmin = l_freq if fmin is None else fmin\n",
    "    fmax = h_freq if fmax is None else fmax\n",
    "    \n",
    "    # Create a figure with subplots for each channel\n",
    "    n_channels = len(epochs.ch_names)\n",
    "    fig, axes = plt.subplots(n_channels, 1, figsize=(10, 2*n_channels), sharex=True)\n",
    "    \n",
    "    # Ensure axes is always a list for consistent indexing\n",
    "    if n_channels == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Frequencies to analyze\n",
    "    freqs = np.arange(l_freq, h_freq, 1)\n",
    "    \n",
    "    # Set colormap based on rescale method\n",
    "    if rescale_method in ['zscore', 'percent']:\n",
    "        cmap = 'RdBu_r'  # Diverging colormap for data centered around zero\n",
    "    else:\n",
    "        cmap = 'viridis'  # Sequential colormap for non-centered data\n",
    "    \n",
    "    for ch_idx, ch_name in enumerate(epochs.ch_names):\n",
    "        # Compute TFR using Morlet wavelets\n",
    "        tfr_spike = epochs[ch_name].compute_tfr(\n",
    "            freqs=freqs, \n",
    "            n_cycles=freqs / 2,  # Adjust n_cycles based on frequency \n",
    "            method='morlet', \n",
    "            use_fft=True, \n",
    "            average=True\n",
    "        )\n",
    "        \n",
    "        # Get the data and apply requested transform\n",
    "        data = tfr_spike.data.copy()\n",
    "        \n",
    "        # Handle small/zero values for log transforms\n",
    "        if rescale_method in ['log', 'db', 'ratio', 'percent']:\n",
    "            # Add small constant to avoid log(0)\n",
    "            epsilon = np.finfo(float).eps\n",
    "            data = np.maximum(data, epsilon)\n",
    "        \n",
    "        # Apply selected rescaling method\n",
    "        if rescale_method == 'log':\n",
    "            # Simple log10 transform\n",
    "            data = np.log10(data)\n",
    "            colorbar_label = 'log₁₀(Power)'\n",
    "        \n",
    "        elif rescale_method == 'db':\n",
    "            # Convert to decibels relative to baseline\n",
    "            # First compute baseline power\n",
    "            baseline_mask = ((tfr_spike.times >= baseline[0]) & \n",
    "                            (tfr_spike.times <= baseline[1]))\n",
    "            baseline_data = np.mean(data[:, :, baseline_mask], axis=2, keepdims=True)\n",
    "            # Then compute dB = 10*log10(data/baseline)\n",
    "            data = 10 * np.log10(data / baseline_data)\n",
    "            colorbar_label = 'Power (dB)'\n",
    "            # Use diverging colormap for dB\n",
    "            cmap = 'RdBu_r'\n",
    "        \n",
    "        elif rescale_method == 'zscore':\n",
    "            # Z-score relative to baseline using MNE's function\n",
    "            data = mne.baseline.rescale(\n",
    "                data,\n",
    "                tfr_spike.times,\n",
    "                baseline=baseline,\n",
    "                mode='zscore'\n",
    "            )\n",
    "            colorbar_label = 'Z-score'\n",
    "        \n",
    "        elif rescale_method == 'percent':\n",
    "            # Percent change from baseline\n",
    "            baseline_mask = ((tfr_spike.times >= baseline[0]) & \n",
    "                            (tfr_spike.times <= baseline[1]))\n",
    "            baseline_data = np.mean(data[:, :, baseline_mask], axis=2, keepdims=True)\n",
    "            data = 100 * (data - baseline_data) / baseline_data\n",
    "            colorbar_label = 'Change (%)'\n",
    "        \n",
    "        elif rescale_method == 'ratio':\n",
    "            # Simple ratio to baseline\n",
    "            baseline_mask = ((tfr_spike.times >= baseline[0]) & \n",
    "                            (tfr_spike.times <= baseline[1]))\n",
    "            baseline_data = np.mean(data[:, :, baseline_mask], axis=2, keepdims=True)\n",
    "            data = data / baseline_data\n",
    "            colorbar_label = 'Ratio to baseline'\n",
    "        \n",
    "        elif rescale_method == 'none':\n",
    "            # No rescaling\n",
    "            colorbar_label = 'Power'\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown rescale method: {rescale_method}\")\n",
    "        \n",
    "        # Now crop to time range of interest for display\n",
    "        time_mask = (tfr_spike.times >= tcrop[0]) & (tfr_spike.times <= tcrop[1])\n",
    "        times_cropped = tfr_spike.times[time_mask]\n",
    "        data_cropped = data[:, :, time_mask]\n",
    "        \n",
    "        # Apply frequency range limits for display\n",
    "        freq_mask = (freqs >= fmin) & (freqs <= fmax)\n",
    "        freqs_display = freqs[freq_mask]\n",
    "        data_display = data_cropped[:, freq_mask, :]\n",
    "        \n",
    "        # Set limits for better visualization\n",
    "        if rescale_method in ['zscore', 'percent', 'db']:\n",
    "            # For methods that produce values centered around zero\n",
    "            vmin, vmax = np.percentile(data_display, [5, 95])\n",
    "            # Make the colormap symmetric around zero\n",
    "            abs_max = max(abs(vmin), abs(vmax))\n",
    "            vmin, vmax = -abs_max, abs_max\n",
    "        else:\n",
    "            # For methods that don't produce values centered around zero\n",
    "            vmin, vmax = np.percentile(data_display, [5, 95])\n",
    "        \n",
    "        # Plot the heatmap\n",
    "        im = axes[ch_idx].pcolormesh(\n",
    "            times_cropped, \n",
    "            freqs_display, \n",
    "            data_display[0, :, :],  # First dimension is epochs/average\n",
    "            cmap=cmap,\n",
    "            vmin=vmin,\n",
    "            vmax=vmax\n",
    "        )\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = fig.colorbar(im, ax=axes[ch_idx])\n",
    "        cbar.set_label(colorbar_label)\n",
    "        \n",
    "        # Add labels and title\n",
    "        axes[ch_idx].set_ylabel('Frequency (Hz)')\n",
    "        if rescale_method == 'none':\n",
    "            title = f'Channel: {ch_name}'\n",
    "        else:\n",
    "            title = f'Channel: {ch_name} ({rescale_method})'\n",
    "        axes[ch_idx].set_title(title)\n",
    "        \n",
    "        # Add vertical line at t=0 (spike time)\n",
    "        axes[ch_idx].axvline(x=0, color='black', linestyle='--', alpha=0.8)\n",
    "        \n",
    "        # # Add horizontal lines at classic frequency bands\n",
    "        # band_colors = {'theta': 'blue', 'alpha': 'green', 'beta': 'orange', 'gamma': 'red'}\n",
    "        # band_ranges = {'theta': (4, 8), 'alpha': (8, 13), 'beta': (13, 30), 'gamma': (30, 50)}\n",
    "        \n",
    "        # for band, (band_min, band_max) in band_ranges.items():\n",
    "        #     if band_min >= fmin and band_min <= fmax:\n",
    "        #         axes[ch_idx].axhline(y=band_min, color=band_colors[band], \n",
    "        #                             linestyle=':', alpha=0.6)\n",
    "        #     if band_max >= fmin and band_max <= fmax:\n",
    "        #         axes[ch_idx].axhline(y=band_max, color=band_colors[band], \n",
    "        #                             linestyle=':', alpha=0.6)\n",
    "    \n",
    "    # Add global x-axis label\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Add title with rescaling method\n",
    "    if rescale_method != 'none':\n",
    "        plt.suptitle(f'Time-Frequency Analysis ({rescale_method})', \n",
    "                   fontsize=12, y=1.02)\n",
    "    else:\n",
    "        plt.suptitle('Time-Frequency Analysis (unscaled)', \n",
    "                   fontsize=12, y=1.02)\n",
    "    \n",
    "    return fig\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting idea\n",
    "- Plot individual ERP time series (option to select channels, time range)\n",
    "- Add groupby variable for multiple files\n",
    "\n",
    "- Night vs. day\n",
    "    - split up the event\n",
    "    - sar object has this information (timestamp)\n",
    "    - use isday function in utils.py\n",
    "\n",
    "Joseph\n",
    "- rythmicity plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epochs.plot(scalings = 'auto', n_epochs = 2)\n",
    "\n",
    "# epochs_clean, rejection_info, rms_per_epoch = reject_epochs_by_rms(epochs, threshold_sd=1)\n",
    "\n",
    "\n",
    "# plot_peri_spike_evoked(epochs)\n",
    "# plot_peri_spike_evoked(epochs_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = epoch_peri_spike(mne_obj)\n",
    "\n",
    "plot_peri_spike_evoked(epochs)\n",
    "\n",
    "# 1. Plain log10 transform\n",
    "fig_log = plot_peri_spike_tfr(\n",
    "    epochs, \n",
    "    l_freq=1, \n",
    "    h_freq=50, \n",
    "    tcrop=(-3, 3),\n",
    "    rescale_method='log'\n",
    ")\n",
    "\n",
    "# 2. dB transform relative to baseline\n",
    "fig_db = plot_peri_spike_tfr(\n",
    "    epochs, \n",
    "    l_freq=1, \n",
    "    h_freq=50, \n",
    "    tcrop=(-3, 3),\n",
    "    rescale_method='db'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick hack to look at peri-spike EEG\n",
    "\n",
    "def analyze_peri_spike_tfr(mne_obj: mne.io.Raw, tmin=-10, tmax=5, tcrop=(-3, 3), baseline=(None, -5), l_freq=1, h_freq=50):\n",
    "    \"\"\"\n",
    "    Analyze time-frequency representation around spikes from an MNE object.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    mne_obj : mne.io.Raw\n",
    "        The MNE object containing the data and spike annotations\n",
    "    tmin : float\n",
    "        Start time relative to spike (in seconds)\n",
    "    tmax : float\n",
    "        End time relative to spike (in seconds)\n",
    "    tcrop : tuple\n",
    "        Time range to crop the data (in seconds)\n",
    "    baseline : tuple\n",
    "        Time range to compute baseline (in seconds)\n",
    "    l_freq : float\n",
    "        Lower frequency bound for filtering\n",
    "    h_freq : float\n",
    "        Upper frequency bound for filtering\n",
    "    \"\"\"\n",
    "    raw = mne_obj.copy()\n",
    "    \n",
    "    # with joblib.parallel_backend('dask', scheduler_host=cluster_general.scheduler_address):\n",
    "    # raw.filter(l_freq=l_freq, h_freq=h_freq)\n",
    "\n",
    "    events, event_dict = mne.events_from_annotations(raw=raw)\n",
    "    # logging.debug(f\"Events: {events}\")\n",
    "    # logging.debug(f\"Event dict: {event_dict}\")\n",
    "\n",
    "    # Process each spike type separately\n",
    "    for channel_name in event_dict.keys():\n",
    "        # logging.debug(f\"Processing channel: {channel_name}\")\n",
    "        \n",
    "        # Create spike-centered epochs\n",
    "        epochs = mne.Epochs(\n",
    "            raw,\n",
    "            tmin=tmin,\n",
    "            tmax=tmax,\n",
    "            baseline=baseline,\n",
    "            event_repeated='drop',\n",
    "            # preload=True,\n",
    "            event_id=channel_name,\n",
    "        )\n",
    "        epochs.drop_bad()\n",
    "        epochs = random_subsample_epochs(epochs).load_data()\n",
    "        epochs.crop(tmin=tcrop[0], tmax=tcrop[1])\n",
    "        # with joblib.parallel_backend('dask', scheduler_host=cluster_general.scheduler_address):\n",
    "        epochs.filter(l_freq=l_freq, h_freq=h_freq)\n",
    "\n",
    "        epochs.compute_psd(fmin=1.0, fmax=80.0).plot(picks=\"eeg\", average = True, exclude=\"bads\", amplitude=False)\n",
    "        # Plot the epochs\n",
    "        epochs.plot_image([channel_name])\n",
    "\n",
    "\n",
    "        \n",
    "        # logging.info(f\"Computing TFR: {channel_name}\")\n",
    "        # with joblib.parallel_config('dask', scheduler_host=cluster_general.scheduler_address):\n",
    "        #     # epochs.tfr_stockwell(l_freq=l_freq, h_freq=h_freq)\n",
    "        #     # tfr_spike = epochs.compute_tfr(freqs=(l_freq, h_freq), method='stockwell', average=True)\n",
    "        #     # TODO take log?\n",
    "        #     # TODO implement morlet over multitaper, it's cleaner. Follow documentation for exact\n",
    "        \n",
    "        tfr_spike = epochs.compute_tfr(freqs=np.arange(l_freq, h_freq, 1), n_cycles=np.arange(l_freq, h_freq, 1) * 2, method='morlet', use_fft=True, average=True)\n",
    "\n",
    "\n",
    "        tfr_spike.data = np.log10(tfr_spike.data)\n",
    "\n",
    "        vmin, vmax = np.percentile(tfr_spike.data, [5, 95])  # Using 5th and 95th percentiles\n",
    "\n",
    "        tfr_spike.plot(picks=[channel_name], baseline=None, mode=\"logratio\", title=channel_name, vlim=(vmin, vmax))\n",
    "\n",
    "\n",
    "        # Return the objects from the first spike type\n",
    "        return epochs#, tfr_spike\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tfr_hilbert(epochs: mne.Epochs, freqs: list[float], bandwidth: float):\n",
    "    data = np.empty((len(epochs), len(epochs.ch_names), freqs.size, epochs.times.size), dtype=complex)\n",
    "    logging.info(f\"data shape: {data.shape}\")\n",
    "    for idx, freq in enumerate(freqs):\n",
    "        data[:, :, idx] = epochs.get_data()\n",
    "        \n",
    "    # TODO implement hilbert transform on narrowband filter\n",
    "\n",
    "    # power = mne.time_frequency.EpochsTFRArray(epochs.info, data, epochs.times, freqs, method=\"hilbert\")\n",
    "    # power.average().plot(\n",
    "    #     [0],\n",
    "    #     baseline=(0.0, 0.1),\n",
    "    #     mode=\"mean\",\n",
    "    #     vlim=(0, 0.1),\n",
    "    #     axes=ax,\n",
    "    #     show=False,\n",
    "\n",
    "        \n",
    "\n",
    "# fig, axs = plt.subplots(1, 3, figsize=(15, 5), sharey=True, layout=\"constrained\")\n",
    "# bandwidths = [1.0, 2.0, 4.0]\n",
    "# for bandwidth, ax in zip(bandwidths, axs):\n",
    "#     data = np.zeros(\n",
    "#         (len(epochs), len(ch_names), freqs.size, epochs.times.size), dtype=complex\n",
    "#     )\n",
    "#     for idx, freq in enumerate(freqs):\n",
    "#         # Filter raw data and re-epoch to avoid the filter being longer than\n",
    "#         # the epoch data for low frequencies and short epochs, such as here.\n",
    "#         raw_filter = raw.copy()\n",
    "#         # NOTE: The bandwidths of the filters are changed from their defaults\n",
    "#         # to exaggerate differences. With the default transition bandwidths,\n",
    "#         # these are all very similar because the filters are almost the same.\n",
    "#         # In practice, using the default is usually a wise choice.\n",
    "#         raw_filter.filter(\n",
    "#             l_freq=freq - bandwidth / 2,\n",
    "#             h_freq=freq + bandwidth / 2,\n",
    "#             # no negative values for large bandwidth and low freq\n",
    "#             l_trans_bandwidth=min([4 * bandwidth, freq - bandwidth]),\n",
    "#             h_trans_bandwidth=4 * bandwidth,\n",
    "#         )\n",
    "#         raw_filter.apply_hilbert()\n",
    "#         epochs_hilb = Epochs(\n",
    "#             raw_filter, events, tmin=0, tmax=n_times / sfreq, baseline=(0, 0.1)\n",
    "#         )\n",
    "#         data[:, :, idx] = epochs_hilb.get_data()\n",
    "#     power = EpochsTFRArray(epochs.info, data, epochs.times, freqs, method=\"hilbert\")\n",
    "#     power.average().plot(\n",
    "#         [0],\n",
    "#         baseline=(0.0, 0.1),\n",
    "#         mode=\"mean\",\n",
    "#         vlim=(0, 0.1),\n",
    "#         axes=ax,\n",
    "#         show=False,\n",
    "#         colorbar=False,\n",
    "#     )\n",
    "#     n_cycles = \"scaled by freqs\" if not isinstance(n_cycles, int) else n_cycles\n",
    "#     ax.set_title(\n",
    "#         \"Sim: Using narrow bandpass filter Hilbert,\\n\"\n",
    "#         f\"bandwidth = {bandwidth}, \"\n",
    "#         f\"transition bandwidth = {4 * bandwidth}\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(mne_obj)\n",
    "epochs = analyze_peri_spike_tfr(mne_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs.plot(scalings = 'auto', n_epochs = 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_mouseEEG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
