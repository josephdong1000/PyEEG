{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Testing Notebook - Yong copy\n",
    "\n",
    "This notebook is meant to test external-facing functions to ensure they are working as expected.\n",
    "\n",
    "A dedicated test_all_functions.py with unit testing might be better, but this is a good benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "import tempfile\n",
    "import logging\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import mne\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ohy2/Documents/GitHubRepo/PyEEG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ohy2/Documents/GitHubRepo/PyEEG/.venv_mouseEEG/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DDFBinaryMetadata', 'LongRecordingOrganizer', 'convert_ddfcolbin_to_ddfrowbin', 'convert_ddfrowbin_to_si', 'convert_units_to_multiplier', 'convert_colpath_to_rowpath', 'filepath_to_index', 'is_day', 'set_temp_directory', 'get_temp_directory', 'LongRecordingAnalyzer', 'MountainSortAnalyzer', 'FragmentAnalyzer']\n"
     ]
    }
   ],
   "source": [
    "# packageroot = Path('../../').resolve()\n",
    "packageroot = Path('/Users/ohy2/Documents/GitHubRepo/PyEEG').resolve()\n",
    "print(packageroot)\n",
    "sys.path.append(str(packageroot))\n",
    "\n",
    "from pythoneeg import core\n",
    "from pythoneeg import visualization\n",
    "# from pythoneeg import constants\n",
    "\n",
    "print(core.__all__)\n",
    "# print(visualization.__all__)\n",
    "# print(dir(constants))\n",
    "\n",
    "core.set_temp_directory('/Users/ohy2/Documents/GitHubRepo/PyEEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script true\n",
    "# animal_ids = ['A5', 'A10', 'F22', 'G25']\n",
    "# animal_ids = ['A5']\n",
    "animal_ids = ['A5', 'A10']\n",
    "# animal_ids = ['F22']\n",
    "# animal_ids = ['G25']\n",
    "base_folder = Path('./test-data').resolve()\n",
    "# base_folder = Path('/mnt/isilon/marsh_single_unit/PythonEEG Data Bins').resolve()\n",
    "for animal_id in animal_ids:\n",
    "    ao = visualization.AnimalOrganizer(base_folder, animal_id, mode=\"concat\", assume_from_number=True, truncate=False)\n",
    "    ao.convert_colbins_to_rowbins(overwrite=False)\n",
    "    ao.convert_rowbins_to_rec() # paralleization breaks if not enough memory\n",
    "\n",
    "    # with Client(cluster_window) as client:\n",
    "    #     client.upload_file(str(packageroot / 'pythoneeg.zip'))\n",
    "    #     display(client)\n",
    "    #     print(\"running war\")\n",
    "    #     war = ao.compute_windowed_analysis(['all'], exclude=['nspike', 'wavetemp'], multiprocess_mode='dask')\n",
    "    #     war.to_pickle_and_json(Path(f'./test-wars/{animal_id}').resolve())\n",
    "\n",
    "    # with Client(cluster_spike) as client:\n",
    "    #     client.upload_file(str(packageroot / 'pythoneeg.zip'))\n",
    "    #     display(client)\n",
    "    #     print(\"running sar\")\n",
    "    #     sar = ao.compute_spike_analysis(multiprocess_mode='dask')\n",
    "\n",
    "    # with Client(LocalCluster()) as client:\n",
    "    #     client.upload_file(str(packageroot / 'pythoneeg.zip'))\n",
    "    #     display(client)\n",
    "    #     print(\"running sar\")\n",
    "    sar = ao.compute_spike_analysis(multiprocess_mode='serial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in sar:\n",
    "    e.convert_to_mne(chunk_len=1440)\n",
    "    e.save_fif_and_json(Path(f'./test-mnes/{e.animal_day}').resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruct_sas = visualization.SpikeAnalysisResult.load_fif_and_json('./test-mnes/A5 WT Dec-12-2023')\n",
    "mne_obj = reconstruct_sas.result_mne\n",
    "\n",
    "\n",
    "\n",
    "print(mne_obj.info)\n",
    "\n",
    "# Apply the custom function to your MNE object\n",
    "#mne_obj.rename_channels(core.utils.clean_channel_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store all mne objects\n",
    "mne_objects = []\n",
    "\n",
    "animal_ids = ['A5', 'A10']\n",
    "\n",
    "# Get all files for each animal ID\n",
    "for animal_id in animal_ids:  # ['A5', 'A10']\n",
    "    # Find all files matching the pattern for this animal\n",
    "    file_pattern = f'./test-mnes/{animal_id}*'\n",
    "    animal_files = glob.glob(file_pattern)\n",
    "    \n",
    "    print(f\"Found {len(animal_files)} files for {animal_id}:\")\n",
    "    for file in animal_files:\n",
    "        print(f\"  - {file}\")\n",
    "        \n",
    "        # Load each file and add to our list\n",
    "        try:\n",
    "            reconstruct_sas = visualization.SpikeAnalysisResult.load_fif_and_json(file)\n",
    "            mne_obj = reconstruct_sas.result_mne\n",
    "            \n",
    "            # Store metadata in the 'temp' dictionary\n",
    "            if 'temp' not in mne_obj.info:\n",
    "                mne_obj.info['temp'] = {}\n",
    "            mne_obj.info['temp']['animal_id'] = animal_id\n",
    "            mne_obj.info['temp']['file_path'] = file\n",
    "            \n",
    "            # Add to our list\n",
    "            mne_objects.append(mne_obj)\n",
    "            print(f\"  ✓ Successfully loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error loading: {str(e)}\")\n",
    "\n",
    "print(f\"\\nTotal MNE objects loaded: {len(mne_objects)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can access the metadata like this:\n",
    "for i, obj in enumerate(mne_objects):\n",
    "    print(f\"Object {i}: Animal {obj.info['temp']['animal_id']}, from {obj.info['temp']['file_path']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events, event_id = mne.events_from_annotations(raw=mne_obj)\n",
    "#fig = mne.viz.plot_events(events, sfreq=mne_obj.info['sfreq'], first_samp=mne_obj.first_samp, event_id=event_id)\n",
    "\n",
    "unique_events, counts = np.unique(events[:, 2], return_counts=True)\n",
    "\n",
    "print(\"Event counts:\")\n",
    "for event_code, count in zip(unique_events, counts):\n",
    "    for name, code in event_id.items():\n",
    "        if code == event_code:\n",
    "            print(f\"{name}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_subsample_epochs(epochs: mne.Epochs, max_epochs=1000):\n",
    "    if len(epochs) > max_epochs:\n",
    "        # Get random indices for subsampling\n",
    "        indices = np.random.choice(len(epochs), size=max_epochs, replace=False)\n",
    "        epochs = epochs[indices]\n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reject_epochs_by_rms(epochs, threshold_sd=2.5, return_scores=True):\n",
    "    \"\"\"\n",
    "    Reject epochs based on RMS amplitude exceeding a threshold in standard deviations,\n",
    "    computing thresholds separately for each channel.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    epochs : mne.Epochs\n",
    "        The epochs object to clean\n",
    "    threshold_sd : float\n",
    "        Threshold in standard deviations from mean RMS\n",
    "    return_scores : bool\n",
    "        Whether to return the RMS scores for all epochs and channels\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    epochs_clean : mne.Epochs\n",
    "        The cleaned epochs object\n",
    "    rejection_info : dict\n",
    "        Dictionary with rejection statistics\n",
    "    rms_scores : ndarray (optional)\n",
    "        RMS amplitude for each epoch and channel if return_scores=True\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # Make sure data is loaded\n",
    "    epochs.load_data()\n",
    "    \n",
    "    # Get the data\n",
    "    data = epochs.get_data()\n",
    "    n_epochs, n_channels, n_times = data.shape\n",
    "    \n",
    "    # Compute RMS for each epoch and each channel\n",
    "    # Formula: RMS = sqrt(mean(x²))\n",
    "    rms_per_epoch_channel = np.sqrt(np.mean(np.square(data), axis=2))  # shape: (n_epochs, n_channels)\n",
    "    \n",
    "    # Compute mean and standard deviation of RMS values for each channel\n",
    "    rms_mean_per_channel = np.mean(rms_per_epoch_channel, axis=0)\n",
    "    rms_std_per_channel = np.std(rms_per_epoch_channel, axis=0)\n",
    "    \n",
    "    # Set upper and lower thresholds for each channel\n",
    "    upper_thresh_per_channel = rms_mean_per_channel + threshold_sd * rms_std_per_channel\n",
    "    lower_thresh_per_channel = rms_mean_per_channel - threshold_sd * rms_std_per_channel\n",
    "    \n",
    "    # Find bad epochs per channel\n",
    "    bad_epochs_mask_per_channel = (rms_per_epoch_channel > upper_thresh_per_channel) | \\\n",
    "                                 (rms_per_epoch_channel < lower_thresh_per_channel)\n",
    "    \n",
    "    # An epoch is considered bad if it's bad in any channel\n",
    "    bad_epochs_mask = np.any(bad_epochs_mask_per_channel, axis=1)\n",
    "    \n",
    "    # Get indices of epochs to drop\n",
    "    bad_indices = np.where(bad_epochs_mask)[0]\n",
    "    \n",
    "    # Create a clean copy with the bad epochs dropped\n",
    "    epochs_clean = epochs.copy().drop(bad_indices)\n",
    "    \n",
    "    # Count bad epochs per channel\n",
    "    bad_epochs_per_channel = np.sum(bad_epochs_mask_per_channel, axis=0)\n",
    "    percent_bad_per_channel = (bad_epochs_per_channel / n_epochs) * 100\n",
    "    \n",
    "    # Overall rejection statistics\n",
    "    n_epochs_rejected = len(bad_indices)\n",
    "    percent_rejected = (n_epochs_rejected / n_epochs) * 100\n",
    "    \n",
    "    # Create info dictionary\n",
    "    channel_info = {\n",
    "        ch_name: {\n",
    "            'bad_epochs': bad_epochs_per_channel[i],\n",
    "            'percent_bad': percent_bad_per_channel[i],\n",
    "            'rms_mean': rms_mean_per_channel[i],\n",
    "            'rms_std': rms_std_per_channel[i],\n",
    "            'upper_threshold': upper_thresh_per_channel[i],\n",
    "            'lower_threshold': lower_thresh_per_channel[i]\n",
    "        }\n",
    "        for i, ch_name in enumerate(epochs.ch_names)\n",
    "    }\n",
    "    \n",
    "    rejection_info = {\n",
    "        'n_epochs_original': n_epochs,\n",
    "        'n_epochs_rejected': n_epochs_rejected,\n",
    "        'n_epochs_kept': n_epochs - n_epochs_rejected,\n",
    "        'percent_rejected': percent_rejected,\n",
    "        'threshold_sd': threshold_sd,\n",
    "        'bad_indices': bad_indices,\n",
    "        'channels': channel_info\n",
    "    }\n",
    "    \n",
    "    # Print rejection statistics\n",
    "    print(f\"Epochs rejection summary:\")\n",
    "    print(f\"  Original epochs: {n_epochs}\")\n",
    "    print(f\"  Rejected epochs: {n_epochs_rejected} ({percent_rejected:.2f}%)\")\n",
    "    print(f\"  Remaining epochs: {n_epochs - n_epochs_rejected}\")\n",
    "    print(f\"  Rejection by channel:\")\n",
    "    \n",
    "    # Sort channels by percent bad\n",
    "    sorted_channels = sorted(\n",
    "        epochs.ch_names, \n",
    "        key=lambda ch: channel_info[ch]['percent_bad'],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    for ch_name in sorted_channels:\n",
    "        info = channel_info[ch_name]\n",
    "        print(f\"    {ch_name}: {info['bad_epochs']} epochs ({info['percent_bad']:.2f}%)\")\n",
    "    \n",
    "    if return_scores:\n",
    "        return epochs_clean, rejection_info, rms_per_epoch_channel\n",
    "    else:\n",
    "        return epochs_clean, rejection_info\n",
    "\n",
    "\n",
    "def epoch_peri_spike(mne_obj: mne.io.Raw, tmin=-10, tmax=5, baseline=(None, -5), l_freq=1, h_freq=50):\n",
    "\n",
    "    raw = mne_obj.copy()\n",
    "    \n",
    "    # with joblib.parallel_backend('dask', scheduler_host=cluster_general.scheduler_address):\n",
    "    # raw.filter(l_freq=l_freq, h_freq=h_freq)\n",
    "\n",
    "    # logging.debug(f\"Events: {events}\")\n",
    "    # logging.debug(f\"Event dict: {event_dict}\")\n",
    "\n",
    "    # Process each spike type separately\n",
    "        # logging.debug(f\"Processing channel: {channel_name}\")\n",
    "        \n",
    "    # Create spike-centered epochs\n",
    "    epochs = mne.Epochs(\n",
    "        raw,\n",
    "        tmin=tmin,\n",
    "        tmax=tmax,\n",
    "        baseline=baseline,\n",
    "        event_repeated='drop',\n",
    "        preload=False)\n",
    "    epochs.drop_bad()\n",
    "    #epochs = random_subsample_epochs(epochs).load_data()\n",
    "    \n",
    "    # with joblib.parallel_backend('dask', scheduler_host=cluster_general.scheduler_address):\n",
    "    epochs.filter(l_freq=l_freq, h_freq=h_freq)\n",
    "    \n",
    "    return epochs\n",
    "\n",
    "def plot_peri_spike_evoked(epochs: mne.epochs, tcrop=(-3, 3)):\n",
    "    \n",
    "    # Create a figure with subplots for each channel\n",
    "    n_channels = len(epochs.ch_names)\n",
    "    fig, axes = plt.subplots(n_channels, 1, figsize=(10, 2*n_channels), sharex=True)\n",
    "\n",
    "    for ch_idx, ch_name in enumerate(epochs.ch_names):\n",
    "        evoked = epochs[ch_name].average()\n",
    "        evoked.crop(tmin=tcrop[0], tmax=tcrop[1])\n",
    "        evoked.plot(picks=[ch_idx], axes=axes[ch_idx], show=False, spatial_colors=False)\n",
    "        axes[ch_idx].set_title(ch_name)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_peri_spike_tfr(epochs, l_freq=1, h_freq=50, baseline=(-10, -5), tcrop=(-3, 3), \n",
    "                        rescale_method='zscore', fmin=None, fmax=None):\n",
    "    \"\"\"\n",
    "    Plot time-frequency representation around spikes with multiple rescaling options.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    epochs : mne.Epochs\n",
    "        The epoch data around spikes\n",
    "    l_freq : float\n",
    "        Lower frequency bound\n",
    "    h_freq : float\n",
    "        Upper frequency bound\n",
    "    baseline : tuple\n",
    "        Time range to use for baseline (for normalization)\n",
    "    tcrop : tuple\n",
    "        Time range to display in plots\n",
    "    rescale_method : str\n",
    "        Method for rescaling data:\n",
    "        - 'log': Apply log10 transform\n",
    "        - 'db': Convert to decibels relative to baseline (10*log10(data/baseline))\n",
    "        - 'zscore': Z-score relative to baseline (default)\n",
    "        - 'percent': Percent change from baseline\n",
    "        - 'ratio': Ratio relative to baseline (data/baseline)\n",
    "        - 'none': No rescaling\n",
    "    fmin : float | None\n",
    "        Minimum frequency to display (if None, use l_freq)\n",
    "    fmax : float | None\n",
    "        Maximum frequency to display (if None, use h_freq)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    fig : matplotlib.Figure\n",
    "        The figure containing the plots\n",
    "    \"\"\"\n",
    "    # Set frequency range for display\n",
    "    fmin = l_freq if fmin is None else fmin\n",
    "    fmax = h_freq if fmax is None else fmax\n",
    "    \n",
    "    # Create a figure with subplots for each channel\n",
    "    n_channels = len(epochs.ch_names)\n",
    "    fig, axes = plt.subplots(n_channels, 1, figsize=(10, 2*n_channels), sharex=True)\n",
    "    \n",
    "    # Ensure axes is always a list for consistent indexing\n",
    "    if n_channels == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Frequencies to analyze\n",
    "    freqs = np.arange(l_freq, h_freq, 1)\n",
    "    \n",
    "    # Set colormap based on rescale method\n",
    "    if rescale_method in ['zscore', 'percent']:\n",
    "        cmap = 'RdBu_r'  # Diverging colormap for data centered around zero\n",
    "    else:\n",
    "        cmap = 'viridis'  # Sequential colormap for non-centered data\n",
    "    \n",
    "    for ch_idx, ch_name in enumerate(epochs.ch_names):\n",
    "        # Compute TFR using Morlet wavelets\n",
    "        tfr_spike = epochs[ch_name].compute_tfr(\n",
    "            freqs=freqs, \n",
    "            n_cycles=freqs / 2,  # Adjust n_cycles based on frequency \n",
    "            method='morlet', \n",
    "            use_fft=True, \n",
    "            average=True\n",
    "        )\n",
    "        \n",
    "        # Get the data and apply requested transform\n",
    "        data = tfr_spike.data.copy()\n",
    "        \n",
    "        # Handle small/zero values for log transforms\n",
    "        if rescale_method in ['log', 'db', 'ratio', 'percent']:\n",
    "            # Add small constant to avoid log(0)\n",
    "            epsilon = np.finfo(float).eps\n",
    "            data = np.maximum(data, epsilon)\n",
    "        \n",
    "        # Apply selected rescaling method\n",
    "        if rescale_method == 'log':\n",
    "            # Simple log10 transform\n",
    "            data = np.log10(data)\n",
    "            colorbar_label = 'log₁₀(Power)'\n",
    "        \n",
    "        elif rescale_method == 'db':\n",
    "            # Convert to decibels relative to baseline\n",
    "            # First compute baseline power\n",
    "            baseline_mask = ((tfr_spike.times >= baseline[0]) & \n",
    "                            (tfr_spike.times <= baseline[1]))\n",
    "            baseline_data = np.mean(data[:, :, baseline_mask], axis=2, keepdims=True)\n",
    "            # Then compute dB = 10*log10(data/baseline)\n",
    "            data = 10 * np.log10(data / baseline_data)\n",
    "            colorbar_label = 'Power (dB)'\n",
    "            # Use diverging colormap for dB\n",
    "            cmap = 'RdBu_r'\n",
    "        \n",
    "        elif rescale_method == 'zscore':\n",
    "            # Z-score relative to baseline using MNE's function\n",
    "            data = mne.baseline.rescale(\n",
    "                data,\n",
    "                tfr_spike.times,\n",
    "                baseline=baseline,\n",
    "                mode='zscore'\n",
    "            )\n",
    "            colorbar_label = 'Z-score'\n",
    "        \n",
    "        elif rescale_method == 'percent':\n",
    "            # Percent change from baseline\n",
    "            baseline_mask = ((tfr_spike.times >= baseline[0]) & \n",
    "                            (tfr_spike.times <= baseline[1]))\n",
    "            baseline_data = np.mean(data[:, :, baseline_mask], axis=2, keepdims=True)\n",
    "            data = 100 * (data - baseline_data) / baseline_data\n",
    "            colorbar_label = 'Change (%)'\n",
    "        \n",
    "        elif rescale_method == 'ratio':\n",
    "            # Simple ratio to baseline\n",
    "            baseline_mask = ((tfr_spike.times >= baseline[0]) & \n",
    "                            (tfr_spike.times <= baseline[1]))\n",
    "            baseline_data = np.mean(data[:, :, baseline_mask], axis=2, keepdims=True)\n",
    "            data = data / baseline_data\n",
    "            colorbar_label = 'Ratio to baseline'\n",
    "        \n",
    "        elif rescale_method == 'none':\n",
    "            # No rescaling\n",
    "            colorbar_label = 'Power'\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown rescale method: {rescale_method}\")\n",
    "        \n",
    "        # Now crop to time range of interest for display\n",
    "        time_mask = (tfr_spike.times >= tcrop[0]) & (tfr_spike.times <= tcrop[1])\n",
    "        times_cropped = tfr_spike.times[time_mask]\n",
    "        data_cropped = data[:, :, time_mask]\n",
    "        \n",
    "        # Apply frequency range limits for display\n",
    "        freq_mask = (freqs >= fmin) & (freqs <= fmax)\n",
    "        freqs_display = freqs[freq_mask]\n",
    "        data_display = data_cropped[:, freq_mask, :]\n",
    "        \n",
    "        # Set limits for better visualization\n",
    "        if rescale_method in ['zscore', 'percent', 'db']:\n",
    "            # For methods that produce values centered around zero\n",
    "            vmin, vmax = np.percentile(data_display, [5, 95])\n",
    "            # Make the colormap symmetric around zero\n",
    "            abs_max = max(abs(vmin), abs(vmax))\n",
    "            vmin, vmax = -abs_max, abs_max\n",
    "        else:\n",
    "            # For methods that don't produce values centered around zero\n",
    "            vmin, vmax = np.percentile(data_display, [5, 95])\n",
    "        \n",
    "        # Plot the heatmap\n",
    "        im = axes[ch_idx].pcolormesh(\n",
    "            times_cropped, \n",
    "            freqs_display, \n",
    "            data_display[0, :, :],  # First dimension is epochs/average\n",
    "            cmap=cmap,\n",
    "            vmin=vmin,\n",
    "            vmax=vmax\n",
    "        )\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = fig.colorbar(im, ax=axes[ch_idx])\n",
    "        cbar.set_label(colorbar_label)\n",
    "        \n",
    "        # Add labels and title\n",
    "        axes[ch_idx].set_ylabel('Frequency (Hz)')\n",
    "        if rescale_method == 'none':\n",
    "            title = f'Channel: {ch_name}'\n",
    "        else:\n",
    "            title = f'Channel: {ch_name} ({rescale_method})'\n",
    "        axes[ch_idx].set_title(title)\n",
    "        \n",
    "        # Add vertical line at t=0 (spike time)\n",
    "        axes[ch_idx].axvline(x=0, color='black', linestyle='--', alpha=0.8)\n",
    "        \n",
    "        # # Add horizontal lines at classic frequency bands\n",
    "        # band_colors = {'theta': 'blue', 'alpha': 'green', 'beta': 'orange', 'gamma': 'red'}\n",
    "        # band_ranges = {'theta': (4, 8), 'alpha': (8, 13), 'beta': (13, 30), 'gamma': (30, 50)}\n",
    "        \n",
    "        # for band, (band_min, band_max) in band_ranges.items():\n",
    "        #     if band_min >= fmin and band_min <= fmax:\n",
    "        #         axes[ch_idx].axhline(y=band_min, color=band_colors[band], \n",
    "        #                             linestyle=':', alpha=0.6)\n",
    "        #     if band_max >= fmin and band_max <= fmax:\n",
    "        #         axes[ch_idx].axhline(y=band_max, color=band_colors[band], \n",
    "        #                             linestyle=':', alpha=0.6)\n",
    "    \n",
    "    # Add global x-axis label\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Add title with rescaling method\n",
    "    if rescale_method != 'none':\n",
    "        plt.suptitle(f'Time-Frequency Analysis ({rescale_method})', \n",
    "                   fontsize=12, y=1.02)\n",
    "    else:\n",
    "        plt.suptitle('Time-Frequency Analysis (unscaled)', \n",
    "                   fontsize=12, y=1.02)\n",
    "    \n",
    "    return fig\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting idea\n",
    "- Plot individual ERP time series (option to select channels, time range)\n",
    "- Add groupby variable for multiple files\n",
    "\n",
    "- Night vs. day\n",
    "    - split up the event\n",
    "    - sar object has this information (timestamp)\n",
    "    - use isday function in utils.py\n",
    "\n",
    "Joseph\n",
    "- rythmicity plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epochs.plot(scalings = 'auto', n_epochs = 2)\n",
    "\n",
    "# epochs_clean, rejection_info, rms_per_epoch = reject_epochs_by_rms(epochs, threshold_sd=1)\n",
    "\n",
    "\n",
    "# plot_peri_spike_evoked(epochs)\n",
    "# plot_peri_spike_evoked(epochs_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = epoch_peri_spike(mne_obj)\n",
    "\n",
    "plot_peri_spike_evoked(epochs)\n",
    "\n",
    "# 1. Plain log10 transform\n",
    "fig_log = plot_peri_spike_tfr(\n",
    "    epochs, \n",
    "    l_freq=1, \n",
    "    h_freq=50, \n",
    "    tcrop=(-3, 3),\n",
    "    rescale_method='log'\n",
    ")\n",
    "\n",
    "# 2. dB transform relative to baseline\n",
    "fig_db = plot_peri_spike_tfr(\n",
    "    epochs, \n",
    "    l_freq=1, \n",
    "    h_freq=50, \n",
    "    tcrop=(-3, 3),\n",
    "    rescale_method='db'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick hack to look at peri-spike EEG\n",
    "\n",
    "def analyze_peri_spike_tfr(mne_obj: mne.io.Raw, tmin=-10, tmax=5, tcrop=(-3, 3), baseline=(None, -5), l_freq=1, h_freq=50):\n",
    "    \"\"\"\n",
    "    Analyze time-frequency representation around spikes from an MNE object.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    mne_obj : mne.io.Raw\n",
    "        The MNE object containing the data and spike annotations\n",
    "    tmin : float\n",
    "        Start time relative to spike (in seconds)\n",
    "    tmax : float\n",
    "        End time relative to spike (in seconds)\n",
    "    tcrop : tuple\n",
    "        Time range to crop the data (in seconds)\n",
    "    baseline : tuple\n",
    "        Time range to compute baseline (in seconds)\n",
    "    l_freq : float\n",
    "        Lower frequency bound for filtering\n",
    "    h_freq : float\n",
    "        Upper frequency bound for filtering\n",
    "    \"\"\"\n",
    "    raw = mne_obj.copy()\n",
    "    \n",
    "    # with joblib.parallel_backend('dask', scheduler_host=cluster_general.scheduler_address):\n",
    "    # raw.filter(l_freq=l_freq, h_freq=h_freq)\n",
    "\n",
    "    events, event_dict = mne.events_from_annotations(raw=raw)\n",
    "    # logging.debug(f\"Events: {events}\")\n",
    "    # logging.debug(f\"Event dict: {event_dict}\")\n",
    "\n",
    "    # Process each spike type separately\n",
    "    for channel_name in event_dict.keys():\n",
    "        # logging.debug(f\"Processing channel: {channel_name}\")\n",
    "        \n",
    "        # Create spike-centered epochs\n",
    "        epochs = mne.Epochs(\n",
    "            raw,\n",
    "            tmin=tmin,\n",
    "            tmax=tmax,\n",
    "            baseline=baseline,\n",
    "            event_repeated='drop',\n",
    "            # preload=True,\n",
    "            event_id=channel_name,\n",
    "        )\n",
    "        epochs.drop_bad()\n",
    "        epochs = random_subsample_epochs(epochs).load_data()\n",
    "        epochs.crop(tmin=tcrop[0], tmax=tcrop[1])\n",
    "        # with joblib.parallel_backend('dask', scheduler_host=cluster_general.scheduler_address):\n",
    "        epochs.filter(l_freq=l_freq, h_freq=h_freq)\n",
    "\n",
    "        epochs.compute_psd(fmin=1.0, fmax=80.0).plot(picks=\"eeg\", average = True, exclude=\"bads\", amplitude=False)\n",
    "        # Plot the epochs\n",
    "        epochs.plot_image([channel_name])\n",
    "\n",
    "\n",
    "        \n",
    "        # logging.info(f\"Computing TFR: {channel_name}\")\n",
    "        # with joblib.parallel_config('dask', scheduler_host=cluster_general.scheduler_address):\n",
    "        #     # epochs.tfr_stockwell(l_freq=l_freq, h_freq=h_freq)\n",
    "        #     # tfr_spike = epochs.compute_tfr(freqs=(l_freq, h_freq), method='stockwell', average=True)\n",
    "        #     # TODO take log?\n",
    "        #     # TODO implement morlet over multitaper, it's cleaner. Follow documentation for exact\n",
    "        \n",
    "        tfr_spike = epochs.compute_tfr(freqs=np.arange(l_freq, h_freq, 1), n_cycles=np.arange(l_freq, h_freq, 1) * 2, method='morlet', use_fft=True, average=True)\n",
    "\n",
    "\n",
    "        tfr_spike.data = np.log10(tfr_spike.data)\n",
    "\n",
    "        vmin, vmax = np.percentile(tfr_spike.data, [5, 95])  # Using 5th and 95th percentiles\n",
    "\n",
    "        tfr_spike.plot(picks=[channel_name], baseline=None, mode=\"logratio\", title=channel_name, vlim=(vmin, vmax))\n",
    "\n",
    "\n",
    "        # Return the objects from the first spike type\n",
    "        return epochs#, tfr_spike\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tfr_hilbert(epochs: mne.Epochs, freqs: list[float], bandwidth: float):\n",
    "    data = np.empty((len(epochs), len(epochs.ch_names), freqs.size, epochs.times.size), dtype=complex)\n",
    "    logging.info(f\"data shape: {data.shape}\")\n",
    "    for idx, freq in enumerate(freqs):\n",
    "        data[:, :, idx] = epochs.get_data()\n",
    "        \n",
    "    # TODO implement hilbert transform on narrowband filter\n",
    "\n",
    "    # power = mne.time_frequency.EpochsTFRArray(epochs.info, data, epochs.times, freqs, method=\"hilbert\")\n",
    "    # power.average().plot(\n",
    "    #     [0],\n",
    "    #     baseline=(0.0, 0.1),\n",
    "    #     mode=\"mean\",\n",
    "    #     vlim=(0, 0.1),\n",
    "    #     axes=ax,\n",
    "    #     show=False,\n",
    "\n",
    "        \n",
    "\n",
    "# fig, axs = plt.subplots(1, 3, figsize=(15, 5), sharey=True, layout=\"constrained\")\n",
    "# bandwidths = [1.0, 2.0, 4.0]\n",
    "# for bandwidth, ax in zip(bandwidths, axs):\n",
    "#     data = np.zeros(\n",
    "#         (len(epochs), len(ch_names), freqs.size, epochs.times.size), dtype=complex\n",
    "#     )\n",
    "#     for idx, freq in enumerate(freqs):\n",
    "#         # Filter raw data and re-epoch to avoid the filter being longer than\n",
    "#         # the epoch data for low frequencies and short epochs, such as here.\n",
    "#         raw_filter = raw.copy()\n",
    "#         # NOTE: The bandwidths of the filters are changed from their defaults\n",
    "#         # to exaggerate differences. With the default transition bandwidths,\n",
    "#         # these are all very similar because the filters are almost the same.\n",
    "#         # In practice, using the default is usually a wise choice.\n",
    "#         raw_filter.filter(\n",
    "#             l_freq=freq - bandwidth / 2,\n",
    "#             h_freq=freq + bandwidth / 2,\n",
    "#             # no negative values for large bandwidth and low freq\n",
    "#             l_trans_bandwidth=min([4 * bandwidth, freq - bandwidth]),\n",
    "#             h_trans_bandwidth=4 * bandwidth,\n",
    "#         )\n",
    "#         raw_filter.apply_hilbert()\n",
    "#         epochs_hilb = Epochs(\n",
    "#             raw_filter, events, tmin=0, tmax=n_times / sfreq, baseline=(0, 0.1)\n",
    "#         )\n",
    "#         data[:, :, idx] = epochs_hilb.get_data()\n",
    "#     power = EpochsTFRArray(epochs.info, data, epochs.times, freqs, method=\"hilbert\")\n",
    "#     power.average().plot(\n",
    "#         [0],\n",
    "#         baseline=(0.0, 0.1),\n",
    "#         mode=\"mean\",\n",
    "#         vlim=(0, 0.1),\n",
    "#         axes=ax,\n",
    "#         show=False,\n",
    "#         colorbar=False,\n",
    "#     )\n",
    "#     n_cycles = \"scaled by freqs\" if not isinstance(n_cycles, int) else n_cycles\n",
    "#     ax.set_title(\n",
    "#         \"Sim: Using narrow bandpass filter Hilbert,\\n\"\n",
    "#         f\"bandwidth = {bandwidth}, \"\n",
    "#         f\"transition bandwidth = {4 * bandwidth}\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(mne_obj)\n",
    "epochs = analyze_peri_spike_tfr(mne_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs.plot(scalings = 'auto', n_epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import logging\n",
    "from typing import Literal, Optional, List, Dict, Any, Union, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "import mne\n",
    "from mne.time_frequency import tfr_morlet\n",
    "\n",
    "class MNEExperimentPlotter:\n",
    "    \"\"\"\n",
    "    A class for creating various plots from a list of MNE objects with grouping capabilities.\n",
    "    \n",
    "    This class provides methods for creating different types of plots (evoked, TFR, etc.)\n",
    "    from MNE data with consistent data processing, grouping, and contrasting.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mne_objects: Union[mne.io.Raw, List[mne.io.Raw]]):\n",
    "        \"\"\"\n",
    "        Initialize plotter with MNE Raw object(s).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        mne_objects : mne.io.Raw or list[mne.io.Raw]\n",
    "            Single MNE Raw object or list of MNE Raw objects\n",
    "        \"\"\"\n",
    "        # Convert single MNE object to list\n",
    "        if not isinstance(mne_objects, list):\n",
    "            mne_objects = [mne_objects]\n",
    "            \n",
    "        self.mne_objects = mne_objects\n",
    "        \n",
    "        # Extract metadata from all objects\n",
    "        self.metadata = []\n",
    "        for obj in mne_objects:\n",
    "            # Ensure temp dict exists\n",
    "            if 'temp' not in obj.info:\n",
    "                obj.info['temp'] = {}\n",
    "            \n",
    "            # Extract metadata from the object\n",
    "            meta = {\n",
    "                'animal_id': obj.info['temp'].get('animal_id', 'Unknown'),\n",
    "                'file_path': obj.info['temp'].get('file_path', 'Unknown'),\n",
    "                'sfreq': obj.info['sfreq'],\n",
    "                'ch_names': obj.ch_names,\n",
    "                'mne_obj': obj  # Store reference to the object itself\n",
    "            }\n",
    "            self.metadata.append(meta)\n",
    "        \n",
    "        # Collect all unique channel names\n",
    "        self.all_channel_names = sorted(list(set([ch for obj in mne_objects for ch in obj.ch_names])))\n",
    "        logging.info(f'all_channel_names: {self.all_channel_names}')\n",
    "\n",
    "    def load_mne_objects(animal_ids, base_folder='./test-mnes'):\n",
    "        \"\"\"\n",
    "        Load MNE objects from SpikeAnalysisResult files for specified animal IDs.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        animal_ids : list of str\n",
    "            List of animal IDs to load data for\n",
    "        base_folder : str or Path\n",
    "            Base folder containing the data files\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        list of mne.io.Raw\n",
    "            List of loaded MNE Raw objects with metadata\n",
    "        \"\"\"\n",
    "        import glob\n",
    "        from pathlib import Path\n",
    "        \n",
    "        # Create a list to store all mne objects\n",
    "        mne_objects = []\n",
    "        \n",
    "        # Get all files for each animal ID\n",
    "        for animal_id in animal_ids:\n",
    "            # Find all files matching the pattern for this animal\n",
    "            file_pattern = f'{base_folder}/{animal_id}*'\n",
    "            animal_files = glob.glob(file_pattern)\n",
    "            \n",
    "            print(f\"Found {len(animal_files)} files for {animal_id}:\")\n",
    "            for file in animal_files:\n",
    "                print(f\"  - {file}\")\n",
    "                \n",
    "                # Load each file and add to our list\n",
    "                try:\n",
    "                    reconstruct_sas = visualization.SpikeAnalysisResult.load_fif_and_json(file)\n",
    "                    mne_obj = reconstruct_sas.result_mne\n",
    "                    \n",
    "                    # Store metadata in the 'temp' dictionary\n",
    "                    if 'temp' not in mne_obj.info:\n",
    "                        mne_obj.info['temp'] = {}\n",
    "                    mne_obj.info['temp']['animal_id'] = animal_id\n",
    "                    mne_obj.info['temp']['file_path'] = file\n",
    "                    \n",
    "                    # Add to our list\n",
    "                    mne_objects.append(mne_obj)\n",
    "                    print(f\"  ✓ Successfully loaded\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  ✗ Error loading: {str(e)}\")\n",
    "        \n",
    "        print(f\"\\nTotal MNE objects loaded: {len(mne_objects)}\")\n",
    "        return mne_objects\n",
    "\n",
    "    def get_objects_by_group(self, group_key: str) -> Dict[str, List[mne.io.Raw]]:\n",
    "        \"\"\"\n",
    "        Group MNE objects by a metadata key\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        group_key : str\n",
    "            Metadata key to group by (e.g. 'animal_id')\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, List[mne.io.Raw]]\n",
    "            Dictionary mapping group values to lists of MNE objects\n",
    "        \"\"\"\n",
    "        groups = {}\n",
    "        for meta in self.metadata:\n",
    "            value = meta.get(group_key, 'Unknown')\n",
    "            if value not in groups:\n",
    "                groups[value] = []\n",
    "            groups[value].append(meta['mne_obj'])\n",
    "        return groups\n",
    "\n",
    "    def create_epochs(self, tmin: float = -0.2, tmax: float = 0.5, \n",
    "                      baseline: Tuple[Optional[float], Optional[float]] = (None, 0),\n",
    "                      event_id: Optional[Dict[str, int]] = None) -> Dict[str, List[mne.Epochs]]:\n",
    "        \"\"\"\n",
    "        Create epochs for all MNE objects\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        tmin : float\n",
    "            Start time before event\n",
    "        tmax : float\n",
    "            End time after event\n",
    "        baseline : tuple\n",
    "            Baseline correction period\n",
    "        event_id : dict\n",
    "            Event IDs to include\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, List[mne.Epochs]]\n",
    "            Dictionary mapping animal_ids to lists of Epochs objects\n",
    "        \"\"\"\n",
    "        grouped_epochs = {}\n",
    "        \n",
    "        for meta in self.metadata:\n",
    "            animal_id = meta['animal_id']\n",
    "            raw = meta['mne_obj']\n",
    "            \n",
    "            try:\n",
    "                # Extract events from annotations\n",
    "                events, event_dict = mne.events_from_annotations(raw)\n",
    "                \n",
    "                # Use provided event_id if given, otherwise use all events\n",
    "                use_event_id = event_id if event_id else event_dict\n",
    "                \n",
    "                # Create epochs\n",
    "                epochs = mne.Epochs(\n",
    "                    raw, \n",
    "                    events, \n",
    "                    event_id=use_event_id,\n",
    "                    tmin=tmin, \n",
    "                    tmax=tmax, \n",
    "                    baseline=baseline,\n",
    "                    preload=True\n",
    "                )\n",
    "                \n",
    "                if animal_id not in grouped_epochs:\n",
    "                    grouped_epochs[animal_id] = []\n",
    "                \n",
    "                grouped_epochs[animal_id].append(epochs)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Error creating epochs for {animal_id}: {str(e)}\")\n",
    "        \n",
    "        return grouped_epochs\n",
    "\n",
    "    def plot_evoked_by_group(self, group_key: str = 'animal_id', \n",
    "                        tmin: float = -0.2, tmax: float = 0.5,\n",
    "                        baseline: Tuple[Optional[float], Optional[float]] = (None, 0),\n",
    "                        event_id: Optional[Dict[str, int]] = None,\n",
    "                        picks: Optional[List[str]] = None,\n",
    "                        combine: str = 'mean',\n",
    "                        title: Optional[str] = None,\n",
    "                        figsize: Optional[Tuple[float, float]] = None) -> Tuple[plt.Figure, List[plt.Axes]]:\n",
    "        \"\"\"\n",
    "        Plot evoked responses grouped by a metadata key\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        group_key : str\n",
    "            Metadata key to group by (e.g. 'animal_id')\n",
    "        tmin : float\n",
    "            Start time before event\n",
    "        tmax : float\n",
    "            End time after event\n",
    "        baseline : tuple\n",
    "            Baseline correction period\n",
    "        event_id : dict\n",
    "            Event IDs to include\n",
    "        picks : list\n",
    "            Channels to include\n",
    "        combine : str\n",
    "            How to combine channels ('mean', 'median', etc.)\n",
    "        title : str\n",
    "            Figure title\n",
    "        figsize : tuple\n",
    "            Figure size\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[plt.Figure, List[plt.Axes]]\n",
    "            Figure and axes objects\n",
    "        \"\"\"\n",
    "        # Get objects grouped by the specified key\n",
    "        grouped_objects = self.get_objects_by_group(group_key)\n",
    "        \n",
    "        # Create figure\n",
    "        if figsize is None:\n",
    "            figsize = (10, 8 * len(grouped_objects))\n",
    "        fig, axes = plt.subplots(len(grouped_objects), 1, figsize=figsize, sharex=True)\n",
    "        if len(grouped_objects) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        # Process each group\n",
    "        for ax_idx, (group_value, objects) in enumerate(grouped_objects.items()):\n",
    "            ax = axes[ax_idx]\n",
    "            \n",
    "            # List to store evoked objects\n",
    "            evoked_list = []\n",
    "            \n",
    "            # Process each object in the group\n",
    "            for raw in objects:\n",
    "                try:\n",
    "                    # Extract events\n",
    "                    events, event_dict = mne.events_from_annotations(raw)\n",
    "                    \n",
    "                    # Use provided event_id if given, otherwise use all events\n",
    "                    use_event_id = event_id if event_id else event_dict\n",
    "                    \n",
    "                    # Create epochs with channel selection\n",
    "                    epochs = mne.Epochs(\n",
    "                        raw, \n",
    "                        events, \n",
    "                        event_id=use_event_id,\n",
    "                        tmin=tmin, \n",
    "                        tmax=tmax, \n",
    "                        baseline=baseline,\n",
    "                        preload=True,\n",
    "                        picks=picks  # Apply picks here when creating epochs\n",
    "                    )\n",
    "                    \n",
    "                    # Get evoked response\n",
    "                    if len(epochs) > 0:\n",
    "                        evoked = epochs.average()\n",
    "                        evoked_list.append(evoked)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Error processing {group_value}: {str(e)}\")\n",
    "            \n",
    "            # Plot evoked responses\n",
    "            if evoked_list:\n",
    "                # Average across all evoked objects in this group\n",
    "                all_evoked = mne.combine_evoked(evoked_list, weights='equal')\n",
    "                \n",
    "                # Plot the data as regular time series\n",
    "                all_evoked.plot(axes=ax, show=False, titles=None, \n",
    "                            time_unit='s', gfp=True, \n",
    "                            spatial_colors=True)\n",
    "                \n",
    "                # Add group title\n",
    "                ax.set_title(f\"{group_key}: {group_value}\")\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, f\"No valid data for {group_value}\", \n",
    "                    ha='center', va='center', transform=ax.transAxes)\n",
    "        \n",
    "        # Set overall title\n",
    "        if title:\n",
    "            fig.suptitle(title, fontsize=16)\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        return fig, axes\n",
    "\n",
    "    def compute_tfr(self, epochs: mne.Epochs, freqs: np.ndarray, \n",
    "                   n_cycles: Union[float, List[float], np.ndarray] = None,\n",
    "                   picks: Optional[List[str]] = None,\n",
    "                   return_itc: bool = False) -> mne.time_frequency.AverageTFR:\n",
    "        \"\"\"\n",
    "        Compute TFR for epochs using Morlet wavelets\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        epochs : mne.Epochs\n",
    "            Epochs object\n",
    "        freqs : array\n",
    "            Frequencies of interest\n",
    "        n_cycles : float or array\n",
    "            Number of cycles in Morlet wavelet\n",
    "        picks : list\n",
    "            Channels to include\n",
    "        return_itc : bool\n",
    "            Whether to return inter-trial coherence\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        mne.time_frequency.AverageTFR\n",
    "            TFR object\n",
    "        \"\"\"\n",
    "        # Set default n_cycles if not provided\n",
    "        if n_cycles is None:\n",
    "            n_cycles = freqs / 2\n",
    "        \n",
    "        # Compute TFR\n",
    "        power, itc = tfr_morlet(epochs, freqs=freqs, n_cycles=n_cycles, \n",
    "                               use_fft=True, return_itc=True, picks=picks)\n",
    "        \n",
    "        return itc if return_itc else power\n",
    "\n",
    "    def plot_tfr_contrast(self, group_key: str = 'animal_id',\n",
    "                        tmin: float = -0.5, tmax: float = 1.0,\n",
    "                        baseline: Tuple[Optional[float], Optional[float]] = (None, 0),\n",
    "                        event_id: Optional[Dict[str, int]] = None,\n",
    "                        freqs: np.ndarray = np.linspace(4, 40, 20),\n",
    "                        n_cycles: Optional[Union[float, List[float], np.ndarray]] = None,\n",
    "                        picks: Optional[List[str]] = None,\n",
    "                        mode: str = 'zscore',\n",
    "                        title: Optional[str] = None,\n",
    "                        figsize: Optional[Tuple[float, float]] = None) -> Tuple[plt.Figure, List[plt.Axes]]:\n",
    "        \"\"\"\n",
    "        Plot TFR contrast between groups\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        group_key : str\n",
    "            Metadata key to group by (e.g. 'animal_id')\n",
    "        tmin : float\n",
    "            Start time before event\n",
    "        tmax : float\n",
    "            End time after event\n",
    "        baseline : tuple\n",
    "            Baseline correction period\n",
    "        event_id : dict\n",
    "            Event IDs to include\n",
    "        freqs : array\n",
    "            Frequencies of interest\n",
    "        n_cycles : float or array\n",
    "            Number of cycles in Morlet wavelet\n",
    "        picks : list\n",
    "            Channels to include\n",
    "        mode : str\n",
    "            Mode for baseline correction ('zscore', 'mean', 'ratio', 'logratio', 'percent')\n",
    "        title : str\n",
    "            Figure title\n",
    "        figsize : tuple\n",
    "            Figure size\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[plt.Figure, List[plt.Axes]]\n",
    "            Figure and axes objects\n",
    "        \"\"\"\n",
    "        # Get objects grouped by the specified key\n",
    "        grouped_objects = self.get_objects_by_group(group_key)\n",
    "        groups = list(grouped_objects.keys())\n",
    "        \n",
    "        # We need at least 2 groups for contrast\n",
    "        if len(groups) < 2:\n",
    "            raise ValueError(f\"Need at least 2 groups for contrast, but only found: {groups}\")\n",
    "        \n",
    "        # Get channel list to use - intersection of channels in all objects\n",
    "        all_channels = [obj.ch_names for group in grouped_objects.values() for obj in group]\n",
    "        common_channels = set(all_channels[0])\n",
    "        for ch_list in all_channels[1:]:\n",
    "            common_channels &= set(ch_list)\n",
    "        common_channels = sorted(list(common_channels))\n",
    "        \n",
    "        if picks:\n",
    "            common_channels = [ch for ch in common_channels if ch in picks]\n",
    "        \n",
    "        if not common_channels:\n",
    "            raise ValueError(\"No common channels found across all objects\")\n",
    "        \n",
    "        logging.info(f\"Using common channels: {common_channels}\")\n",
    "        \n",
    "        # Dictionary to store TFR results for each group\n",
    "        tfr_by_group = {}\n",
    "        \n",
    "        # Process each group\n",
    "        for group, objects in grouped_objects.items():\n",
    "            # List to store TFR objects\n",
    "            tfr_list = []\n",
    "            \n",
    "            # Process each object in the group\n",
    "            for raw in objects:\n",
    "                try:\n",
    "                    # Extract events\n",
    "                    events, event_dict = mne.events_from_annotations(raw)\n",
    "                    \n",
    "                    # Use provided event_id if given, otherwise use all events\n",
    "                    use_event_id = event_id if event_id else event_dict\n",
    "                    \n",
    "                    # Create epochs\n",
    "                    epochs = mne.Epochs(\n",
    "                        raw, \n",
    "                        events, \n",
    "                        event_id=use_event_id,\n",
    "                        tmin=tmin, \n",
    "                        tmax=tmax, \n",
    "                        baseline=baseline,\n",
    "                        preload=True,\n",
    "                        picks=common_channels\n",
    "                    )\n",
    "                    \n",
    "                    # Compute TFR\n",
    "                    if len(epochs) > 0:\n",
    "                        tfr = self.compute_tfr(epochs, freqs, n_cycles, picks=common_channels)\n",
    "                        tfr_list.append(tfr)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Error processing {group}: {str(e)}\")\n",
    "            \n",
    "            # Average TFRs for this group\n",
    "            if tfr_list:\n",
    "                # Create a grand average TFR\n",
    "                tfr_avg = tfr_list[0].copy()\n",
    "                if len(tfr_list) > 1:\n",
    "                    for tfr in tfr_list[1:]:\n",
    "                        tfr_avg._data += tfr._data\n",
    "                    tfr_avg._data /= len(tfr_list)\n",
    "                \n",
    "                # Apply baseline correction\n",
    "                tfr_avg.apply_baseline(baseline=baseline, mode=mode)\n",
    "                \n",
    "                # Store result\n",
    "                tfr_by_group[group] = tfr_avg\n",
    "        \n",
    "        # Create pairwise contrasts\n",
    "        contrasts = []\n",
    "        group_pairs = []\n",
    "        for i, group1 in enumerate(groups):\n",
    "            for group2 in groups[i+1:]:\n",
    "                if group1 in tfr_by_group and group2 in tfr_by_group:\n",
    "                    # Create a contrast TFR\n",
    "                    contrast = tfr_by_group[group1].copy()\n",
    "                    contrast._data -= tfr_by_group[group2]._data\n",
    "                    \n",
    "                    contrasts.append(contrast)\n",
    "                    group_pairs.append((group1, group2))\n",
    "        \n",
    "        # Determine subplot layout\n",
    "        n_channels = len(common_channels)\n",
    "        n_contrasts = len(contrasts)\n",
    "        \n",
    "        # Create figure\n",
    "        if figsize is None:\n",
    "            figsize = (4 * n_contrasts, 3 * n_channels)\n",
    "        \n",
    "        fig, axes = plt.subplots(n_channels, n_contrasts, figsize=figsize)\n",
    "        \n",
    "        # Handle case of single channel or contrast\n",
    "        if n_channels == 1 and n_contrasts == 1:\n",
    "            axes = np.array([[axes]])\n",
    "        elif n_channels == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        elif n_contrasts == 1:\n",
    "            axes = axes.reshape(-1, 1)\n",
    "        \n",
    "        # Plot each contrast for each channel\n",
    "        for contrast_idx, (contrast, group_pair) in enumerate(zip(contrasts, group_pairs)):\n",
    "            for ch_idx, ch_name in enumerate(common_channels):\n",
    "                ax = axes[ch_idx, contrast_idx]\n",
    "                \n",
    "                # Extract data for this channel\n",
    "                ch_idx_in_tfr = contrast.ch_names.index(ch_name)\n",
    "                data = contrast.data[ch_idx_in_tfr]\n",
    "                \n",
    "                # Plot TFR\n",
    "                im = ax.pcolormesh(contrast.times, contrast.freqs, data,\n",
    "                                  cmap='RdBu_r', vmin=-np.max(np.abs(data)), vmax=np.max(np.abs(data)))\n",
    "                \n",
    "                # Add colorbar for rightmost column\n",
    "                if contrast_idx == n_contrasts - 1:\n",
    "                    plt.colorbar(im, ax=ax)\n",
    "                \n",
    "                # Set labels\n",
    "                if ch_idx == 0:  # Top row\n",
    "                    ax.set_title(f\"{group_pair[0]} - {group_pair[1]}\")\n",
    "                \n",
    "                if contrast_idx == 0:  # Leftmost column\n",
    "                    ax.set_ylabel(f\"{ch_name}\\nFreq (Hz)\")\n",
    "                \n",
    "                if ch_idx == n_channels - 1:  # Bottom row\n",
    "                    ax.set_xlabel(\"Time (s)\")\n",
    "        \n",
    "        # Set overall title\n",
    "        if title:\n",
    "            fig.suptitle(title, fontsize=16)\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mne_objects' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# After loading your MNE objects as shown in your existing code\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m plotter \u001b[38;5;241m=\u001b[39m MNEExperimentPlotter(\u001b[43mmne_objects\u001b[49m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# To plot evoked responses grouped by animal ID\u001b[39;00m\n\u001b[1;32m      5\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plotter\u001b[38;5;241m.\u001b[39mplot_evoked_by_group(group_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manimal_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mne_objects' is not defined"
     ]
    }
   ],
   "source": [
    "# Load MNE objects using the static method\n",
    "animal_ids = ['A5', 'A10']\n",
    "mne_objects = MNEExperimentPlotter.load_mne_objects(animal_ids, base_folder='./test-mnes')\n",
    "\n",
    "# After loading your MNE objects as shown in your existing code\n",
    "plotter = MNEExperimentPlotter(mne_objects)\n",
    "\n",
    "# To plot evoked responses grouped by animal ID\n",
    "fig, axes = plotter.plot_evoked_by_group(group_key='animal_id')\n",
    "\n",
    "# To plot TFR contrast between animals\n",
    "fig, axes = plotter.plot_tfr_contrast(\n",
    "    group_key='animal_id',\n",
    "    tmin=-0.5, \n",
    "    tmax=1.0,\n",
    "    freqs=np.linspace(4, 40, 20),\n",
    "    mode='zscore'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_mouseEEG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
